---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 4.1 | Generalizing The t-Test*

---

## The t-test: Wait Time Example
<p class="subheader">How surprising would it be for the average wait time to be 10 minutes?</p>

:::: {.columns}
::: {.column width="40%"}
**One-sample t-test:**

- $H_0: \mu = 10$  
- $H_1: \mu \neq 10$
- Test statistic: $t = 1.86$
- Degrees of freedom: 29
- p-value: 0.072
:::

::: {.column width="60%"}
```python
# Imports
import numpy as np
from scipy import stats
```

<br>

```python
# Sample Data
sample_mu = 10.85
pop_mu = 10    # null hypothesis
std_dev = 2.5    
n = 30
```

<br>

```python
# Calculate t-statistic
t_stat = (sample_mu - pop_mu) / (std_dev / np.sqrt(n))
```

<br>

```python
# Calculate p-value
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))
```
:::
::::

. . .

*> so there is a 7.2% chance of seeing data this extreme given a true mean of 10*

---

## The t-test: Wait Time Example
<p class="subheader">How surprising would it be for the average wait time to be 10 minutes?</p>

*> there is a 7.2% chance of seeing data this extreme given a true mean of 10*

```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Define parameters
sample_mu = 10.85  # Sample mean
pop_mu = 10        # Null hypothesis (H0)
std_dev = 2.5      # Standard deviation  
n = 30             # Sample size
se = std_dev / np.sqrt(n)  # Standard error
df = n - 1         # Degrees of freedom

# Calculate t-statistic
t_stat = (sample_mu - pop_mu) / se  # About 1.86

# Create the plot
plt.figure(figsize=(10, 4))
x = np.linspace(pop_mu - 3*se, pop_mu + 3*se, 1000)
y = stats.norm.pdf(x, pop_mu, se)  # Normal approximation for simplicity

# Plot the sampling distribution under H0
plt.plot(x, y, 'b-', linewidth=2)

# Shade the tails (two-tailed p-value)
cut_point = abs(sample_mu - pop_mu)
plt.fill_between(x, y, where=(x >= pop_mu + cut_point) | (x <= pop_mu - cut_point), 
                alpha=0.3, color='red')

# Add vertical lines for null, sample mean, and symmetric point
plt.axvline(pop_mu, color='blue', linestyle='--', label='H0: μ = 10')
plt.axvline(sample_mu, color='red', linestyle='--', label='Sample Mean: 10.85')
plt.axvline(pop_mu - cut_point, color='purple', linestyle='--', label='Equally Extreme: 9.15')

# Set y-axis to start at 0
y_max = max(y) * 1.1  # Add a little padding at the top
plt.ylim(0, y_max)

# Add annotations with arrows instead of legend
arrow_props = dict(arrowstyle='->', connectionstyle='arc3,rad=0.2', color='black')

# Annotation for null hypothesis
plt.annotate('Null: μ = 10', 
            xy=(pop_mu, 0.5*max(y)), 
            xytext=(pop_mu-0.5, 0.4*max(y)),
            arrowprops=arrow_props)

# Annotation for sample mean
plt.annotate('Sample Mean: 10.85', 
            xy=(sample_mu, 0.5*max(y)), 
            xytext=(sample_mu+0.3, 0.7*max(y)),
            arrowprops=arrow_props)

# Annotation for equally extreme point
plt.annotate('Equally Extreme: 9.15', 
            xy=(pop_mu - cut_point, 0.5*max(y)), 
            xytext=(pop_mu - cut_point-0.5, 0.6*max(y)),
            arrowprops=arrow_props)

plt.xlabel("Minutes", fontsize=20)
plt.yticks([])
plt.grid(False)
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> but why check both tails? why also consider 9.15, which is equally far from 10 but on the opposite side?*

---

## One-Tail vs Two-Tail
<p class="subheader">... there are at least two key reasons for a two-tail test.</p>

. . .

1. **Scientific Integrity**: Since we're testing the hypothesis "$\mu = 10$" against "$\mu \neq 10$", we should be equally open to evidence in either direction.

. . .

2. **Statistical Reasoning**: If the true mean is 10, our sampling distribution is centered there, and random samples could fall on either side.

---

## Two-Tail t-Test
<p class="subheader">We can perform this very simply from raw data.</p>


*> import basic packages*
```python
# Imports
import scipy.stats as stats
import numpy as np
```

. . .

*> load the observed data and define the null hypothesis*
```python
# Load Data
data = [11.39, 13.55, 10.54, 12.45, 9.  , 12.34, 7.74, 10.97, 9.77, 10.03, 9.52,
    14.09, 8.01, 13.4 , 13.46, 9.43, 10.39, 10.32, 9.36, 7.42, 9.95, 13.72,
    8.14, 12.71, 12.43, 12.83, 11.46, 13.12, 10.12, 8.27]
population_mean = 10
```

. . .

*> perform the test*
```python
# Perform two-tailed t-test
t_stat, p_value = stats.ttest_1samp(data, population_mean)
```

---

## T-Test as Regression
<p class="subheader">Showing the connection with flipped axes</p>

```{python}
# Imports
import scipy.stats as stats
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Load Data (same as previous slide)
data = [11.39, 13.55, 10.54, 12.45, 9.0, 12.34, 7.74, 10.97, 9.77, 10.03, 9.52,
        14.09, 8.01, 13.4, 13.46, 9.43, 10.39, 10.32, 9.36, 7.42, 9.95, 13.72,
        8.14, 12.71, 12.43, 12.83, 11.46, 13.12, 10.12, 8.27]
population_mean = 10

# Calculate key statistics
sample_mean = np.mean(data)
sample_std = np.std(data, ddof=1)
n = len(data)
se = sample_std / np.sqrt(n)
t_stat = (sample_mean - population_mean) / se
df = n - 1
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))

# Create figure with simple, clean design
fig, ax = plt.subplots(figsize=(10, 6))

# Create the t-distribution values
t_values = np.linspace(-4, 4, 1000)
t_densities = stats.t.pdf(t_values, df)

# Plot the t-distribution horizontally (flipped axes)
ax.plot(t_densities, t_values, 'b-', lw=2, label='T-distribution')

# Add horizontal lines for the t-statistic and null hypothesis
ax.axhline(y=0, color='green', linestyle='--', label='Null Hypothesis (μ = 10)')
ax.axhline(y=t_stat, color='red', linestyle='-', label=f'T-statistic = {t_stat:.2f}')

# Shade the p-value regions
if t_stat > 0:
    # Shade upper tail
    upper_t = np.linspace(t_stat, 4, 100)
    upper_density = stats.t.pdf(upper_t, df)
    ax.fill_betweenx(upper_t, 0, upper_density, alpha=0.3, color='red')
    
    # Shade lower tail
    lower_t = np.linspace(-4, -t_stat, 100)
    lower_density = stats.t.pdf(lower_t, df)
    ax.fill_betweenx(lower_t, 0, lower_density, alpha=0.3, color='red')
else:
    # If t_stat is negative, shade the opposite regions
    upper_t = np.linspace(-t_stat, 4, 100)
    upper_density = stats.t.pdf(upper_t, df)
    ax.fill_betweenx(upper_t, 0, upper_density, alpha=0.3, color='red')
    
    # Shade lower tail
    lower_t = np.linspace(-4, t_stat, 100)
    lower_density = stats.t.pdf(lower_t, df)
    ax.fill_betweenx(lower_t, 0, lower_density, alpha=0.3, color='red')

# Add regression approach simple annotation
ax.annotate(f'Regression Intercept Test\nβ₀ = {sample_mean:.2f}, p = {p_value:.3f}',
            xy=(stats.t.pdf(t_stat, df)/2, t_stat), 
            xytext=(stats.t.pdf(0, df)/2, 2),
            arrowprops=dict(facecolor='black', shrink=0.05, width=1),
            fontsize=10,
            bbox=dict(facecolor='white', alpha=0.8))

# Add t-test annotation
ax.annotate(f'One-sample t-test\nt = {t_stat:.2f}, p = {p_value:.3f}',
            xy=(stats.t.pdf(t_stat, df)/2, t_stat),
            xytext=(stats.t.pdf(0, df)/2, -2),
            arrowprops=dict(facecolor='black', shrink=0.05, width=1),
            fontsize=10,
            bbox=dict(facecolor='white', alpha=0.8))

# Set labels with flipped orientation
ax.set_xlabel('Probability Density')
ax.set_ylabel('T-statistic value')

# Set y-axis limits to focus on the important region
ax.set_ylim(-4, 4)

# Add a title
ax.set_title('T-Test as Regression: Two Ways to Test μ = 10')

# Add a legend
ax.legend(loc='upper right')

# Remove unnecessary spines
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

plt.tight_layout()
plt.show()
```

<p class="subheader">Both approaches test the same hypothesis and give identical results.</p>

---

## t-Tests: Simple General Linear Model
<p class="subheader">... draw a "regression" line with only a vertical intercept.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy import stats
import seaborn as sns

# Generate a sample
np.random.seed(42)
sample_size = 30
wait_times = np.random.normal(10.85, 2.5, sample_size)

# Calculate key statistics
sample_mean = np.mean(wait_times)
sample_std = np.std(wait_times, ddof=1)
std_err = sample_std / np.sqrt(sample_size)
null_value = 10
t_stat = (sample_mean - null_value) / std_err

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot 1: Scatter plot with horizontal line at mean
ax1.scatter(np.ones(sample_size), wait_times, alpha=0.6, color='blue')
ax1.axhline(y=sample_mean, color='green', linestyle='-', linewidth=2)
ax1.axhline(y=null_value, color='red', linestyle='--', linewidth=2)
ax1.set_xlim(0.5, 1.5)
ax1.set_ylim(np.min(wait_times)-1, np.max(wait_times)+1)
ax1.set_ylabel('Wait Time (minutes)')
ax1.set_xticks([])
ax1.set_title('Wait Times as Data Points')
ax1.annotate(f'Sample Mean = {sample_mean:.2f}', xy=(1, sample_mean), 
             xytext=(1.1, sample_mean+0.5), color='green')
ax1.annotate(f'Null Value = 10.00', xy=(1, null_value), 
             xytext=(1.1, null_value-0.5), color='red')

# Plot 2: Sampling distribution of the mean centered on sample mean
x_values = np.linspace(sample_mean - 3*std_err, sample_mean + 3*std_err, 1000)
y_dist = stats.norm.pdf(x_values, sample_mean, std_err)

# Plot sampling distribution horizontally
ax2.plot(y_dist, x_values, 'b-', linewidth=2)
ax2.axhline(y=sample_mean, color='green', linestyle='-', linewidth=2)
ax2.axhline(y=null_value, color='red', linestyle='--', linewidth=2)

# Shade the tail areas for two-tailed test relative to null hypothesis
# We need to find points on the distribution that are equidistant from the null value
diff = abs(sample_mean - null_value)
upper_bound = null_value + diff
lower_bound = null_value - diff

ax2.fill_betweenx(
    x_values[(x_values >= upper_bound) | (x_values <= lower_bound)],
    0, 
    y_dist[(x_values >= upper_bound) | (x_values <= lower_bound)],
    color='green', alpha=0.2)

# Format the second plot
ax2.set_xticks([])
ax2.set_title('Sampling Distribution of Mean')
ax2.annotate(f'Sample Mean = {sample_mean:.2f}', xy=(y_dist.max()/3, sample_mean), 
             xytext=(y_dist.max()/2, sample_mean+0.3), color='green')
ax2.annotate(f'Null Value = 10.00', xy=(y_dist.max()/3, null_value), 
             xytext=(y_dist.max()/2, null_value-0.3), color='red')

# Set y-limits for the second plot to match the first plot's key region
y_range = max(sample_mean + 3*std_err, null_value + 3*std_err) - min(sample_mean - 3*std_err, null_value - 3*std_err)
y_center = (sample_mean + null_value) / 2
ax2.set_ylim(y_center - y_range/2, y_center + y_range/2)

# Model fit using statsmodels
X = np.ones(sample_size)  # Intercept only
model = sm.OLS(wait_times, X)
results = model.fit()

# Add regression note
fig.text(0.5, 0.01, f"Regression result: β₀ = {results.params[0]:.2f}, t-stat = {results.tvalues[0]:.2f}, p-value = {results.pvalues[0]:.4f}", 
         ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout(rect=[0, 0.05, 1, 1])
sns.despine(offset=5)
plt.show()
```

. . .

*> in regression terms, we're testing if the intercept (mean) equals some specific value*

. . .

```python
# One-sample t-test using regression
import statsmodels.api as sm
import numpy as np

# Data and null hypothesis value
wait_times = [10.5, 9.8, 11.2, 10.9, 12.4, 10.7, 11.5, 9.9, 10.6, 11.1]
null_value = 10

# Create a column of ones (for intercept only)
X = np.ones(len(wait_times))

# Fit the model (intercept only)
model = sm.OLS(wait_times, X)
results = model.fit()

# The intercept is the sample mean
print(f"Sample mean: {results.params[0]:.3f}")

# t-test for H₀: μ = 10
t_stat = (results.params[0] - null_value) / results.bse[0]
print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {results.pvalues[0]/2:.4f} (one-tailed)")
print(f"p-value: {results.pvalues[0]:.4f} (two-tailed)")
```

---

## Moving from One Sample to Two
<p class="subheader">Now we want to compare two means</p>

<br>

. . .

*> instead of "is µ=10?", we're asking "is µ₁=µ₂?"*

. . .

**Common scenarios:**
- Wait times at different times of day
- Appointment lengths with different doctors
- Wages across different groups
- Treatment effects in experimental settings

. . .

*> the core question: "are these two means different from each other?"*

---

## Distribution of the Difference in Means
<p class="subheader">What if the true means are equal? How different would sample means be?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# Parameters
np.random.seed(42)
true_mean = 10
sample_size = 30
n_simulations = 1000
std_dev = 2.5

# Simulate differences between sample means when true means are equal
diff_means = []
for _ in range(n_simulations):
    sample1 = np.random.normal(true_mean, std_dev, sample_size)
    sample2 = np.random.normal(true_mean, std_dev, sample_size)
    diff_means.append(np.mean(sample1) - np.mean(sample2))

# Plot the distribution of differences
plt.figure(figsize=(10, 4))
plt.hist(diff_means, bins=30, alpha=0.7, density=True, color='skyblue', edgecolor='white')

# Add a theoretical normal curve
x = np.linspace(min(diff_means), max(diff_means), 1000)
# The standard error of the difference is sqrt(SE1²+SE2²)
se_diff = np.sqrt(2) * std_dev / np.sqrt(sample_size)
y = stats.norm.pdf(x, 0, se_diff)
plt.plot(x, y, 'r-', linewidth=2)

# Add vertical line at zero
plt.axvline(x=0, color='green', linestyle='--', linewidth=2)

# Add annotations
plt.title('Distribution of Differences in Sample Means (when true means are equal)', fontsize=14)
plt.xlabel('Difference in Sample Means ($\\bar{x}_1 - \\bar{x}_2$)', fontsize=12)
plt.ylabel('Density', fontsize=12)
plt.annotate(f'SE of difference = {se_diff:.3f}', xy=(2, 0.15), 
             fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> just like before: if means are equal, what differences would we expect by chance?*

. . .

*> if we see a big difference, would that be surprising?*

---

## Simulation: Wait Times Example
<p class="subheader">What if we only have our samples? Let's simulate!</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# Parameters - these represent our collected samples
np.random.seed(42)
morning = [7.2, 6.8, 5.9, 7.5, 6.5, 6.0, 7.1, 6.3, 6.7, 6.4, 6.9, 7.2, 6.2, 6.8, 7.0, 6.5, 7.3, 6.6, 7.1, 6.7]
afternoon = [8.5, 9.2, 8.1, 8.8, 9.5, 8.3, 7.9, 9.0, 8.7, 9.3, 8.6, 9.1, 8.4, 8.9, 9.4]

morning_mean = np.mean(morning)
afternoon_mean = np.mean(afternoon)
morning_std = np.std(morning, ddof=1)
afternoon_std = np.std(afternoon, ddof=1)
morning_size = len(morning)
afternoon_size = len(afternoon)
observed_diff = morning_mean - afternoon_mean

# Standard errors
se_morning = morning_std / np.sqrt(morning_size)
se_afternoon = afternoon_std / np.sqrt(afternoon_size)
se_diff = np.sqrt(se_morning**2 + se_afternoon**2)

# Create figure with multiple panels
fig, axes = plt.subplots(2, 2, figsize=(12, 6))

# Panel 1: Raw data (top left)
axes[0, 0].boxplot([morning, afternoon], labels=['Morning', 'Afternoon'])
axes[0, 0].scatter(np.ones(morning_size), morning, alpha=0.6, color='blue')
axes[0, 0].scatter(np.ones(afternoon_size) * 2, afternoon, alpha=0.6, color='red')
axes[0, 0].set_ylabel('Wait Time (minutes)')
axes[0, 0].set_title('Our Samples: Morning vs. Afternoon')

# Panel 2: Bootstrap sampling distributions (top right)
n_bootstrap = 1000
bootstrap_means_morning = []
bootstrap_means_afternoon = []

for _ in range(n_bootstrap):
    # Resample with replacement
    boot_morning = np.random.choice(morning, size=morning_size, replace=True)
    boot_afternoon = np.random.choice(afternoon, size=afternoon_size, replace=True)
    
    # Calculate and store means
    bootstrap_means_morning.append(np.mean(boot_morning))
    bootstrap_means_afternoon.append(np.mean(boot_afternoon))

axes[0, 1].hist(bootstrap_means_morning, bins=20, alpha=0.5, color='blue', label='Morning means')
axes[0, 1].hist(bootstrap_means_afternoon, bins=20, alpha=0.5, color='red', label='Afternoon means')
axes[0, 1].axvline(x=morning_mean, color='blue', linestyle='--')
axes[0, 1].axvline(x=afternoon_mean, color='red', linestyle='--')
axes[0, 1].set_title('Bootstrap Distributions of Sample Means')
axes[0, 1].legend()

# Panel 3: Bootstrap distribution of differences (bottom left)
bootstrap_diffs = np.array(bootstrap_means_morning) - np.array(bootstrap_means_afternoon)

axes[1, 0].hist(bootstrap_diffs, bins=25, alpha=0.7, color='purple')
axes[1, 0].axvline(x=observed_diff, color='purple', linestyle='-', linewidth=2, 
                  label=f'Observed: {observed_diff:.2f}')
axes[1, 0].axvline(x=0, color='k', linestyle='--', linewidth=1, label='No difference (H₀)')
axes[1, 0].set_title('Bootstrap Distribution of Mean Differences')
axes[1, 0].legend()

# Panel 4: Permutation test for hypothesis testing (bottom right)
n_permutations = 1000
combined = morning + afternoon
permutation_diffs = []

for _ in range(n_permutations):
    # Randomly shuffle and split
    np.random.shuffle(combined)
    perm_morning = combined[:morning_size]
    perm_afternoon = combined[morning_size:]
    
    # Calculate difference under null (no difference between groups)
    perm_diff = np.mean(perm_morning) - np.mean(perm_afternoon)
    permutation_diffs.append(perm_diff)

# Calculate p-value from permutation test
p_value = np.mean(np.abs(permutation_diffs) >= np.abs(observed_diff))

axes[1, 1].hist(permutation_diffs, bins=30, alpha=0.7, color='gray')
axes[1, 1].axvline(x=observed_diff, color='purple', linestyle='-', linewidth=2, 
                  label=f'Observed diff: {observed_diff:.2f}')
axes[1, 1].axvline(x=-observed_diff, color='purple', linestyle='--', linewidth=2)

# Shade areas beyond observed difference (in both directions for two-tailed test)
extreme_indices = np.abs(permutation_diffs) >= np.abs(observed_diff)
extreme_values = np.array(permutation_diffs)[extreme_indices]
axes[1, 1].hist(extreme_values, bins=30, alpha=0.7, color='purple')

axes[1, 1].set_title('Permutation Test (Null: No Difference)')
axes[1, 1].legend()
axes[1, 1].text(0, 30, f'p-value ≈ {p_value:.4f}', bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
sns.despine()
plt.show()
```

. . .

*> we only have our samples - we don't know the true distributions*

. . .

*> top-left: the actual data we collected*

. . .

*> top-right: bootstrap to see how sample means might vary*

. . .

*> bottom-left: bootstrap distribution of their differences*

. . .

*> bottom-right: permutation test to see if the difference could be due to chance*

. . .

*> the p-value tells us how often we'd see differences this extreme by random chance*

---

## The Math: Combining Standard Errors
<p class="subheader">The standard error of a difference combines two sources of sampling variation</p>

<br>

. . .

For a single sample mean:
$SE_{\bar{x}} = \frac{s}{\sqrt{n}}$

. . .

For the difference between two sample means:
$SE_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

. . .

*> two sources of sampling variation combine, but not additively*

. . .

*> if variances are equal ($s_1^2 = s_2^2 = s^2$) and sample sizes are equal ($n_1 = n_2 = n$):*
$SE_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{2s^2}{n}} = \frac{s\sqrt{2}}{\sqrt{n}}$

---

## Two-Sample t-test: Step by Step
<p class="subheader">Example: Morning vs. Afternoon Wait Times</p>

```{python}
#| echo: true
import numpy as np
from scipy import stats

# Sample data: Wait times in minutes
morning = [7.2, 6.8, 5.9, 7.5, 6.5, 6.0, 7.1, 6.3, 6.7, 6.4]
afternoon = [8.5, 9.2, 8.1, 8.8, 9.5, 8.3, 7.9, 9.0, 8.7, 9.3]

# Calculate statistics
mean_morning = np.mean(morning)
mean_afternoon = np.mean(afternoon)
diff_means = mean_morning - mean_afternoon

std_morning = np.std(morning, ddof=1)
std_afternoon = np.std(afternoon, ddof=1)
n_morning = len(morning)
n_afternoon = len(afternoon)

# Calculate pooled standard error of the difference
se_diff = np.sqrt((std_morning**2 / n_morning) + 
                 (std_afternoon**2 / n_afternoon))

# Calculate t-statistic
t_stat = diff_means / se_diff

# Calculate p-value (two-tailed test)
df = min(n_morning - 1, n_afternoon - 1)  # Conservative df
p_value = 2 * stats.t.cdf(-abs(t_stat), df)

print(f"Morning mean: {mean_morning:.2f}, Afternoon mean: {mean_afternoon:.2f}")
print(f"Difference in means: {diff_means:.2f}")
print(f"Standard error of difference: {se_diff:.3f}")
print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.6f}")
```

. . .

*> small p-value = surprising difference (reject null hypothesis of equal means)*

. . .

*> large p-value = difference could easily happen by chance (fail to reject null)*

---

## Visualizing the Difference
<p class="subheader">Morning vs. Afternoon Wait Times</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Sample data: Wait times in minutes
morning = [7.2, 6.8, 5.9, 7.5, 6.5, 6.0, 7.1, 6.3, 6.7, 6.4]
afternoon = [8.5, 9.2, 8.1, 8.8, 9.5, 8.3, 7.9, 9.0, 8.7, 9.3]

# Calculate statistics
mean_morning = np.mean(morning)
mean_afternoon = np.mean(afternoon)
diff_means = mean_morning - mean_afternoon

std_morning = np.std(morning, ddof=1)
std_afternoon = np.std(afternoon, ddof=1)
n_morning = len(morning)
n_afternoon = len(afternoon)

# Calculate standard errors
se_morning = std_morning / np.sqrt(n_morning)
se_afternoon = std_afternoon / np.sqrt(n_afternoon)
se_diff = np.sqrt((std_morning**2 / n_morning) + (std_afternoon**2 / n_afternoon))

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot 1: Box plots with data points
ax1.boxplot([morning, afternoon], labels=['Morning', 'Afternoon'])
ax1.scatter(np.ones(n_morning), morning, alpha=0.6, color='blue')
ax1.scatter(np.ones(n_afternoon) * 2, afternoon, alpha=0.6, color='red')
ax1.set_ylabel('Wait Time (minutes)')
ax1.set_title('Wait Times: Morning vs. Afternoon')

# Plot 2: Means with confidence intervals
means = [mean_morning, mean_afternoon]
errors = [se_morning, se_afternoon]
ax2.bar(['Morning', 'Afternoon'], means, yerr=errors, alpha=0.7, 
       capsize=10, color=['blue', 'red'])
ax2.set_ylabel('Mean Wait Time (minutes)')
ax2.set_title('Mean Wait Times with Standard Errors')
ax2.annotate(f'Difference: {-diff_means:.2f} min\np-value: {p_value:.4f}', 
            xy=(0.5, mean_morning), xytext=(0.5, mean_morning - 1.5),
            ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
sns.despine()
plt.show()
```

. . .

*> clear visualizations help communicate the magnitude and significance of differences*

---

## Two-Sample t-test Using Regression
<p class="subheader">The relationship between t-tests and linear regression</p>

```{python}
#| echo: true
import numpy as np
import statsmodels.api as sm
import pandas as pd

# Combine the data
df = pd.DataFrame({
    'wait_time': morning + afternoon,
    'group': ['morning'] * len(morning) + ['afternoon'] * len(afternoon)
})

# Create dummy variable (1 for afternoon, 0 for morning)
df['is_afternoon'] = (df['group'] == 'afternoon').astype(int)

# Run the regression
X = sm.add_constant(df['is_afternoon'])
model = sm.OLS(df['wait_time'], X)
results = model.fit()

# Print results
print(results.summary().tables[1])
```

. . .

*> the intercept (β₀) is the mean of the reference group (morning)*

. . .

*> the coefficient (β₁) is the difference between groups (afternoon - morning)*

. . .

*> the t-statistic tests if this difference equals zero*

. . .

*> identical p-value to the two-sample t-test!*

---

## Key Insights
<p class="subheader">Connecting t-tests and regression</p>

<br>

. . .

**One-sample t-test:**
- Regression with only an intercept
- Tests if the intercept equals a specific value

. . .

**Two-sample t-test:**
- Regression with an intercept and one dummy variable
- Tests if the coefficient on the dummy equals zero

. . .

**Why this matters:**
- t-tests are just a special case of regression
- This provides a foundation for more complex analyses
- The same framework extends to multiple groups and controlling for other variables

---

## Common Applications in Economics
<p class="subheader">Two-sample t-tests are everywhere!</p>

<br>

. . .

**Labor Economics:**
- Wage gaps between different demographic groups
- Employment effects of policy changes

. . .

**Development Economics:**
- Impact of interventions on economic outcomes
- Differences between treatment and control groups

. . .

**Financial Economics:**
- Comparing returns across different time periods
- Testing market efficiency

. . .

*> one of the most common tests in economic research*

---

## Looking Forward
<p class="subheader">Extending to multiple groups</p>

<br>

. . .

**Next time:**
- Comparing more than two groups
- Analysis of Variance (ANOVA)
- Multiple comparisons and their challenges

. . .

**Coming soon:**
- Multiple regression
- Controlling for confounding variables
- Interaction effects

. . .

*> all built on the same fundamental statistical framework*