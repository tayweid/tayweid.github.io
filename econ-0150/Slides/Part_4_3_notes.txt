# Regression Fundamentals: OLS, T-tests, and ANOVA

## Introduction

We've just established the idea for how a t-test works. We know the distribution of the sample mean, so we can calculate the probability of seeing our sample given a null hypothesis. This is a very simple version of a much more powerful model.

It's more powerful but not fundamentally more complex, so do not fear! Essentially all we're going to do is draw a bunch of lines on a scatterplot. These lines will have the form: y = β₀ + β₁x

## The Intercept-Only Model

Let's start with the simplest one, where there is no x (or x = 0). This looks like a jittered plot, where we have a meaningful vertical axis and we're trying to draw a line through it.

The simplest thing we can do is put the line at the mean of y. You'll remember that the variance is just the squared difference between the mean and each data point. So if we draw these differences, what we call residuals, and square them, we get variance.

Placing the line at the mean minimizes the sum of squared errors. This is a big idea we'll use frequently. 

So if we minimize the mean squared error, we actually choose β₀ = ȳ, the mean! In our climate change example, β₀ would represent the average temperature across our cities.

You'll remember that we have sampling error in every sample. So if we redraw a sample of temperatures a couple of times, we can see that the location of β₀ is different each time.

If we plot these β₀'s on a histogram, we get a t-distribution! So if I were to ask you the probability of getting this sample mean under the standard null of β₀=0, what would you say? We can find this directly from the t-distribution.

## Regression and T-tests

This is nice - we have a line with no slope and we're doing a t-test. But what if the data has a relationship with another variable?

Let's try the case where we want to model the effect of green space percentage on urban temperatures. We could just use the average temperature across all cities (our β₀ from before), but that would assume that cities with different amounts of green space have the same temperature. This doesn't make sense if there's a clear relationship.

Instead, we can allow our model to have a slope: y = β₀ + β₁x + ε and minimize the squared error. The intercept β₀ now represents the predicted temperature when there is zero green space, and β₁ represents how much the temperature changes for each percentage point increase in green space.

Like before, we have sampling error, now not just in the intercept, but in the slope too. If we were to collect many samples, we'd get slightly different slopes each time.

Let's also plot the slopes on a histogram. Does this look familiar? Yes! It's a t-distribution again. So if I were to ask you for the probability we see the data given the standard null of β₁ = 0 (i.e., no relationship between green space and temperature), how would you answer? Find the p-value using the t-distribution, just like before!

## Model Diagnostics

Before we trust our regression results, we need to check if the model's assumptions are met. There are four key assumptions:

1. **Linearity**: The relationship between X and Y is linear
2. **Independence**: Observations are independent from each other 
3. **Homoskedasticity**: Equal error variance across all values of X
4. **Normality**: Errors are normally distributed

To check these assumptions, we analyze the residuals from our model:

**Checking for linearity**: Plot residuals against fitted values. We should see no pattern in this plot. If we see a curve or other pattern, our model may be misspecified. In our climate example, a curved pattern might suggest temperature relates to green space non-linearly.

**Checking for homoskedasticity**: Look for fan or funnel shapes in residual plots. If the spread of residuals changes across the range of X, we may have heteroskedasticity. For example, larger cities might show more temperature variation than smaller ones.

**Checking for normality**: Create a Q-Q plot of residuals. Points should follow a straight diagonal line. Deviations indicate non-normal errors. In climate data, extreme temperature outliers might cause heavy tails.

**Checking for influential points**: Calculate leverage and Cook's distance. High-influence points substantially change regression results when removed. Extremely dense urban areas or rural regions might be influential outliers in our dataset.

## Two-Sample T-Test Using Regression

Let's see how regression can implement a two-sample t-test. Imagine we want to compare temperatures between cities with high green space versus those with low green space.

We create a dummy variable called "high_green" that equals 1 for cities with above-median green space and 0 for cities with below-median green space. Then we run:

```
Temperature = β₀ + β₁ × high_green + ε
```

The interpretation is:
- β₀ = Average temperature in low green space cities
- β₁ = Temperature difference between high and low green space cities

The p-value for β₁ tells us if this temperature difference is statistically significant. This is exactly the same as running a two-sample t-test between these groups.

## Environmental Justice Example

We can apply the same approach to test for environmental inequalities. For example, we can compare pollution levels between high-income and low-income neighborhoods:

```
Pollution = β₀ + β₁ × low_income + ε
```

Here:
- β₀ = Average pollution in high-income areas
- β₁ = Additional pollution in low-income areas

A significant positive β₁ would indicate that low-income areas experience higher pollution levels, suggesting environmental inequality. The beauty of the regression approach is that we can easily extend it to control for other factors, which we'll see later.

## ANOVA Using Regression

What if we have more than two groups? For example, we might want to compare temperatures across four climate regions: Coastal, Mountain, Desert, and Plains.

Traditionally, you might think we need a new test called ANOVA (Analysis of Variance). But we can do the same analysis using regression with dummy variables:

```
Temperature = β₀ + β₁ × Mountain + β₂ × Desert + β₃ × Plains + ε
```

In this model:
- β₀ = Average temperature in Coastal areas (our reference group)
- β₁ = Difference between Mountain and Coastal areas
- β₂ = Difference between Desert and Coastal areas  
- β₃ = Difference between Plains and Coastal areas

The t-tests for each coefficient are testing whether that specific difference is statistically significant. For example, if β₂ has a p-value < 0.05, we would conclude that Desert regions have significantly different temperatures from Coastal regions.

The beauty of this approach is that we can easily change the reference group to make different comparisons. If we want to compare all regions against Desert instead, we just recode our dummy variables:

```
Temperature = β₀ + β₁ × Coastal + β₂ × Mountain + β₃ × Plains + ε
```

Now β₀ represents the average temperature in Desert areas, and the other coefficients show differences compared to Desert.

## Multiple Regression

The real power comes when we add more predictor variables to our model. For example, we could predict pollution based on both income level and population density:

```
Pollution = β₀ + β₁ × Income + β₂ × Density + ε
```

Now we can interpret each coefficient while holding other variables constant:
- β₁ = Effect of income on pollution, holding density constant
- β₂ = Effect of density on pollution, holding income constant

This approach allows us to isolate the relationship between each predictor and our outcome, controlling for potential confounding factors. It's especially important for environmental justice research, where multiple factors might influence pollution levels.

## Example: Wait Times Throughout The Day

Let's work through a concrete example. Imagine we have data on wait times at a service center at different times throughout the day, and we want to see if wait times increase later in the day.

First, we'll set up our data and model:

```python
import pandas as pd
import statsmodels.api as sm
import numpy as np

# Minutes after opening and corresponding wait times (in minutes)
minutes_after_open = [0, 10.17, 20.34, ..., 589.83, 600]
wait_times = [5.99, 4.83, 6.50, ..., 11.56, 12.95]

# Combine into a dataframe
data = pd.DataFrame({
    'minutes_after_open': minutes_after_open, 
    'wait_times': wait_times,
})

# Set up the regression model
X = sm.add_constant(data['minutes_after_open'])  # Adds the intercept term
model = sm.OLS(data['wait_times'], X).fit()
print(model.summary().tables[1])
```

The output might look something like:

```
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.3748      0.431     12.463      0.000       4.511       6.238
minutes        0.0097      0.001      8.154      0.000       0.007       0.012
==============================================================================
```

What does this tell us?
- β₀ (const) = 5.37: When the service center first opens (minutes = 0), the estimated wait time is 5.37 minutes
- β₁ (minutes) = 0.0097: For each minute later in the day, the wait time increases by about 0.01 minutes
- The p-value for β₁ is very small ("0.000"), indicating a statistically significant relationship between time of day and wait time

This is the power of regression - it not only tells us if there's a relationship but quantifies exactly how much wait time increases throughout the day.

## The General Linear Model: A Unified Framework

All of these methods—one-sample t-tests, two-sample t-tests, ANOVA, and multiple regression—are part of the same general linear model framework. They're not fundamentally different tests but variations of the same approach with different predictor variables.

Here's how they all fit together:

**One-sample t-test:**
- Regression with only an intercept: y = β₀ + ε
- Tests whether β₀ = μ₀ (null value)

**Two-sample t-test:**
- Regression with a dummy variable: y = β₀ + β₁ × Group + ε
- Tests whether β₁ = 0 (no difference between groups)

**ANOVA:**
- Regression with multiple dummy variables: y = β₀ + β₁ × Group₁ + β₂ × Group₂ + ... + ε
- Tests whether coefficients = 0 (no differences between groups)

**Multiple regression:**
- Adds more predictor variables: y = β₀ + β₁ × X₁ + β₂ × X₂ + ... + ε
- Each coefficient has its own t-test

In each case, we're estimating coefficients that minimize the sum of squared errors, and using the t-distribution to test hypotheses about these coefficients.

## Economic Applications

Regression is the workhorse of empirical economics. Here are some examples of how these techniques apply to different economic questions:

**Labor Economics:**
- Effect of education on wages: wage = β₀ + β₁ × education + ε
- The coefficient β₁ represents the return to an additional year of education

**Policy Analysis:**
- Impact of minimum wage on employment: employment = β₀ + β₁ × min_wage + ε
- Is β₁ significantly negative (suggesting job losses) or close to zero (suggesting minimal impact)?

**Environmental Economics:**
- Relationship between green infrastructure and property values: property_value = β₀ + β₁ × green_space + ε
- A positive β₁ would suggest environmental amenities increase property values

**Finance:**
- Asset pricing model: return = β₀ + β₁ × market_return + ε
- The coefficient β₁ represents the stock's "beta" or sensitivity to market movements

In each case, we're not just asking if there's a relationship, but quantifying the magnitude of that relationship.

## Key Takeaways

1. **T-tests are regression models** - A t-test is just a regression with a specific structure.

2. **Regression gives more information** - Beyond testing for statistical significance, regression quantifies the relationship between variables.

3. **The framework is unifying** - The same underlying statistical model handles different types of analyses.

4. **Interpretation is straightforward** - Coefficients have clear meanings (e.g., change in Y per unit change in X).

5. **Assumptions matter** - Always check residuals to ensure your inferences are valid.

## Looking Forward

In future classes, we'll extend this framework to include:
- Multiple regression with several predictors
- Controlling for confounding variables
- Categorical variables and dummy coding
- Interaction effects
- Non-linear relationships

All of these build on the same statistical foundation we explored today. The key insight is that once you understand regression, you have the tools to approach a wide range of statistical analyses in economics.

## Practice Example for Climate Data Analysis

For your practice assignment, try running a regression to examine how urban temperature (Y) relates to green space percentage (X) in the dataset I've provided. Calculate:
1. The intercept (β₀) - what would be the temperature in a city with no green space?
2. The slope (β₁) - how much does temperature change per percentage point of green space?
3. The p-value - is this relationship statistically significant?
4. The R-squared - what proportion of temperature variation is explained by green space?

Then check your model assumptions by examining the residuals. Create a residual plot and a Q-Q plot to ensure your inferences are valid.

In your capstone project, you'll be able to extend this to multiple predictors, comparing different urban planning strategies while controlling for population density, building height, and other factors that influence urban heat islands.