---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis stillset.</p>

<br> 

### *Part 4.1 | Numerical Predictors*

---

## GLM: bivariate data
<p class="subheader">Do people wait longer later in the day?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))

ax1.set_title(f'Raw Data \n ', fontsize=16)

# Plot 1: Intercept-only model (t-test approach)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)

# Plot 2
ax2.set_yticks([])
ax2.set_xticks([])

# Plot 3
ax3.set_yticks([])
ax3.set_xticks([])

# Use the same styling as your other plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=True, bottom=True, right=True, top=True, trim=True)
sns.despine(ax=ax3, left=True, bottom=True, right=True, top=True, trim=True)
plt.tight_layout()
```

---

## GLM: bivariate data
<p class="subheader">Do people wait longer later in the day?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))

ax1.set_title(f'Raw Data \n', fontsize=16)

# Plot 1: Intercept-only model (t-test approach)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)

# Plot 2: Intercept-only model
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
ax2.axhline(mean_wait, color='red', linestyle='-', linewidth=2, label=f'$\\beta_0$ = {mean_wait:.2f}; $\\beta_1$ = 0')

# Add vertical error lines for intercept-only model
for i in range(0, n, 1):  # Show errors for every 5th point
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], mean_wait], 'green', linestyle=':', alpha=0.5)

ax2.set_xlabel('Minutes After Opening', fontsize=14)
ax2.set_ylabel('', fontsize=14)
ax2.set_title(f'Model 1: \n $y = \\beta_0 + \\epsilon$', fontsize=16)
ax2.legend(loc=4)

# Plot 3
ax3.set_yticks([])
ax3.set_xticks([])

# Use the same styling as your other plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax3, left=True, bottom=True, right=True, top=True, trim=True)
plt.tight_layout()
```

. . .

*> but in general we don't ask many questions about vertical incercepts*

---

## GLM: bivariate data
<p class="subheader">Do people wait longer later in the day?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))

ax1.set_title(f'Raw Data \n', fontsize=16)

# Plot 1: Intercept-only model (t-test approach)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)

# Plot 2: Intercept-only model
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
ax2.axhline(mean_wait, color='red', linestyle='-', linewidth=2, label=f'$\\beta_0$ = {mean_wait:.2f}; $\\beta_1$ = 0')

# Add vertical error lines for intercept-only model
for i in range(0, n, 1):  # Show errors for every 5th point
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], mean_wait], 'green', linestyle=':', alpha=0.5)

ax2.set_xlabel('Minutes After Opening', fontsize=14)
ax2.set_ylabel('', fontsize=14)
ax2.set_title(f'Model 1: \n $y = \\beta_0 + \\epsilon$', fontsize=16)
ax2.legend(loc=4)

# Plot 3
ax3.set_yticks([])
ax3.set_xticks([])

# Use the same styling as your other plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax3, left=True, bottom=True, right=True, top=True, trim=True)
plt.tight_layout()
```

Lets compare two models.

- **Model 1** (Intercept Only): $y = b$
- **Model 2** (Intercept+Slope): $y = mx + b$

---

## GLM: bivariate data
<p class="subheader">Do people wait longer later in the day?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))

ax1.set_title(f'Raw Data \n', fontsize=16)

# Plot 1: Intercept-only model (t-test approach)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)

# Plot 2: Intercept-only model
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
ax2.axhline(mean_wait, color='red', linestyle='-', linewidth=2, label=f'$\\beta_0$ = {mean_wait:.2f}; $\\beta_1$ = 0')

# Add vertical error lines for intercept-only model
for i in range(0, n, 1):  # Show errors for every 5th point
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], mean_wait], 'green', linestyle=':', alpha=0.5)

ax2.set_xlabel('Minutes After Opening', fontsize=14)
ax2.set_ylabel('', fontsize=14)
ax2.set_title(f'Model 1: \n $y = \\beta_0 + \\epsilon$', fontsize=16)
ax2.legend(loc=4)

# Plot 3: Linear regression model
ax3.scatter(minutes_after_open, wait_times, alpha=0.7)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept + slope * x_line
ax3.plot(x_line, y_line, 'r-', linewidth=2, 
        label=f'$\\beta_0$ = {intercept:.2f}, $\\beta_1$ = {slope:.2f}')

# Add vertical error lines for regression model
for i in range(0, n, 1):  # Show errors for every 5th point
    y_pred = intercept + slope * minutes_after_open[i]
    ax3.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax3.set_xlabel('Minutes After Opening', fontsize=14)
ax3.set_title(f'Model 2: \n $y = \\beta_0 + \\beta_1 x + \\epsilon$', fontsize=16)
ax3.legend(loc=4)

# Use the same styling as your other plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax3, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
```

. . .

*> a slope (β₁) improves model fit (MSE; 'wrongness') when there's a relationship*

. . .

*> the intercept is no longer the mean*

---

## Bivariate GLM: minimizing MSE
<p class="subheader">Which model minimizes the models' 'wrongness' (Mean Squared Error)?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  
wait_times = np.maximum(0, wait_times)  

# Fit the optimal regression model
slope_opt, intercept_opt, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)

# Define three different models
# Model A: Too steep
slope_a, intercept_a = 0.02, 3.5

# Model B: Too shallow  
slope_b, intercept_b = 0.004, 7

# Model C: Optimal (from regression)
slope_c, intercept_c = slope_opt, intercept_opt

# Calculate MSE for all models
mse_a = np.mean((wait_times - (intercept_a + slope_a * minutes_after_open))**2)
mse_b = np.mean((wait_times - (intercept_b + slope_b * minutes_after_open))**2)
mse_c = np.mean((wait_times - (intercept_c + slope_c * minutes_after_open))**2)

# Create a figure with three subplots
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))

# Plot 1: Model A (too steep)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line_a = intercept_a + slope_a * x_line
ax1.plot(x_line, y_line_a, 'r-', linewidth=2, 
        label=f'$\\beta_0$ = {intercept_a:.1f}, $\\beta_1$ = {slope_a:.3f}')

# Add vertical error lines for Model A (show subset for clarity)
for i in range(0, n, 3):  
    y_pred = intercept_a + slope_a * minutes_after_open[i]
    ax1.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)
ax1.set_title(f'Model A\nMSE = {mse_a:.2f}', fontsize=16)
ax1.legend(loc=4)

# Plot 2: Model B (too shallow)
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
y_line_b = intercept_b + slope_b * x_line
ax2.plot(x_line, y_line_b, 'r-', linewidth=2, 
        label=f'$\\beta_0$ = {intercept_b:.1f}, $\\beta_1$ = {slope_b:.3f}')

# Add vertical error lines for Model B
for i in range(0, n, 3):  
    y_pred = intercept_b + slope_b * minutes_after_open[i]
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax2.set_xlabel('Minutes After Opening', fontsize=14)
ax2.set_title(f'Model B\nMSE = {mse_b:.2f}', fontsize=16)
ax2.legend(loc=4)

# Plot 3: Model C (optimal)
ax3.scatter(minutes_after_open, wait_times, alpha=0.7)
y_line_c = intercept_c + slope_c * x_line
ax3.plot(x_line, y_line_c, 'r-', linewidth=2, 
        label=f'$\\beta_0$ = {intercept_c:.1f}, $\\beta_1$ = {slope_c:.3f}')

# Add vertical error lines for Model C
for i in range(0, n, 3):  
    y_pred = intercept_c + slope_c * minutes_after_open[i]
    ax3.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax3.set_xlabel('Minutes After Opening', fontsize=14)
ax3.set_title(f'Model C\nMSE = {mse_c:.2f}', fontsize=16)
ax3.legend(loc=4)

# Use the same styling as your other plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax3, left=False, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
```

. . .

*> Model C minimizes MSE!*

---

## Bivariate GLM: minimizing MSE
<p class="subheader">GLM selects the $\beta_1$ with the smallest MSE.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  
wait_times = np.maximum(0, wait_times)  

# Fit the optimal regression model
slope_opt, intercept_opt, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# LEFT PLOT: Model C (optimal)
ax1.scatter(minutes_after_open, wait_times, alpha=0.7)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept_opt + slope_opt * x_line
ax1.plot(x_line, y_line, 'r-', linewidth=2, 
        label=f'$\\beta_0$ = {intercept_opt:.2f}, $\\beta_1$ = {slope_opt:.3f}')

# Add vertical error lines
for i in range(0, n, 3):  
    y_pred = intercept_opt + slope_opt * minutes_after_open[i]
    ax1.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax1.set_xlabel('Minutes After Opening', fontsize=14)
ax1.set_ylabel('Wait Time (minutes)', fontsize=14)
ax1.set_title('Model C: \n $y = \\beta_0 + \\beta_1 x + \\varepsilon$', fontsize=16)
ax1.legend(loc=4)

# RIGHT PLOT: MSE as a function of beta_1
# Try different slopes while keeping intercept fixed at optimal
beta_1_range = np.linspace(-0.005, 0.025, 100)
mse_values = []

for beta_1 in beta_1_range:
    # Calculate predictions with this slope (using optimal intercept)
    predictions = intercept_opt + beta_1 * minutes_after_open
    mse = np.mean((wait_times - predictions)**2)
    mse_values.append(mse)

# Plot MSE vs beta_1
ax2.plot(beta_1_range, mse_values, 'b-', linewidth=2)
ax2.axvline(slope_opt, color='red', linestyle='--', linewidth=2, 
            label=f'$\\hat{{\\beta}}_1$ = {slope_opt:.3f}')
ax2.scatter([slope_opt], [min(mse_values)], color='red', s=100, zorder=5)

ax2.set_xlabel('Slope ($\\beta_1$)', fontsize=14)
ax2.set_ylabel('Mean Squared Error', fontsize=14)
ax2.set_title('MSE for Different Slopes', fontsize=16)
ax2.legend()

# Style both plots
sns.despine(ax=ax1, left=False, bottom=False, right=True, top=True, trim=True)
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
```

. . .

*> this slope (β₁) gives the best guess of the relationship between x and y*

. . .

*> but what if the true slope is zero ... could this slope be just sampling error?*

---

## Bivariate GLM: sampling error
<p class="subheader">Like before, if we take many samples, we get slighly different slopes and slighly different fits.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set seed for reproducibility
np.random.seed(42)

# Define true parameters (for the data generation process)
true_intercept = 5
true_slope = 0.01
noise_std = 4.0

# Function to generate a single sample dataset
def generate_sample(n=20):
    x = np.linspace(0, 600, n)
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
    return x, y

# Generate 8 different samples and fit models
n_examples = 8
samples = []
slopes = []
intercepts = []

for i in range(n_examples):
    x, y = generate_sample()
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    samples.append((x, y))
    slopes.append(slope)
    intercepts.append(intercept)

# Create subplot grid for the 8 examples
fig, axes = plt.subplots(2, 4, figsize=(11, 6))
axes = axes.flatten()

for i, ax in enumerate(axes):
    x, y = samples[i]
    slope = slopes[i]
    intercept = intercepts[i]
    
    # Plot data points
    ax.scatter(x, y, alpha=0.5, s=20)
    
    # Plot regression line
    x_line = np.array([min(x), max(x)])
    y_line = intercept + slope * x_line
    ax.plot(x_line, y_line, 'r-', linewidth=2)
    
    # Clean up axis labels - only show y-axis label for leftmost plots
    if i % 4 == 0:
        ax.set_ylabel('Wait Time', fontsize=12)
    # Only show x-axis label for bottom plots
    if i >= 4:
        ax.set_xlabel('Minutes After Opening', fontsize=12)
    else:
        ax.set_xlabel('')
    
    # Set title
    ax.set_title(f'Sample {i+1}: Slope = {slope:.4f}', fontsize=12)
    ax.set_ylim(1,15)
    
    sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
plt.show()
```

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">The slope coefficient follows a normal distribution centered on the population slope.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set random seed for reproducibility
np.random.seed(42)
# True parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
sample_size = 20
n_simulations = 1000
# Generate x values (same for all simulations)
x = np.linspace(0, 600, sample_size)
# Run many simulations to get slope coefficients
slope_coefficients = []
for i in range(n_simulations):
    # Generate y with noise
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, sample_size)
    
    # Calculate regression slope
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    slope_coefficients.append(slope)
# Create the plot
plt.figure(figsize=(10, 4))
# Plot histogram of slope coefficients
plt.hist(slope_coefficients, bins=30, alpha=0.2, density=True, 
         color='red', edgecolor='white', label='Sample Slopes')
# Overlay theoretical t-distribution
# Standard error of the slope
se_slope = np.std(slope_coefficients)
x_pdf = np.linspace(min(slope_coefficients), max(slope_coefficients), 1000)
y_pdf = stats.t.pdf(x_pdf, df=sample_size-2, loc=true_slope, scale=se_slope)
plt.plot(x_pdf, y_pdf, color='green', linestyle='--', linewidth=2, label='Sampling Dist.')
# Add vertical line for true slope
plt.axvline(true_slope, color='black', linestyle='--', linewidth=2, 
           label=f'True Slope')
# Add vertical line for null hypothesis (slope = 0)
plt.xlabel('Slope Coefficient: $\\beta_1$', fontsize=16)
plt.xlim(-0.005,0.025)
plt.legend()
plt.yticks([])
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> the slopes follow a normal distribution around the population relationship!*

. . .

*> this lets us perform a t-test on the slope!*

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">The slope coefficient follows a normal distribution centered on the population slope.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set random seed for reproducibility
np.random.seed(42)
# True parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
sample_size = 20
n_simulations = 1000

sample_slope = 0.008
se_slope = np.std(slope_coefficients)

# Generate x values (same for all simulations)
x = np.linspace(0, 600, sample_size)
# Run many simulations to get slope coefficients
slope_coefficients = []
for i in range(n_simulations):
    # Generate y with noise
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, sample_size)
    
    # Calculate regression slope
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    slope_coefficients.append(slope)
# Create the plot
plt.figure(figsize=(10, 4))
# Plot histogram of slope coefficients
plt.hist(slope_coefficients, bins=30, alpha=0.05, density=True, 
         color='red', edgecolor='white', label='Sample Slopes')
# Overlay theoretical t-distribution
# Standard error of the slope
se_slope = np.std(slope_coefficients)
x_pdf = np.linspace(min(slope_coefficients), max(slope_coefficients), 1000)
y_pdf = stats.t.pdf(x_pdf, df=sample_size-2, loc=true_slope, scale=se_slope)
plt.plot(x_pdf, y_pdf, color='green', linestyle='--', linewidth=2, label='Sampling Dist.')
# Add vertical line for true slope
plt.axvline(sample_slope, color='red', linestyle='--', linewidth=2, 
           label=f'Sample Slope')
# Add vertical line for null hypothesis (slope = 0)
plt.xlabel('Slope Coefficient: $\\beta_1$', fontsize=16)
plt.xlim(-0.005,0.025)
plt.legend()
plt.yticks([])
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

*> we don't know the entire distribution, just our sample slope*

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">The slope coefficient follows a normal distribution centered on the population slope.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set random seed for reproducibility
np.random.seed(42)
# True parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
sample_size = 20
n_simulations = 1000

sample_slope = 0.008
se_slope = np.std(slope_coefficients)

# Generate x values (same for all simulations)
x = np.linspace(0, 600, sample_size)
# Run many simulations to get slope coefficients
slope_coefficients = []
for i in range(n_simulations):
    # Generate y with noise
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, sample_size)
    
    # Calculate regression slope
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    slope_coefficients.append(slope)
# Create the plot
plt.figure(figsize=(10, 4))
# Plot histogram of slope coefficients
plt.hist(slope_coefficients, bins=30, alpha=0.02, density=True, 
         color='red', edgecolor='white')
# Overlay theoretical t-distribution
# Standard error of the slope
se_slope = np.std(slope_coefficients)
x_pdf = np.linspace(min(slope_coefficients), max(slope_coefficients), 1000)
y_pdf = stats.t.pdf(x_pdf, df=sample_size-2, loc=0, scale=se_slope)
plt.plot(x_pdf, y_pdf, color='green', linestyle='--', linewidth=2, label='Sampling Dist.')
# Add vertical line for true slope
plt.axvline(sample_slope, color='red', linestyle='--', linewidth=2, 
           label=f'Sample Slope')
plt.axvline(0, color='black', linestyle='-', linewidth=2, 
           label=f'Default Null')
# Add vertical line for null hypothesis (slope = 0)
plt.xlabel('Slope Coefficient: $\\beta_1$', fontsize=16)
plt.xlim(-0.005,0.025)
plt.legend()
plt.yticks([])
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

*> center the distribution on our null*

*> check the distance from the sample*

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">The slope coefficient follows a normal distribution centered on the population slope.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set random seed for reproducibility
np.random.seed(42)
# True parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
sample_size = 20
n_simulations = 1000
sample_slope = 0.008
se_slope = np.std(slope_coefficients)
null=0
# Generate x values (same for all simulations)
x = np.linspace(0, 600, sample_size)
# Run many simulations to get slope coefficients
slope_coefficients = []
for i in range(n_simulations):
    # Generate y with noise
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, sample_size)
    
    # Calculate regression slope
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    slope_coefficients.append(slope)
# Create the plot
plt.figure(figsize=(10, 4))
# Plot histogram of slope coefficients
plt.hist(slope_coefficients, bins=30, alpha=0.02, density=True, 
         color='red', edgecolor='white')
# Overlay theoretical t-distribution
# Standard error of the slope
se_slope = np.std(slope_coefficients)
x_pdf = np.linspace(-max(slope_coefficients), max(slope_coefficients), 1000)
y_pdf = stats.t.pdf(x_pdf, df=sample_size-2, loc=null, scale=se_slope)
plt.plot(x_pdf, y_pdf, color='green', linestyle='--', linewidth=2, label='Sampling Dist.')

# Add vertical line for true slope
plt.axvline(sample_slope, color='red', linestyle='--', linewidth=2, 
           label=f'Sample Slope')
# Add vertical line for null hypothesis (slope = 0)
plt.axvline(0, color='black', linestyle='-', linewidth=2, 
           label=f'Default Null')
plt.axvline(-sample_slope, color='red', linestyle='--', linewidth=2, alpha=0.3,
           label=f'Equally Extreme')

plt.xlabel('Slope Coefficient: $\\beta_1$', fontsize=16)
plt.xlim(-0.025,0.025)
plt.legend()
plt.yticks([])
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

*> the p-value is the probability of something as far from the null as our sample*

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">The slope coefficient follows a normal distribution centered on the population slope.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Set random seed for reproducibility
np.random.seed(42)
# True parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
sample_size = 20
n_simulations = 1000
sample_slope = 0.008
se_slope = np.std(slope_coefficients)
null=0
# Generate x values (same for all simulations)
x = np.linspace(0, 600, sample_size)
# Run many simulations to get slope coefficients
slope_coefficients = []
for i in range(n_simulations):
    # Generate y with noise
    y = true_intercept + true_slope * x + np.random.normal(0, noise_std, sample_size)
    
    # Calculate regression slope
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    slope_coefficients.append(slope)
# Create the plot
plt.figure(figsize=(10, 4))
# Plot histogram of slope coefficients
plt.hist(slope_coefficients, bins=30, alpha=0.01, density=True, 
         color='red', edgecolor='white')
# Overlay theoretical t-distribution
# Standard error of the slope
se_slope = np.std(slope_coefficients)
x_pdf = np.linspace(-max(slope_coefficients), max(slope_coefficients), 1000)
y_pdf = stats.t.pdf(x_pdf, df=sample_size-2, loc=null, scale=se_slope)
plt.plot(x_pdf, y_pdf, color='green', linestyle='--', linewidth=2, label='Sampling Dist.')

# Add vertical line for true slope
plt.axvline(sample_slope, color='red', linestyle='--', linewidth=2, 
           label=f'Sample Slope')
# Add vertical line for null hypothesis (slope = 0)
plt.axvline(0, color='black', linestyle='-', linewidth=2, 
           label=f'Default Null')
plt.axvline(-sample_slope, color='red', linestyle='--', linewidth=2, alpha=0.3,
           label=f'Equally Extreme')

# SHADE P-VALUE REGIONS
# Right tail (beyond sample slope)
x_right = x_pdf[x_pdf >= sample_slope]
y_right = stats.t.pdf(x_right, df=sample_size-2, loc=null, scale=se_slope)
plt.fill_between(x_right, 0, y_right, alpha=0.3, color='blue', label='p-Value')

# Left tail (beyond negative sample slope)
x_left = x_pdf[x_pdf <= -sample_slope]
y_left = stats.t.pdf(x_left, df=sample_size-2, loc=null, scale=se_slope)
plt.fill_between(x_left, 0, y_left, alpha=0.3, color='blue')

plt.xlabel('Slope Coefficient: $\\beta_1$', fontsize=16)
plt.xlim(-0.025,0.025)
plt.legend()
plt.yticks([])
sns.despine(left=True, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

*> p-value: the 'surprisingness' of our sample if $\beta_1 = 0$*

. . .

*> the probability of seeing our sample by chance if there is no relationship*

. . .

*> a small p-value is evidence against the null hypothesis ($\beta_1 = 0$)*

---

## Bivariate GLM: sampling distribution of slopes
<p class="subheader">Many possible models we might observe by chance if the null ($\beta_1 = 0$) were true.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})

# Set seed for reproducibility
np.random.seed(42)

# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 50

# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)

# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Simulate what we'd see under the null hypothesis
# Generate many samples under H0: slope = 0
n_null_samples = 50
null_intercepts = []
null_slopes = []

for i in range(n_null_samples):
    # Under null, y = intercept + noise (no relationship with x)
    # Generate y values with just an intercept (mean) and noise
    y_null = np.mean(y) + np.random.normal(0, noise_std, n)
    
    # Fit a line to this null-generated data
    slope_null, intercept_null, _, _, _ = stats.linregress(x, y_null)
    null_slopes.append(slope_null)
    null_intercepts.append(intercept_null)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot the null distribution lines FIRST (so they're behind)
x_line = np.array([min(x), max(x)])
for i in range(n_null_samples):
    y_line_null = null_intercepts[i] + null_slopes[i] * x_line
    ax.plot(x_line, y_line_null, color='gray', linewidth=0.5, alpha=0.3)
ax.plot(x_line, y_line_null, color='gray', linewidth=0.5, alpha=0.3, label='Null Slopes')

# Plot data points
ax.scatter(x, y, alpha=0.1, s=40, zorder=5)

# Plot main regression line LAST (so it's on top)
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'Sample slope = {slope_main:.4f}', zorder=10)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc='upper left')

sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
plt.show()
```

. . .

*> how likely does it look like this slope was drawn from the null slopes?*

. . .

*> p-value: the probability a slope as extreme as ours under the null ($\beta_1=0$)*

---

## Exercise 4.1 | Happiness and Per Capita GDP
<p class="subheader">Are wealtheir countries happier?</p>

<br><br>

```{.python}
# Model: y = b + mx
model = smf.ols('Life_Evaluation ~ log_GDP_PerCap', data).fit() # Intercept is included by default
print(model.summary().tables[1])
```

. . .

<br>

```{.python}
# Visualize: Numerical x Numerical
sns.regplot(data=data, y='Life_Evaluation', x='log_GDP_PerCap')#, ci=None), line_kws={'color': 'red'}
```

---

## GLM: predictions
<p class="subheader">What wait time should we expect at 100 minutes after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot data points - highlight those near 100 minutes
for i, (xi, yi) in enumerate(zip(x, y)):
    ax.scatter(xi, yi, alpha=0.8, s=60, color='blue', zorder=6)

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc=4)
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```
---

## GLM: predictions
<p class="subheader">What wait time should we expect at 100 minutes after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot data points - highlight those near 100 minutes
for i, (xi, yi) in enumerate(zip(x, y)):
    if abs(xi - 100) < 30:  # Points within 30 minutes of 100
        ax.scatter(xi, yi, alpha=0.8, s=60, color='blue', zorder=6)
    else:
        ax.scatter(xi, yi, alpha=0.05, s=40, zorder=5, color='blue')

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

# HIGHLIGHT THE PREDICTION AT 100 MINUTES
x_pred = 100
y_pred = intercept_main + slope_main * x_pred

# Add vertical line at x=100
ax.axvline(x=100, color='green', linestyle='--', alpha=0.5, linewidth=1.5, zorder=3)

# Add shaded region around x=100
ax.axvspan(70, 130, alpha=0.1, color='green', zorder=1)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc=4)
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

---

## GLM: predictions
<p class="subheader">What wait time should we expect at 100 minutes after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot data points - highlight those near 100 minutes
for i, (xi, yi) in enumerate(zip(x, y)):
    if abs(xi - 100) < 30:  # Points within 30 minutes of 100
        ax.scatter(xi, yi, alpha=0.8, s=60, color='blue', zorder=6)
    else:
        ax.scatter(xi, yi, alpha=0.05, s=40, zorder=5, color='blue')

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

# HIGHLIGHT THE PREDICTION AT 100 MINUTES
x_pred = 100
y_pred = intercept_main + slope_main * x_pred

# Add vertical line at x=100
ax.axvline(x=100, color='green', linestyle='--', alpha=0.5, linewidth=1.5, zorder=3)

# Add horizontal line showing the prediction
ax.plot([0, x_pred], [y_pred, y_pred], 'green', linestyle=':', alpha=0.7, linewidth=1.5, zorder=3)

# Add the prediction point
ax.scatter([x_pred], [y_pred], color='green', s=150, marker='*', 
           label=f'Prediction at 100 min: {y_pred:.1f}', zorder=11)

# Add shaded region around x=100
ax.axvspan(70, 130, alpha=0.1, color='green', zorder=1)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc=4)
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> you can find this with a calculator!*

. . .

*> plug $x=100$ into the equation $y = 4.31 + 0.011 x$*


---

## GLM: predictions
<p class="subheader">What wait time should we expect at 200 minutes after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot data points - highlight those near 100 minutes
for i, (xi, yi) in enumerate(zip(x, y)):
    if abs(xi - 200) < 30:  # Points within 30 minutes of 200
        ax.scatter(xi, yi, alpha=0.8, s=60, color='blue', zorder=6)
    else:
        ax.scatter(xi, yi, alpha=0.05, s=40, zorder=5, color='blue')

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

# HIGHLIGHT THE PREDICTION AT 100 MINUTES
x_pred = 200
y_pred = intercept_main + slope_main * x_pred

# Add vertical line at x=200
ax.axvline(x=200, color='green', linestyle='--', alpha=0.5, linewidth=1.5, zorder=3)

# Add shaded region around x=100
ax.axvspan(170, 230, alpha=0.1, color='green', zorder=1)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc=4)
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

---

## GLM: predictions
<p class="subheader">What wait time should we expect at 200 minutes after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)

# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(7, 5))

# Plot data points - highlight those near 100 minutes
for i, (xi, yi) in enumerate(zip(x, y)):
    if abs(xi - 200) < 30:  # Points within 30 minutes of 200
        ax.scatter(xi, yi, alpha=0.8, s=60, color='blue', zorder=6)
    else:
        ax.scatter(xi, yi, alpha=0.05, s=40, zorder=5, color='blue')

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

# HIGHLIGHT THE PREDICTION AT 100 MINUTES
x_pred = 200
y_pred = intercept_main + slope_main * x_pred

# Add vertical line at x=200
ax.axvline(x=200, color='green', linestyle='--', alpha=0.5, linewidth=1.5, zorder=3)

# Add shaded region around x=100
ax.axvspan(170, 230, alpha=0.1, color='green', zorder=1)

# Add horizontal line showing the prediction
ax.plot([0, x_pred], [y_pred, y_pred], 'green', linestyle=':', alpha=0.7, linewidth=1.5, zorder=3)

# Add the prediction point
ax.scatter([x_pred], [y_pred], color='green', s=150, marker='*', 
           label=f'Prediction at 200 min: {y_pred:.1f}', zorder=11)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_title('', fontsize=16)
ax.legend(loc=4)
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

---

## Exercise 4.1 | Happiness and Per Capita GDP
<p class="subheader">Are wealtheir countries happier?</p>

<br><br>

```{.python}
# Parameters
b0, b1 = model.params  # intercept, slope
```

. . .

<br>

```{.python}
# Predict: Log GDP of 3
prediction = b0 + b1 * 3
print(prediction)
```

---

## GLM: interpretation
<p class="subheader">How much does wait time increase every minute after open?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              
    'font.serif': ['Times New Roman'],   
    'font.style': 'italic',              
})
# Set seed for reproducibility
np.random.seed(42)
# Define true parameters
true_intercept = 5
true_slope = 0.01
noise_std = 4.0
n = 100
# Generate one sample dataset
x = np.linspace(0, 600, n)
y = true_intercept + true_slope * x + np.random.normal(0, noise_std, n)
# Fit the main model
slope_main, intercept_main, r_value, p_value, std_err_main = stats.linregress(x, y)
# Create the figure
fig, ax = plt.subplots(1, 1, figsize=(8, 5))

# Plot all data points faintly
ax.scatter(x, y, alpha=0.05, s=40, zorder=5, color='blue')

# Plot main regression line
x_line = np.array([min(x), max(x)])
y_line_main = intercept_main + slope_main * x_line
ax.plot(x_line, y_line_main, 'r-', linewidth=2.5, 
        label=f'$\\hat{{y}} = {intercept_main:.2f} + {slope_main:.3f}x$', zorder=10)

# DEMONSTRATE SLOPE WITH TWO PREDICTIONS
x1 = 200
x2 = 300  # 100 minutes later
y1_pred = intercept_main + slope_main * x1
y2_pred = intercept_main + slope_main * x2

# Highlight data points near these x values
for i, (xi, yi) in enumerate(zip(x, y)):
    if abs(xi - x1) < 20 or abs(xi - x2) < 20:
        ax.scatter(xi, yi, alpha=0.6, s=60, color='blue', zorder=6)

# Add vertical lines at both x values
ax.axvline(x=x1, color='green', linestyle='--', alpha=0.4, linewidth=1.5, zorder=3)
ax.axvline(x=x2, color='green', linestyle='--', alpha=0.4, linewidth=1.5, zorder=3)

# Add horizontal prediction lines
ax.plot([0, x1], [y1_pred, y1_pred], 'green', linestyle=':', alpha=0.5, linewidth=1.5, zorder=3)
ax.plot([0, x2], [y2_pred, y2_pred], 'green', linestyle=':', alpha=0.5, linewidth=1.5, zorder=3)

# Add prediction points
ax.scatter([x1], [y1_pred], color='green', s=150, marker='o', zorder=11)
ax.scatter([x2], [y2_pred], color='green', s=150, marker='o', zorder=11)

# SHOW THE SLOPE VISUALLY
# Get y-axis limits to position arrows
ylims = ax.get_ylim()
y_bottom = ylims[0] + 0.5  # Just above bottom axis

# Draw horizontal arrow showing change in x (near bottom of plot)
ax.annotate('', xy=(x2, y_bottom), xytext=(x1, y_bottom),
            arrowprops=dict(arrowstyle='<->', color='purple', lw=2))
ax.text((x1+x2)/2, y_bottom + 0.5, f'Δx = {x2-x1:.0f} min', 
        ha='center', fontsize=12, color='purple')

# Draw vertical arrow showing change in y (near left side of plot)
x_left = 20  # Just to the right of y-axis
ax.annotate('', xy=(x_left, y2_pred+0.3), xytext=(x_left, y1_pred-0.3),
            arrowprops=dict(arrowstyle='<->', color='purple', lw=2))
ax.text(x_left + 20, (y1_pred+y2_pred)/2, f'Δy = {y2_pred-y1_pred:.2f}', 
        va='center', fontsize=12, color='purple')

# Connect the two predictions with a light line to show they're on the regression line
ax.plot([x1, x2], [y1_pred, y2_pred], 'g-', alpha=0.3, linewidth=3, zorder=9)

ax.set_xlabel('Minutes After Opening', fontsize=14)
ax.set_ylabel('Wait Time (minutes)', fontsize=14)
ax.set_xlim(0, 620)
ax.legend(loc='upper left')
sns.despine(ax=ax, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> $\beta_1$ tells us how much $y$ increases with every 1 unit increase in $x$*

---

## Exercise 4.1 | Happiness and Per Capita GDP
<p class="subheader">How much does happiness increase for each additional $1,000 of per capita GDP? </p>

---

## The General Linear Model
<p class="subheader">GLM performs a t-test on all model coefficients.</p>

. . .

**Univariate** *(Part 3)*: $y = \beta_0 + \epsilon$

. . .

- Equivalent to a one-sample t-test
- Tests whether $\beta_0 = \mu_0$ (default null)

. . .

**Numerical Predictor**: $y = \beta_0 + \beta_1 x + \epsilon$

. . .

- $x$ is a numerical variable (like age, income, temperature, etc.)
- Tests both intercept ($\beta_0 = 0$) and slope ($\beta_1 = 0$)
- Null hypothesis on slope: no relationship between x and y ($\beta_1 = 0$)


---

## The General Linear Model
<p class="subheader">GLM uses the idea of a t-test with any coefficient.</p>

**Categorical Predictor** *(next time)*: $y = \beta_0 + \beta_1 x + \epsilon$

. . .

- $x$ is a categorical variable (like age, income, temperature, etc.)
- Equivalent to a two-sample t-test (when $x$ is binary)

. . .

**Multivariate GLM** *(Part 5)*:

. . .

- Adds more predictor variables: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \varepsilon$
- Each coefficient has its own t-test against the null that it equals zero

---

## Economic Applications
<p class="subheader">GLM is the workhorse statistical tool in empirical economics.</p>

. . .

**Labor Economics:** *relationship between education and wages.*

$$\text{wage} = \beta_0 + \beta_1 \text{education} + \varepsilon$$

. . .

**Policy Analysis:** *relationship between minimum wages and employment.*

$$\text{employment} = \beta_0 + \beta_1 \text{minimum_wage} + \varepsilon$$

. . .

**Political Economy:** *relationship between neighbor's party and voter turnout*

$$\text{voted} = \beta_0 + \beta_1 \text{neighborhood_politics} + \varepsilon$$

---

## Bivariate GLM: Numerical Predictors 
<p class="subheader">Summary</p>

. . .

**GLM Framework:** 

- *T-tests and regression are part of the same very flexible framework.*

. . .

**Numerical Predictors:** 

- *Bivariate GLM extends the t-test by allowing continuous predictors.*

. . .

**Same Distribution:** 

- *Coefficient estimates follow t-distributions centered on the true population values.*

. . .

**Same Interpretation:** 

- *The p-values have the same interpretation: probability of seeing results this extreme if the null is true.*

---

## Looking Forward
<p class="subheader">Extending the GLM framework</p>

. . .

**Next Up:**

- Part 4.2 | Bad Models
- Part 4.3 | Categorical Predictors
- Part 4.4 | Timeseries
- Part 4.5 | Causality

. . .

**Later:**

- Part 5.1 | Numerical Controls
- Part 5.2 | Categorical Controls
- Part 5.3 | Interactions
- Part 5.4 | Model Selection

. . .

*> all built on the same statistical foundation*