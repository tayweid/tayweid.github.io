---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 4.4 | Multiple Regression*

---

## Multiple Regression
<p class="subheader">Wages depend on more than just education</p>

*Wages also depend on:*

- Experience
- Industry
- Location
- And many other factors

. . .

*> how can we handle multiple relationships at once?*

---

## Modeling Relationships Separately
<p class="subheader">What if we build a regression model for both relationships separately?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set seed for reproducibility
np.random.seed(42)

# Generate education data (years)
education = np.concatenate([np.random.randint(10, 13, 50), 
                           np.random.randint(13, 17, 70),
                           np.random.randint(17, 22, 30)])

# Generate experience data (years) - negatively correlated with education
mean_experience = 40 - 1.5 * education  # Higher education -> less experience on average
experience = np.maximum(0, mean_experience + np.random.normal(0, 3, len(education)))

# Generate wage data affected by both education and experience
educ_effect = 1500
exp_effect = 500
base_wage = 15000
error_scale = 5000

wages = base_wage + educ_effect * education + exp_effect * experience + np.random.normal(0, error_scale, len(education))

# Create figure with side-by-side plots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot education vs wages
slope_educ, intercept_educ, r_educ, p_educ, se_educ = stats.linregress(education, wages)
ax1.scatter(education, wages, alpha=0.6, color='#4C72B0')
x_line = np.array([min(education), max(education)])
y_line = intercept_educ + slope_educ * x_line
ax1.plot(x_line, y_line, color='red', linewidth=2, 
        label=f'Slope ≈ ${slope_educ:.0f}/year')
ax1.set_title('Education vs. Wages', fontsize=14)
ax1.set_xlabel('Years of Education', fontsize=12)
ax1.set_ylabel('Annual Wage ($)', fontsize=12)
ax1.legend()

# Plot experience vs wages
slope_exp, intercept_exp, r_exp, p_exp, se_exp = stats.linregress(experience, wages)
ax2.scatter(experience, wages, alpha=0.6, color='#4C72B0')
x_line = np.array([min(experience), max(experience)])
y_line = intercept_exp + slope_exp * x_line
ax2.plot(x_line, y_line, color='red', linewidth=2, 
        label=f'Slope ≈ ${slope_exp:.0f}/year')
ax2.set_title('Experience vs. Wages', fontsize=14)
ax2.set_xlabel('Years of Experience', fontsize=12)
ax2.set_ylabel('Annual Wage ($)', fontsize=12)
ax2.legend()

sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> does this mean years of experience has a negative relationship with wages?*

---

## The Challenge: Related Variables
<p class="subheader">Education and experience are correlated!</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Plot the relationship between education and experience
plt.figure(figsize=(8, 5))
slope_rel, intercept_rel, r_rel, p_rel, se_rel = stats.linregress(education, experience)
plt.scatter(education, experience, alpha=0.6, color='#4C72B0')
x_line = np.array([min(education), max(education)])
y_line = intercept_rel + slope_rel * x_line
plt.plot(x_line, y_line, color='red', linewidth=2, 
         label=f'Correlation: {r_rel:.2f}')

plt.title('Relationship Between Education and Experience', fontsize=14)
plt.xlabel('Years of Education', fontsize=12)
plt.ylabel('Years of Experience', fontsize=12)
plt.legend()
sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> more education usually means less work experience*

. . .

*> if we look at one without accounting for the other, we get misleading results*

---

## Multiple Regression
<p class="subheader">We can adjust for multiple variables simultaneously.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Fit multiple regression model
X = sm.add_constant(np.column_stack((education, experience)))
model = sm.OLS(wages, X).fit()

# Create a 3D figure
fig = plt.figure(figsize=(10, 5))
ax = fig.add_subplot(111, projection='3d')

# Plot original data points
ax.scatter(education, experience, wages, c='#4C72B0', alpha=0.6)

# Create a meshgrid to visualize the regression plane
edu_range = np.linspace(min(education), max(education), 20)
exp_range = np.linspace(min(experience), max(experience), 20)
edu_grid, exp_grid = np.meshgrid(edu_range, exp_range)
Z = model.params[0] + model.params[1] * edu_grid + model.params[2] * exp_grid

# Plot the regression plane
ax.plot_surface(edu_grid, exp_grid, Z, alpha=0.3, color='red')

# Add labels
ax.set_xlabel('Education (years)', fontsize=12)
ax.set_ylabel('Experience (years)', fontsize=12)
ax.set_zlabel('Wage ($)', fontsize=12)

# Rotate the plot for better viewing
ax.view_init(elev=20, azim=235)

plt.tight_layout()
plt.show()
```

. . .

*> multiple regression gives each variable's effect "holding others constant"*

---

## The Multiple Regression Equation
<p class="subheader">Extending the best-fitting line to multiple dimensions</p>

<br>

. . .

**Single Variable:**

$$\text{Wage} = \beta_0 + \beta_1 \times \text{Education} + \epsilon$$

. . .

**Multiple Variables:**

$$\text{Wage} = \beta_0 + \beta_1 \times \text{Education} + \beta_2 \times \text{Experience} + \epsilon$$

. . .

**Interpretation:**

- $\beta_0$ = Base wage (intercept)
- $\beta_1$ = Effect of one more year of education, *holding experience constant*
- $\beta_2$ = Effect of one more year of experience, *holding education constant*

---

## Example: Testing with Multiple Regression
<p class="subheader">We can test individual variables or groups of variables</p>

```python
import statsmodels.formula.api as smf

# Fit multiple regression model
model = smf.ols('INCLOG10 ~ EDU + AGE', data=data).fit()
```

. . .

*> can test each one like before (t-test)*

. . .

*> "Are education AND age related to wages?"*

. . .

*> does this mean the model without `AGE` was wrong?*

. . .

*> how do we know if we've included everything?*

---

## Indicator (dummy) Variables
<p class="subheader">... we can easily turn numerical or categorical variables into indicator variables. </p>

<br>

```python
# 1. Simple binary indicator (above/below threshold)
model1 = smf.ols('INCLOG10 ~ I(EDU > 12)', data=data).fit()
```

. . .

<br>

```python
# 2. Multiple thresholds/categories
model2 = smf.ols('INCLOG10 ~ I(EDU > 12) + I(EDU < 9)', data=data).fit()
```

. . .

<br>

```python
# 3. Indicators from existing categorical variable
model3 = smf.ols('INCLOG10 ~ EDU + C(DEGFIELD) data=data).fit()
```

---

## Looking Forward
<p class="subheader">Next steps in building the general linear model...</p>

<br>

. . .

**Next topics:**

- Omitted variable bias
- Fixed effects
- Multicollinearity
- Causality
- Basic time series
- Multiple slope models
