---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader-center">The economist's data analysis skillset.</p>

<br>

### *Part 3.2 | Sampling and the Central Limit Theorem*

---

## A Big Question
<p class="subheader">If all we see is the sample, how do we learn about a population?</p>

<br><br>

. . .

- In general, a population's random variables will be unobservable.

. . .

- If we only see a sample, what *can* we say about the population?

---

## Random Variables: Known
<p class="subheader">If we know the random variable, we can learn many things about the population.</p>

:::: {.columns}
::: {.column width="40%"}
- Probability wait time < 10:
  - P(X < 10) = 0.21
- Probability wait time > 15:
  - P(X > 15) = 0.11
- Probability between 10 - 15:
  - P(10 < X < 15) = 0.59
:::

::: {.column width="60%"}
![](i/c_01.png)
:::
::::

. . .

<br>

*> when we know the probability function, we can calculate everything exactly*

---

## Random Variables: Known
<p class="subheader">If we know the random variable, we can learn many things about the population.</p>

![](i/distributions.png)

. . .

*> but what can we know about the population if we only see the sample?*

---

## Random Variables: Unknown
<p class="subheader">But if all we see is the sample, what can we know about a population?</p>

![](i/sample.png)

. . .

*> how do we learn about $\mu$ if all we have is $n$, $\bar{x}$, and $S$?*

---

## Exercise 3.2 | Sampling Dice (n=1)
<p class="subheader">Let's pretend we don't know the probability function for dice.</p>

<br>

Let's start with something simple.

<br>

1. Roll a die once (sample size: n=1).
2. We'll plot the distribution of our samples.

---

## Exercise 3.2 | Results (n=1)
<p class="subheader">Your samples have a lot of variability!</p>

![](i/c_02.png)

. . .

*> this variability perfectly matches what we would expect from a fair die*

---

## Exercise 3.2 | Sampling Dice (n=2)
<p class="subheader">Now take a sample of two rolls and compute the mean.</p>

<br>

Next is something slightly less boring.

<br>

1. Roll a die twice (sample size: n=2).
2. Calculate the mean of your two rolls.
3. We'll plot the distribution of your sample means.

---

## Exercise 3.2 | Results (n=2)
<p class="subheader">Each sample has a slightly different sample mean.</p>

![](i/c_03.png)

<br>

. . .

*> there's a lot of variability in your sample means!*

. . .

*> what do you expect to see when we plot these sample means ($\bar{x}$)?*

---

## Exercise 3.2 | Results (n=2)
<p class="subheader">The distribution of sample means bunches in the middle.</p>

![](i/c_04.png)

. . .

<br>

*> our sample means are more bunched (like a pyramid) in the middle! why?*

. . .

*> there are more ways to get 7/2 than 2/2!*

---

## Exercise 3.2 | Sampling Dice (n=3)
<p class="subheader">Now take a sample of three rolls and compute the mean.</p>

<br>

Next is something even less boring.

<br>

1. Roll a die three times (sample size: n=3).
2. Calculate the mean of your three rolls.
3. We'll plot the distribution of your sample means.

---

## Exercise 3.2 | Results (n=3)
<p class="subheader">The distribution of sample means with n=3.</p>

![](i/c_06.png)

. . .

*> what do you notice about the shape with n=3?*

---

## Exercise 3.2 | Results (n=3)
<p class="subheader">The distribution of sample means with n=3.</p>

![](i/c_07.png)

*> there's some curvature to the shape — the edges are rounding into a curve*

---

## Exercise 3.2 | Sampling Dice (n=30)
<p class="subheader">Now let's really increase the sample size.</p>

<br>

Next is something very un-boring.

<br>

1. Roll a die thirty times (sample size: n=30).
2. We'll simulate this 1,000 times and plot the distribution of sample means.

---

## Exercise 3.2 | Results (n=30)
<p class="subheader">Your individual samples each look different.</p>

![](i/c_08.png)

. . .

*> there are even more ways your sample could look!*

. . .

*> what do you expect to see when we plot these sample means ($\bar{x}$)?*

---

## Exercise 3.2 | Results (n=30)
<p class="subheader">What happens when we really increase the sample size?</p>

![](i/c_09.png)

*> the distribution of sample means gets tighter and more bell-shaped*

---

## Exercise 3.2 | Results (n=30)
<p class="subheader">What happens when we really increase the sample size?</p>

![](i/c_10.png)

. . .

*> what is this probability function in red?*

---

## The Central Limit Theorem
<p class="subheader">The distribution of sample means approximates a normal distribution as sample size increases.</p>

:::: {.columns}
::: {.column width="50%"}

$$\bar{x} \sim N\Big(\mu, \frac{\sigma}{\sqrt{n}}\Big)$$

<br>

::: {.incremental}
1. **Shape**: the sampling distribution is normal
2. **Center**: it's centered on the population mean $\mu$
3. **Spread**: the standard error $\sigma/\sqrt{n}$ shrinks with larger $n$
:::

:::

::: {.column width="50%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
n_means = 1000
sample_size = 30
sample_means = [np.mean(np.random.normal(12, 2.5, sample_size))
                for _ in range(n_means)]

plt.figure(figsize=(5, 3))
plt.hist(sample_means, bins=25, density=True, alpha=0.5)
x = np.linspace(10, 14, 1000)
y = stats.norm.pdf(x, 12, 2.5/np.sqrt(sample_size))
plt.plot(x, y, 'r-')
plt.axvline(x=12, color='blue', linestyle='--')
plt.title(f'Distribution of Sample Means (n={sample_size})')
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)

plt.tight_layout()
plt.show()
```
:::
::::

---

## The Standard Error
<p class="subheader">Where does $\sigma / \sqrt{n}$ come from?</p>

<br>

Each observation $x_i$ is drawn independently with variance $\sigma^2$, so:
$$Var(x_1 + x_2 + \cdots + x_n) = n\sigma^2$$

. . .

Dividing by $n$ divides the variance by $n^2$:
$$Var\Big(\frac{x_1 + x_2 + \cdots + x_n}{n}\Big) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$$

. . .

Take the square root:
$$SD(\bar{x}) = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$$

---

## Skewed Distributions
<p class="subheader">Does the CLT work for distributions that aren't as nice?</p>

<br><br>

**Question**: *Does the CLT still work when the population looks asymmetric?*

---

## Exercise 3.2 | Skewed Population
<p class="subheader">Simulate 1,000 sample means from a chi-squared population with n=1.</p>

<br>

. . .

```{.python}
# Simulate 1000 sample means from a skewed population
samples = stats.chi2.rvs(df=3, size=(1000, 1))
sample_means = samples.mean(axis=1)
sns.histplot(sample_means, bins=30)
```

. . .

*> with n=1, the sample means are just the raw observations*

---

## Exercise 3.2 | Skewed Population (n=1)
<p class="subheader">The distribution of sample means looks just like the population — very skewed.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
means = stats.chi2.rvs(df=3, size=(1000, 1)).mean(axis=1)

plt.figure(figsize=(9, 3))
plt.hist(means, bins=30, alpha=0.7, density=True, color='#4C72B0', label='Sample means (n=1)')
plt.yticks([])
plt.xticks([0, 3, 6, 9, 12, 15])
plt.xlabel('Value', fontsize=16)
plt.xlim(0, 15)
plt.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> now increase the sample size to n=5*

---

## Exercise 3.2 | Skewed Population (n=5)
<p class="subheader">The skew is already diminishing.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
means = stats.chi2.rvs(df=3, size=(1000, 5)).mean(axis=1)

plt.figure(figsize=(9, 3))
plt.hist(means, bins=30, alpha=0.7, density=True, color='#4C72B0', label='Sample means (n=5)')
plt.yticks([])
plt.xticks([0, 3, 6, 9, 12, 15])
plt.xlabel('Value', fontsize=16)
plt.xlim(0, 15)
plt.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> now increase to n=30*

---

## Exercise 3.2 | Skewed Population (n=30)
<p class="subheader">It looks normal — despite the skewed population.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
means = stats.chi2.rvs(df=3, size=(1000, 30)).mean(axis=1)

plt.figure(figsize=(9, 3))
plt.hist(means, bins=30, alpha=0.7, density=True, color='#4C72B0', label='Sample means (n=30)')
plt.yticks([])
plt.xticks([0, 3, 6, 9, 12, 15])
plt.xlabel('Value', fontsize=16)
plt.xlim(0, 15)
plt.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> now increase to n=1000*

---

## Exercise 3.2 | Skewed Population (n=1000)
<p class="subheader">Very tight, very normal.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
means = stats.chi2.rvs(df=3, size=(1000, 1000)).mean(axis=1)

plt.figure(figsize=(9, 3))
plt.hist(means, bins=30, alpha=0.7, density=True, color='#4C72B0', label='Sample means (n=1000)')
plt.yticks([])
plt.xticks([0, 3, 6, 9, 12, 15])
plt.xlabel('Value', fontsize=16)
plt.xlim(0, 15)
plt.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> the skew has completely disappeared*

---

## Exercise 3.2 | Skewed Population
<p class="subheader">Overlay the n=1 distribution behind the n=1000 distribution.</p>

. . .

```{.python}
# Overlay n=1 (raw population) behind n=1000 (tight, normal)
means_1 = stats.chi2.rvs(df=3, size=(1000, 1)).mean(axis=1)
means_1000 = stats.chi2.rvs(df=3, size=(1000, 1000)).mean(axis=1)

sns.histplot(means_1, bins=30, alpha=0.3, stat='density', label='Sample means (n=1)')
sns.histplot(means_1000, bins=30, alpha=0.7, stat='density', label='Sample means (n=1000)')
```

---

## Exercise 3.2 | Skewed Population
<p class="subheader">From skewed population to normal sampling distribution.</p>

<br>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
means_1 = stats.chi2.rvs(df=3, size=(1000, 1)).mean(axis=1)
means_1000 = stats.chi2.rvs(df=3, size=(1000, 1000)).mean(axis=1)

plt.figure(figsize=(9, 3))
plt.hist(means_1, bins=30, alpha=0.3, density=True, color='#4C72B0', label='Sample means (n=1)')
plt.hist(means_1000, bins=30, alpha=0.7, density=True, color='#4C72B0', edgecolor='#4C72B0', label='Sample means (n=1000)')
plt.yticks([])
plt.xticks([0, 3, 6, 9, 12, 15])
plt.xlabel('Value', fontsize=16)
plt.xlim(0, 15)
plt.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> the CLT works for (nearly) any distribution shape*

---

## Exercise 3.2 | Skewed Population
<p class="subheader">The full picture — sample means converge to normal as n increases.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

def skewed_dist(size):
    return stats.chi2.rvs(df=3, size=size)

sample_sizes = [1, 5, 30, 1000]
n_samples = 10000
fig, axes = plt.subplots(4, 1, figsize=(10, 6))

for i, n in enumerate(sample_sizes):
    sample_means = skewed_dist((n_samples, n)).mean(axis=1)

    bin_size = 30
    edge_color='white'
    if n > 100:
      bin_size = 60
      edge_color = '#4C72B0'

    axes[i].hist(sample_means, bins=bin_size, density=True, alpha=0.7,
                  edgecolor=edge_color, color='#4C72B0', label=f'Sample Mean (n={n})')
    axes[i].set_xlabel("")
    axes[i].set_xlim(0, 15)
    axes[i].set_yticks([])
    axes[i].legend()

    if n >= 5:
        x = np.linspace(min(sample_means), max(sample_means), 100)
        mean_est = 3
        std_est = np.sqrt(6/n)
        y = stats.norm.pdf(x, mean_est, std_est)
        axes[i].plot(x, y, 'r--', linewidth=2)

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

---

## Key Properties
<p class="subheader">Three things to notice about the sampling distribution of $\bar{x}$.</p>

<br>

::: {.incremental}
1. **Unbiased**: the sampling distribution is centered on the population mean $\mu$.
2. **Precise**: the standard error $\sigma/\sqrt{n}$ shrinks as sample size increases.
3. **Universal**: the shape approaches normal regardless of the population.
:::

---

## Assumptions
<p class="subheader">The CLT isn't magic. There are a few conditions.</p>

<br>

::: {.incremental}
1. **Independence**: observations don't influence each other.
2. **Identical distribution**: observations come from the same population.
3. **Sample size**: $n \geq 30$ is usually sufficient.
:::

---

## What We've Achieved
<p class="subheader">From an unobservable population to a knowable sampling distribution.</p>

<br>

::: {.incremental}
1. **Problem**: the population distribution is unobservable.
2. **Insight**: the distribution of $\bar{x}$ is knowable even when the population isn't.
3. **Implication**: that distribution is centered on $\mu$, linking sample to population.
:::

---

## Looking Ahead
<p class="subheader">We know the sampling distribution. Now what do we do with it?</p>

<br><br>

:::{.incremental}
- Part 3.3 | **Confidence Intervals** - how close is $\bar{x}$ to the true $\mu$?
- Part 3.4 | **Hypothesis Testing** - can we test whether $\mu$ equals a specific value?
:::

. . .

<br>

*> the CLT gives us the distribution — Parts 3.3 and 3.4 show us how to use it*
