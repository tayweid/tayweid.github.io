---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader-center">The economist's data analysis skillset.</p>

<br> 

### *Part 3.4 | Hypothesis Testing*

---

## A Big Question
<p class="subheader">How do we learn about the population when we don't know $\mu$ or $\sigma$?</p>

<br>

:::{.incremental}
- **Part 3.1** | Known Random Variables
    - If we know the random variable, we can answer all kinds of probability questions
- **Part 3.2** | Sampling and Unknown Random Variables
    - The sample means of unknown random variables will approximate a normal distribution around the truth
- **Part 3.3** | Confidence Intervals
    - We can use the sampling distribution to know the probability that the sample mean ($\bar{x}$) will be close to the population mean ($\mu$)
:::

---

## Sampling Distribution: Unknown $\mu$; Known $\sigma$
<p class="subheader">If we know the population mean, we know the sampling distribution is approximately normal.</p>

<br><br>

:::{.incremental}
- The sample mean is drawn from an approximately normal distribution with mean $\mu$ and standard error $\sigma / \sqrt{n}$.
- Each time we draw a sample we see a different sample mean.
- What do we do that we don't observe $\mu$? We measure 'closeness'.
:::

---

## Unknown $\mu$: Two Perspectives
<p class="subheader">There are two mathematically equivalent perspectives to think about "closeness" between $\mu$ and $\bar{x}$.</p>

. . .

Perspective 1: probability $\bar{x}$ is close to $\mu$

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# Setup
x = np.linspace(8, 16, 1000)
mu = 12
sigma = 2.5
n = 30
se = sigma/np.sqrt(n)
sample_mean = 11.6  # Example sample mean

fig, ax1 = plt.subplots(1, 1, figsize=(9, 2))

# First plot: x̄ around μ
y1 = stats.norm.pdf(x, mu, se)
ax1.plot(x, y1, 'b-', alpha=0.5)
ax1.axvline(x=mu, color='green', linestyle='--', label='$\mu$')
ax1.axvline(x=sample_mean, color='red', linestyle='--', label='$\\bar{x}$')
ax1.fill_between(x[(x >= mu-se) & (x <= mu+se)], 
                 y1[(x >= mu-se) & (x <= mu+se)], 
                 color='blue', alpha=0.3)
ax1.set_title('Confidence Interval centered on $\mu$')
ax1.set_yticks([])
ax1.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=-4, trim=True)
plt.tight_layout()
plt.show()
```

. . .

Perspective 2: probability $\mu$ is close to $\bar{x}$


```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Setup
x = np.linspace(8, 16, 1000)
mu = 12
sigma = 2.5
n = 30
se = sigma/np.sqrt(n)

fig, ax2 = plt.subplots(1, 1, figsize=(9, 2))

# Second plot: μ around x̄
sample_mean = 11.6  # Example sample mean
y2 = stats.norm.pdf(x, sample_mean, se)
ax2.plot(x, y2, 'b-', alpha=0.5)
ax2.axvline(x=mu, color='green', linestyle='--', label='$\mu$')
ax2.axvline(x=sample_mean, color='red', linestyle='--', label='$\\bar{x}$')
ax2.fill_between(x[(x >= sample_mean-se) & (x <= sample_mean+se)], 
                 y2[(x >= sample_mean-se) & (x <= sample_mean+se)], 
                 color='blue', alpha=0.3)
ax2.set_title('Confidence Interval centered on $\\bar{x}$')
ax2.set_yticks([])
ax2.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=-4, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> if $\bar{x}$ is in the CI around $\mu$, then $\mu$ will be in the CI around $\bar{x}$! *

---

## Unknown $\mu$: Two Perspectives
<p class="subheader">There are two mathematically equivalent perspectives to think about "closeness" between $\mu$ and $\bar{x}$.</p>

I repeatedly sampled a distribution and constructed a 95% confidence interval.

![](i/Confidence_Intervals_Around_Mean.png){fig-align="center"}

. . .

*> the samples with $\bar{x}$ in the CI around $\mu$ have $\mu$ in the CI around $\bar{x}$*

---

## Unknown $\mu$: Two Perspectives
<p class="subheader">There are two mathematically equivalent perspectives to think about "closeness" between $\mu$ and $\bar{x}$.</p>

I repeatedly sampled a distribution and constructed a 95% confidence interval.

![](i/Confidence_Intervals_Around_Mean.png){fig-align="center"}

*> it is mathematically equivalent to check whether $\mu$ is in the CI around $\bar{x}$!*

---

## Unknown $\mu$: How 'close' is $\mu$ to $\bar{x}?$
<p class="subheader">The distance between $\bar{x}$ and $\mu$ works both ways.</p>

Now we can use the **Sampling Distribution** around $\bar{x}$ to know the probability that $\mu$ is any distance from $\bar{x}$. 

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Setup
x = np.linspace(8, 16, 1000)
mu = 12
sigma = 2.5
n = 30
se = sigma/np.sqrt(n)

fig, ax2 = plt.subplots(1, 1, figsize=(9, 2))

# Second plot: μ around x̄
sample_mean = 11.6  # Example sample mean
y2 = stats.norm.pdf(x, sample_mean, se)
ax2.plot(x, y2, 'b-', alpha=0.5)
ax2.axvline(x=mu, color='green', linestyle='--', label='$\mu$')
ax2.axvline(x=sample_mean, color='red', linestyle='--', label='$\\bar{x}$')
ax2.fill_between(x[(x >= sample_mean-se) & (x <= sample_mean+se)], 
                 y2[(x >= sample_mean-se) & (x <= sample_mean+se)], 
                 color='blue', alpha=0.3)
ax2.set_title('Confidence Interval centered on $\\bar{x}$')
ax2.set_yticks([])
ax2.legend()

sns.despine(left=True, bottom=False, right=True, top=True, offset=-4, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> same distribution shape, just different reference points*

---

## Unknown $\mu$: How 'close' is $\mu$ to $\bar{x}?$
<p class="subheader">Each sample gives us a different $\bar{x}$ and $S$.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
fig, axes = plt.subplots(1, 5, figsize=(10, 4))

mu = 12
sigma = 2.5
n = 15  # Changed to 15
se = sigma/np.sqrt(n)  # Known sigma for sampling distribution

for i in range(5):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    
    # Strip plot at x=1
    axes[i].scatter(np.ones(n)*0.7, sample, alpha=0.6, s=30)
    axes[i].axhline(y=x_bar, color='red', linestyle='--', linewidth=1, zorder=-1, label='$\\bar{x}$' if i==0 else '')
    axes[i].axhline(y=mu, color='green', linestyle='--', linewidth=1, alpha=1, label='$\mu$' if i==0 else '')
    
    # Add sampling distribution centered on x_bar at x=0 (on y-axis)
    y_range = np.linspace(x_bar - 4*se, x_bar + 4*se, 200)
    x_dist = stats.norm.pdf(y_range, x_bar, se)
    # Scale the distribution to fit nicely on the plot
    x_dist_scaled = 0.5 * x_dist / max(x_dist)
    axes[i].plot(x_dist_scaled, y_range, 'b-', linewidth=2, alpha=0.5)
    
    # Fill confidence interval (1 SE)
    y_ci = y_range[(y_range >= x_bar-se) & (y_range <= x_bar+se)]
    x_ci = stats.norm.pdf(y_ci, x_bar, se)
    x_ci_scaled = 0.5 * x_ci / max(x_dist)
    axes[i].fill_betweenx(y_ci, 0, x_ci_scaled, alpha=0.3, color='blue')
    
    axes[i].set_xlim(0, 1)
    axes[i].set_ylim(7, 17)
    axes[i].set_xticks([])
    axes[i].set_title(f'x̄={x_bar:.1f}\ns={s:.1f}', fontsize=12)
    if i == 0:
        axes[i].set_ylabel('Wait Time (minutes)', fontsize=12)
        axes[i].legend(loc='upper left', fontsize=10)

sns.despine(left=False, bottom=True, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
```

. . .

*> notice both $\bar{x}$ (red lines) and $S$ vary across samples*

. . .

*> each sample creates its own confidence interval for where $\mu$ could be*

. . .

*> now we know the probability $\mu$ is in the CI around $\bar{x}$!*

---

## Unknown $\sigma$: How 'close' is $\mu$ to $\bar{x}?$
<p class="subheader">Each sample gives us a different $\bar{x}$ and $S$.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
fig, axes = plt.subplots(1, 5, figsize=(10, 4))

mu = 12
sigma = 2.5
n = 15  # Changed to 15
se = sigma/np.sqrt(n)  # Known sigma for sampling distribution

for i in range(5):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    
    # Strip plot at x=1
    axes[i].scatter(np.ones(n)*0.7, sample, alpha=0.6, s=30)
    axes[i].axhline(y=x_bar, color='red', linestyle='--', linewidth=1, zorder=-1, label='$\\bar{x}$' if i==0 else '')
    axes[i].axhline(y=mu, color='green', linestyle='--', linewidth=1, alpha=1, label='$\mu$' if i==0 else '')
    
    # Add sampling distribution centered on x_bar at x=0 (on y-axis)
    y_range = np.linspace(x_bar - 4*se, x_bar + 4*se, 200)
    x_dist = stats.norm.pdf(y_range, x_bar, se)
    # Scale the distribution to fit nicely on the plot
    x_dist_scaled = 0.5 * x_dist / max(x_dist)
    axes[i].plot(x_dist_scaled, y_range, 'b-', linewidth=2, alpha=0.5)
    
    # Fill confidence interval (1 SE)
    y_ci = y_range[(y_range >= x_bar-se) & (y_range <= x_bar+se)]
    x_ci = stats.norm.pdf(y_ci, x_bar, se)
    x_ci_scaled = 0.5 * x_ci / max(x_dist)
    axes[i].fill_betweenx(y_ci, 0, x_ci_scaled, alpha=0.3, color='blue')
    
    axes[i].set_xlim(0, 1)
    axes[i].set_ylim(7, 17)
    axes[i].set_xticks([])
    axes[i].set_title(f'x̄={x_bar:.1f}\ns={s:.1f}', fontsize=12)
    if i == 0:
        axes[i].set_ylabel('Wait Time (minutes)', fontsize=12)
        axes[i].legend(loc='upper left', fontsize=10)

sns.despine(left=False, bottom=True, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> but here we're creating the Confidence Intervals using a known $\sigma$, which we will never actually observe*

*> each sample has a different $S$!*

---

## Unknown $\sigma$: Variability of $S$
<p class="subheader">Just like $\bar{x}$ varies around $\mu$, the $S$ varies around $\sigma$.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(42)
n_samples = 1000
n = 15
sigma = 2.5

sample_sds = []
for _ in range(n_samples):
    sample = np.random.normal(12, sigma, n)
    sample_sds.append(np.std(sample, ddof=1))

plt.figure(figsize=(10, 3))
plt.hist(sample_sds, bins=30, density=True, alpha=0.7, color='orange')
plt.axvline(x=sigma, color='blue', linestyle='--', linewidth=2, label=f'True σ = {sigma}')
plt.axvline(x=np.mean(sample_sds), color='red', linestyle='-', linewidth=2, 
            label=f'Mean of s = {np.mean(sample_sds):.2f}')
plt.xlabel('Sample Standard Deviation', fontsize=14)
plt.ylabel('', fontsize=14)
plt.title(f'Distribution of Sample SD (n={n}, 1000 samples)', fontsize=14)
plt.legend()
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> we centered the Sampling Distribution on $\bar{x}$ instead of $\mu$*

. . .

*> what would happen if we used the $S$ in place of $\sigma$ as a guess?*

---

## Exercise 3.4 | Sampling Variation in $S$
<p class="subheader">Will a 90% confidence interval using $S$ in place of $\sigma$ correctly contain roughly 90% of the population means?</p>

. . .

Samples ($n=5$) with the sampling distribuion centered on the population mean to show the differences in each samples' spread.

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
fig, axes = plt.subplots(1, 7, figsize=(10, 3.5))

mu = 12
sigma = 2.5
n = 5  # Changed to 15

for i in range(7):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s/np.sqrt(n)
    
    # Strip plot at x=1
    axes[i].scatter(np.ones(n)*0.7, sample, alpha=0.6, s=30)
    axes[i].axhline(y=mu, color='green', linestyle='--', linewidth=1, alpha=1, label='$\mu$' if i==0 else '')
    
    # Add sampling distribution centered on x_bar at x=0 (on y-axis)
    y_range = np.linspace(x_bar - 4*se, x_bar + 4*se, 200)
    x_dist = stats.norm.pdf(y_range, mu, se)
    # Scale the distribution to fit nicely on the plot
    x_dist_scaled = 0.5 * x_dist / max(x_dist)
    axes[i].plot(x_dist_scaled, y_range, 'b-', linewidth=2, alpha=0.5)
    
    axes[i].set_xlim(0, 1)
    axes[i].set_ylim(6,18)
    axes[i].set_xticks([])
    axes[i].set_title(f'x̄={x_bar:.1f}\ns={s:.1f}', fontsize=12)
    if i == 0:
        axes[i].set_ylabel('Wait Time (minutes)', fontsize=12)
        axes[i].legend(loc='upper left', fontsize=10)

sns.despine(left=False, bottom=True, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

---

## Exercise 3.4 | Sampling Variation in $S$
<p class="subheader">Will a 90% confidence interval using $S$ in place of $\sigma$ correctly contain roughly 90% of the population means?</p>


<br><br>

Simulate many samples and check how often the 90% confidence interval contains the population mean when we simply swap $S$ for $\sigma$.

<br><br>

. . .

*> theres an additional layer of variability in the sampling distribution coming from the variability in the sample standard deviation ($S$)*

---

## Exercise 3.4 | Sampling Variation in $S$
<p class="subheader">Using the normal distribution with $S$ gives wrong coverage rates (n=15).</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(421)
n_simulations = 1000
n = 15
mu = 12
sigma = 2.5
confidence_level = 0.90

# Track coverage
normal_coverage = []
t_coverage = []

for _ in range(n_simulations):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s / np.sqrt(n)
    
    # Normal CI (incorrect)
    z_crit = stats.norm.ppf((1 + confidence_level) / 2)
    normal_ci_lower = x_bar - z_crit * se
    normal_ci_upper = x_bar + z_crit * se
    normal_coverage.append(normal_ci_lower <= mu <= normal_ci_upper)
    
    # t CI (correct)
    t_crit = stats.t.ppf((1 + confidence_level) / 2, df=n-1)
    t_ci_lower = x_bar - t_crit * se
    t_ci_upper = x_bar + t_crit * se
    t_coverage.append(t_ci_lower <= mu <= t_ci_upper)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot coverage rates
categories = ['Normal\n(too confident)', 't-distribution\n(correct)']
coverage_rates = [np.mean(normal_coverage), np.mean(t_coverage)]
colors = ['red', 'green']

bars = ax1.bar(categories, coverage_rates, color=colors, alpha=0.7)
ax1.axhline(y=0.9, color='blue', linestyle='--', label='Target: 90%')
ax1.set_ylabel('')
ax1.set_title('Coverage of 90% Confidence Intervals', fontsize=14)
ax1.set_ylim(0.8, 1.0)
ax1.legend()

# Add text labels
for bar, rate in zip(bars, coverage_rates):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{rate:.1%}', ha='center', va='bottom', fontsize=12)

# Show distribution comparison
x = np.linspace(-3, 3, 1000)
for n,a in zip([3,5,30,200],[0.1,0.3,0.6,0.8]):
    ax2.plot(x, stats.t.pdf(x, df=n-1), 'g-', label=f't (df={n-1})', alpha=a)
ax2.plot(x, stats.norm.pdf(x), 'r--', label='Normal', alpha=0.7)
ax2.set_xlabel('Standard Errors from Mean', fontsize=14)
ax2.set_ylabel('')
ax2.set_yticks([])
ax2.set_title('t-Distribution approaches Normal', fontsize=14)
ax2.legend()

sns.despine(left=False, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> we would predict 90% when the actual number is lower (87.5%)*

. . .

*> we would be **too confident** if we use the Normal with $S/\sqrt{n}$*

---

## Exercise 3.4 | Sampling Variation in $S$
<p class="subheader">Will a 90% confidence interval using $S$ in place of $\sigma$ correctly contain roughly 90% of the population means?</p>

. . .

Samples ($n=5$) with the sampling distribuion centered on the population mean to show the differences in each samples' spread.

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
fig, axes = plt.subplots(1, 7, figsize=(10, 3.5))

mu = 12
sigma = 2.5
n = 5  # Changed to 15

for i in range(7):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s/np.sqrt(n)
    
    # Strip plot at x=1
    axes[i].scatter(np.ones(n)*0.7, sample, alpha=0.6, s=30)
    axes[i].axhline(y=mu, color='green', linestyle='--', linewidth=1, alpha=1, label='$\mu$' if i==0 else '')
    
    # Add sampling distribution centered on x_bar at x=0 (on y-axis)
    y_range = np.linspace(x_bar - 4*se, x_bar + 4*se, 200)
    x_dist = stats.norm.pdf(y_range, mu, se)
    # Scale the distribution to fit nicely on the plot
    x_dist_scaled = 0.5 * x_dist / max(x_dist)
    axes[i].plot(x_dist_scaled, y_range, 'b-', linewidth=2, alpha=0.5)
    
    axes[i].set_xlim(0, 1)
    axes[i].set_ylim(6,18)
    axes[i].set_xticks([])
    axes[i].set_title(f'x̄={x_bar:.1f}\ns={s:.1f}', fontsize=12)
    if i == 0:
        axes[i].set_ylabel('Wait Time (minutes)', fontsize=12)
        axes[i].legend(loc='upper left', fontsize=10)

sns.despine(left=False, bottom=True, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

---

## Exercise 3.4 | Sampling Variation in $S$
<p class="subheader">Will a 90% confidence interval using $S$ in place of $\sigma$ correctly contain roughly 90% of the population means?</p>

Samples ($n=30$) with the sampling distribuion centered on the population mean to show the differences in each samples' spread.

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
fig, axes = plt.subplots(1, 7, figsize=(10, 3.5))

mu = 12
sigma = 2.5
n = 30  # Changed to 15

for i in range(7):
    sample = np.random.normal(mu, sigma, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s/np.sqrt(n)
    
    # Strip plot at x=1
    axes[i].scatter(np.ones(n)*0.7, sample, alpha=0.6, s=30)
    axes[i].axhline(y=mu, color='green', linestyle='--', linewidth=1, alpha=1, label='$\mu$' if i==0 else '')
    
    # Add sampling distribution centered on x_bar at x=0 (on y-axis)
    y_range = np.linspace(x_bar - 4*se, x_bar + 4*se, 200)
    x_dist = stats.norm.pdf(y_range, mu, se)
    # Scale the distribution to fit nicely on the plot
    x_dist_scaled = 0.5 * x_dist / max(x_dist)
    axes[i].plot(x_dist_scaled, y_range, 'b-', linewidth=2, alpha=0.5)
    
    axes[i].set_xlim(0, 1)
    axes[i].set_ylim(6,18)
    axes[i].set_xticks([])
    axes[i].set_title(f'x̄={x_bar:.1f}\ns={s:.1f}', fontsize=12)
    if i == 0:
        axes[i].set_ylabel('Wait Time (minutes)', fontsize=12)
        axes[i].legend(loc='upper left', fontsize=10)

sns.despine(left=False, bottom=True, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

*> as the sample size grows (now n=30), this variability gets smaller*

. . .

*> but we'll always use a t-Distribution instead of a Normal for testing*

---

## Unknown $\mu$ and $\sigma$: Building Models
<p class="subheader">What if we want to test a specific claim about the unobserved population mean?</p>

<br>

. . .

Is our data consistent with the following specific claim?

- "The mean wait time is 10 minutes."

. . .

<br>

*> instead of finding where some $\mu$ might be, we're testing a specific value of $\mu$*
---

## Example: Wait Times
<p class="subheader">If $\bar{x}=10.85$, is that consistent with $\mu_0=10$?</p>

:::: {.columns}
::: {.column width="60%"}
If sample standard deviation is $s = 2.5$:

$$SE = \frac{s}{\sqrt{n}}$$

$$SE = \frac{2.5}{\sqrt{30}}$$

$$SE = 0.456$$
:::

::: {.column width="40%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(5, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```
:::
::::

```python
s = 2.5
n = 30
se = s / np.sqrt(30)
```

---

## Example: Wait Times
<p class="subheader">The math to answer this question is identical to confidence intervals.</p>

:::: {.columns}
::: {.column width="60%"}
If sample standard deviation is $s = 2.5$:

$$SE = 0.456$$

If true mean is $\mu_0 = 10$:

$$\bar{x} \sim t_{29}(10, 0.456)$$

So the critical value for 95%:
$$t_{crit} = 2.045$$
:::

::: {.column width="40%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(5, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```
:::
::::

```python
stats.t.interval(0.95, df=30)
```

---

## Example: Wait Times
<p class="subheader">The math to answer this question is identical to confidence intervals.</p>

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(11, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```

A 95% confidence interval around $\mu_0$ would be: $[9.07, 10.93]$

. . .

*> our observed mean ($\bar{x} = 10.85$) is within this interval — not surprising if μ=10*

. . .

*> but if we observed $\bar{x} = 11.5$, that would be outside the interval — surprising!*

---

## The Null Hypothesis
<p class="subheader">We formalize this approach by setting up a "null hypothesis"</p>

<br>

. . .

**Null Hypothesis** ($H_0$): *The specific value or claim we're testing*

. . .

- $H_0: \mu = 10$ (wait time is 10 minutes)

. . .

**Alternative Hypothesis** ($H_1$ or $H_a$): *What we accept if we reject the null*

. . .

- $H_1: \mu \neq 10$ (wait time is not 10 minutes)

. . .

**Testing Approach**: 

- Calculate how "surprising" our data would be if $H_0$ were true
- If sufficiently surprising, we reject $H_0$

---

## Quantifying Surprise: p-values
<p class="subheader">The p-value measures how compatible our data is with the null hypothesis.</p>

<br>

. . .

**p-value**: *The probability of observing a test statistic at least as extreme as ours, if the null hypothesis were true*

. . .

<br>

**For our example:**

- Null: $\mu = 10$

. . .

- Observed: $\bar{x} = 10.85$

. . .

*> How likely is it to get $\bar{x}$ this far or farther from 10, if the true mean is 10?*

---

## Quantifying Surprise: p-values
<p class="subheader">Example cont.: What is the probability of an error as large as the observed mean?</p>

*> how likely is it to get $\bar{x}$ this far or farther from 10, if the true mean is 10?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.norm.pdf(x, mu_0, std_err)

# Calculate two-tailed p-value
z_score = (observed_mean - mu_0) / std_err
p_value = stats.t.cdf((mu_0-observed_mean)/se, df=29) * 2

plt.figure(figsize=(11, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

# Shade the area representing the p-value
x_right = np.linspace(observed_mean, mu_0+3*std_err, 500)
y_right = stats.norm.pdf(x_right, mu_0, std_err)
plt.fill_between(x_right, 0, y_right, color='red', alpha=0.3)

x_left = np.linspace(mu_0-3*std_err, mu_0-(observed_mean-mu_0), 500)
y_left = stats.norm.pdf(x_left, mu_0, std_err)
plt.fill_between(x_left, 0, y_left, color='red', alpha=0.3)

plt.annotate(f'p-value = {p_value:.3f}', xy=(mu_0, 0.4),
             xytext=(mu_0, 0.6), fontsize=14, ha='center',
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))

plt.title('Two-tailed Test for μ=10', fontsize=16)
plt.xlabel('Sample Mean (minutes)', fontsize=14)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=-5, trim=True)
plt.tight_layout()
plt.show()
```

. . .

```python
stats.t.cdf((mu_0-xbar)/se, df=n-1)) * 2
```

. . .

*> interpretation: if μ=10, we'd see $\bar{x}$ this far from 10 about 7.2% of the time*

. . .

*> often, we reject $H_0$ if p-value < 0.05 (5%)*

. . .

*> here, p-value > 0.05, so we don't reject the claim that μ=10*

---

## Test Statistic: The t-statistic
<p class="subheader">We can standardize our result for easier interpretation</p>

. . .

The **t-statistic** measures how many standard errors our sample mean is from the null value:

. . .

$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$$

. . .

Where:

- $\bar{x}$ is our sample mean (10.85)
- $\mu_0$ is our null value (10)
- $s$ is our sample standard deviation (2.5)
- $n$ is our sample size (30)

---

## Test Statistic: The t-statistic
<p class="subheader">We can standardize our result for easier interpretation</p>

The **t-statistic** measures how many standard errors our sample mean is from the null value:

$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} = \frac{10.85 - 10}{2.5/\sqrt{30}} = \frac{0.85}{0.456} = 1.86$$

Where:

- $\bar{x}$ is our sample mean (10.85)
- $\mu_0$ is our null value (10)
- $s$ is our sample standard deviation (2.5)
- $n$ is our sample size (30)

---

## The t-test
<p class="subheader">This example has become a formal hypothesis test.</p>

:::: {.columns}
::: {.column width="40%"}
**One-sample t-test:**

- $H_0: \mu = 10$  
- $H_1: \mu \neq 10$
- Test statistic: $t = 1.86$
- Degrees of freedom: 29
- p-value: 0.072

**Decision rule:**

- If p-value < 0.05, reject $H_0$
- Otherwise, fail to reject $H_0$
:::

::: {.column width="60%"}
```python
# Imports
import numpy as np
from scipy import stats
```

<br>

```python
# Sample Data
sample_mu = 10.85
pop_mu = 10    # null hypothesis
std_dev = 2.5    
n = 30
```

<br>

```python
# Calculate t-statistic
t_stat = (sample_mu - pop_mu) / (std_dev / np.sqrt(n))
```

<br>

```python
# Calculate p-value
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))
```
:::
::::

. . .

*> t-tests are a univariate version of regression*

---

## Statistical vs. Practical Significance
<p class="subheader">A caution about hypothesis testing</p>

<br>

. . .

**Statistical significance:**

- Formal rejection of the null hypothesis (p < 0.05)
- Only tells us if the effect is unlikely due to chance

. . .

**Practical significance:**

- Whether the effect size matters in the real world
- A statistically significant result can still be tiny

. . .

*> with large samples, even tiny differences can be statistically significant*

. . .

*> always consider the magnitude of the effect, not just the p-value*

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that $H_0$ is true

   - The p-value doesn't tell us if the null hypothesis is correct. It assumes the null is true and then calculates how surprising our result would be under that assumption.
   - *Example:* A p-value of 0.04 doesn't mean there's a 4% chance the null hypothesis is true.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that the results occurred by chance

   - All results reflect some combination of real effects and random variation. The p-value doesn't separate these components.
   - *Example:* A p-value of 0.04 doesn't mean there's a 4% chance our results are due to chance and 96% chance they're real.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that $H_1$ is true

   - The p-value doesn't directly address the alternative hypothesis or its likelihood.
   - *Example:* A p-value of 0.04 doesn't mean there's a 96% chance the alternative hypothesis is true.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

✅ **Correct:** The probability of observing a test statistic at least as extreme as ours, if $H_0$ were true

   - It measures the compatibility between our data and the null hypothesis.
   - *Example:* A p-value of 0.04 means: "If the null hypothesis were true, we'd see results this extreme or more extreme only about 4% of the time."

*> think of it like this: The p-value answers "How surprising is this data if the null hypothesis is true?" not "Is the null hypothesis true?"*

---

## Looking Forward: Bivariate GLM
<p class="subheader">This framework extends directly to regression analysis.</p>

<br>

. . .

**Next time:**

- Bivariate GLM: Comparing means between two groups

. . .

*> the hypothesis testing framework is foundational for modern science*

---

## Looking Forward: Regression
<p class="subheader">This framework extends directly to regression analysis.</p>

. . .

**Today's model**: $E[y] = \beta_0$ (just an intercept)

. . .

**Next:** $E[y] = \beta_0 + \beta_1 x$ (intercept and slope)

. . .

- Each $\beta$ coefficient will have its own t-test
- Same framework: estimate ± t-critical × SE
- The t-distribution accounts for uncertainty in our estimates

. . .

*> regression is just an extension of what we learned today*

---

## Summary
<p class="subheader">We've built the foundation for statistical modeling.</p>

<br>

:::{.incremental}
- Flipped perspective: center on what we observe ($\bar{x}$) not what's unknown ($\mu$)
- Sample SD varies, creating need for t-distribution
- Built our first model: $E[y] = \beta_0$
- Tested hypotheses by shifting data
- Connected hypothesis tests to confidence intervals
:::

. . .

<br>

*> these tools form the foundation of econometric analysis*