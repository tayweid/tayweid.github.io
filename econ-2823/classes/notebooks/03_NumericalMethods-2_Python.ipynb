{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 2 - Optimization (Python Version)\n",
    "___\n",
    "Continuing on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're continuing from last time, let me just import and redefine some of the functions from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "import utils\n",
    "\n",
    "# Set up plotting style\n",
    "utils.set_pitt_style()\n",
    "\n",
    "# Define Pitt colors for plotting\n",
    "PITT_BLUE = utils.PITT_BLUE\n",
    "PITT_GOLD = utils.PITT_GOLD\n",
    "PITT_GRAY = utils.PITT_GRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Taylor coefficient matrix for high-order derivatives\n",
    "A = np.zeros((5, 5))\n",
    "for i, offset in enumerate([-2, -1, 0, 1, 2]):\n",
    "    for j in range(5):\n",
    "        A[i, j] = (offset ** j) / np.math.factorial(j)\n",
    "\n",
    "# Reorder rows to match: [f(x+2e), f(x+e), f(x), f(x-e), f(x-2e)]\n",
    "A = A[[4, 3, 2, 1, 0], :]\n",
    "\n",
    "# Weights for first derivative (O(eps^4) method)\n",
    "b = np.array([0, 1, 0, 0, 0])\n",
    "A_inv = np.linalg.inv(A)\n",
    "w1 = b @ A_inv\n",
    "\n",
    "# Weights for second derivative (O(eps^3) method)\n",
    "b = np.array([0, 0, 1, 0, 0])\n",
    "w2 = b @ A_inv\n",
    "\n",
    "# Define high-order numerical derivative functions\n",
    "def n_deriv_1(f, x, eps=1e-6):\n",
    "    \"\"\"Five-point stencil first derivative (O(eps^4) accuracy).\"\"\"\n",
    "    return (w1[0]*f(x+2*eps) + w1[1]*f(x+eps) + w1[2]*f(x) + \n",
    "            w1[3]*f(x-eps) + w1[4]*f(x-2*eps)) / eps\n",
    "\n",
    "def n_deriv_2(f, x, eps=1e-3):\n",
    "    \"\"\"Five-point stencil second derivative (O(eps^3) accuracy).\"\"\"\n",
    "    return (w2[0]*f(x+2*eps) + w2[1]*f(x+eps) + w2[2]*f(x) + \n",
    "            w2[3]*f(x-eps) + w2[4]*f(x-2*eps)) / eps**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "**Checklist:**\n",
    "- [x] Numerical derivatives\n",
    "- [x] Numerical solutions to equations\n",
    "- [ ] Numerical Optimization\n",
    "\n",
    "We're going to show how numerical optimization works within:\n",
    "- one dimension\n",
    "- with a continuous function\n",
    "\n",
    "If the problem is smooth then we can try to use the same **first-order conditions** that you've used in the theoretical parts of the course. A necessary condition to be at an optimal point for a continuous differentiable function is that the slope is zero:\n",
    "$$ f^\\prime(x)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Use Analytical Derivatives\n",
    "\n",
    "Just code up the analytical derivative, and solve for its roots!\n",
    "\n",
    "Define our test functions:\n",
    "- $ f(x)= 2 x^2 - 10 x + 3 $ \n",
    "- $g(x)=f(x)e^{-x/10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def f(x):\n",
    "    return 2*x**2 - 10*x + 3\n",
    "\n",
    "def g(x):\n",
    "    return f(x) * np.exp(-x/10)\n",
    "\n",
    "# Analytical derivatives\n",
    "def d_f(x):\n",
    "    return 4*x - 10\n",
    "\n",
    "def d_g(x):\n",
    "    return -0.1 * np.exp(-0.1*x) * f(x) + np.exp(-0.1*x) * (4*x - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton-Raphson for root finding\n",
    "def newton_raphson(func_in, x0, tol=1e-8, eps=1e-6, output=False):\n",
    "    \"\"\"\n",
    "    Newton-Raphson root finding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func_in : callable\n",
    "        Function to find root of\n",
    "    x0 : float\n",
    "        Initial guess\n",
    "    tol : float\n",
    "        Tolerance\n",
    "    eps : float\n",
    "        Step size for numerical derivative\n",
    "    output : bool\n",
    "        Whether to print progress\n",
    "    \"\"\"\n",
    "    xx = x0\n",
    "    fval = func_in(xx)\n",
    "    error = abs(func_in(xx))\n",
    "    ii = 1\n",
    "    \n",
    "    while error > tol:\n",
    "        fd = n_deriv_1(func_in, xx, eps)\n",
    "        xx = xx - fval / fd\n",
    "        fval = func_in(xx)\n",
    "        error = abs(fval)\n",
    "        ii += 1\n",
    "    \n",
    "    if output:\n",
    "        print(f\"Converged to solution {xx:.5f} in {ii-1} steps\")\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot derivative of f(x) to find where f'(x) = 0\n",
    "x = np.linspace(0, 5, 200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x, d_f(x), color=PITT_BLUE, linewidth=2, label=\"f'(x)\")\n",
    "ax.axhline(y=0, color=PITT_GOLD, linewidth=1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x)\")\n",
    "ax.set_title(\"Derivative of f(x) = 2x² - 10x + 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal by solving f'(x) = 0\n",
    "f_sol = newton_raphson(d_f, 1, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the medium function g(x)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(0, 40, 200)\n",
    "ax.plot(x, d_g(x), color=PITT_BLUE, linewidth=2, label=\"g'(x)\")\n",
    "ax.axhline(y=0, color=PITT_GOLD, linewidth=1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"g'(x)\")\n",
    "ax.set_title(\"Derivative of g(x) = (2x² - 10x + 3)exp(-x/10)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find both solutions\n",
    "g_sol_1 = newton_raphson(d_g, 0, output=True)\n",
    "g_sol_2 = newton_raphson(d_g, 20, output=True)\n",
    "\n",
    "print(f\"\\ng({g_sol_1:.5f}) = {g(g_sol_1):.5f}\")\n",
    "print(f\"g({g_sol_2:.5f}) = {g(g_sol_2):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Fully Numeric Optimization\n",
    "\n",
    "Suppose we don't want to even think about the derivatives, then we can just go fully numeric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson_fullnumeric(f, x0, tol=1e-8, eps=1e-6, maxiter=50, output=False):\n",
    "    \"\"\"\n",
    "    Fully numeric optimization using Newton-Raphson.\n",
    "    Solves f'(x) = 0 using numerical derivatives.\n",
    "    \"\"\"\n",
    "    xx = x0\n",
    "    # Numerical first derivative\n",
    "    fval = n_deriv_1(f, xx, eps)\n",
    "    error = abs(fval)\n",
    "    ii = 1\n",
    "    \n",
    "    while error > tol and ii < maxiter:\n",
    "        # Numerical second derivative\n",
    "        fd = n_deriv_2(f, xx, np.sqrt(eps))\n",
    "        # Newton-Raphson: x1 = x0 - f'(x0)/f''(x0)\n",
    "        xx = xx - fval / fd\n",
    "        fval = n_deriv_1(f, xx, eps)\n",
    "        error = abs(fval)\n",
    "        ii += 1\n",
    "    \n",
    "    if ii >= maxiter:\n",
    "        print(\"Exited due to non-convergence\")\n",
    "    if output and ii < maxiter:\n",
    "        print(f\"Converged to solution {xx:.5f} in {ii-1} steps\")\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the easy function\n",
    "f_sol_nd = newton_raphson_fullnumeric(f, x0=1, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the medium function\n",
    "g_sol_1_nd = newton_raphson_fullnumeric(g, 0, output=True)\n",
    "g_sol_2_nd = newton_raphson_fullnumeric(g, 20, output=True)\n",
    "\n",
    "# Differences from analytical method?\n",
    "print(f\"\\nDifference from analytical: {g_sol_1 - g_sol_1_nd:.2e}, {g_sol_2 - g_sol_2_nd:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of Optima: Maxima vs Minima\n",
    "\n",
    "Both local minima and local maxima satisfy the $f^\\prime(x)=0$ condition. We can figure out which type by checking the second derivative:\n",
    "\n",
    "* For a **maximum**: $f^{\\prime\\prime}(x)<0$ (slope decreasing)\n",
    "* For a **minimum**: $f^{\\prime\\prime}(x)>0$ (slope increasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check second-order conditions at each solution\n",
    "soc_1 = n_deriv_2(g, g_sol_1, 1e-5)\n",
    "soc_2 = n_deriv_2(g, g_sol_2, 1e-5)\n",
    "\n",
    "print(f\"Solution 1 (x = {g_sol_1:.5f}): f''(x) = {soc_1:.5f}\")\n",
    "print(f\"  -> {'LOCAL MINIMUM' if soc_1 > 0 else 'LOCAL MAXIMUM'}\")\n",
    "\n",
    "print(f\"\\nSolution 2 (x = {g_sol_2:.5f}): f''(x) = {soc_2:.5f}\")\n",
    "print(f\"  -> {'LOCAL MINIMUM' if soc_2 > 0 else 'LOCAL MAXIMUM'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the solutions\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(g_sol_1 - 5, g_sol_2 + 5, 200)\n",
    "\n",
    "ax.plot(x, g(x), color=PITT_BLUE, linewidth=2, label='g(x)')\n",
    "ax.axhline(y=g(g_sol_1), color=PITT_GOLD, linewidth=1, alpha=0.7)\n",
    "ax.axhline(y=g(g_sol_2), color=PITT_GOLD, linewidth=1, alpha=0.7)\n",
    "ax.scatter([g_sol_1, g_sol_2], [g(g_sol_1), g(g_sol_2)], \n",
    "           color='black', s=100, zorder=5)\n",
    "\n",
    "ax.annotate(f'Min: ({g_sol_1:.2f}, {g(g_sol_1):.2f})', \n",
    "            (g_sol_1, g(g_sol_1)), xytext=(10, -20), textcoords='offset points')\n",
    "ax.annotate(f'Max: ({g_sol_2:.2f}, {g(g_sol_2):.2f})', \n",
    "            (g_sol_2, g(g_sol_2)), xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('g(x)')\n",
    "ax.set_title('Illustrating Local Optima')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global vs Local Optima\n",
    "\n",
    "Only one of these is a **global** optimum. Let's zoom out to see the bigger picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom out\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(-5, 105, 500)\n",
    "\n",
    "ax.plot(x, g(x), color=PITT_BLUE, linewidth=2)\n",
    "ax.axhline(y=g(g_sol_1), color=PITT_GOLD, linewidth=1, alpha=0.7, label=f'g(min) = {g(g_sol_1):.2f}')\n",
    "ax.axhline(y=g(g_sol_2), color=PITT_GOLD, linewidth=1, alpha=0.7, label=f'g(max) = {g(g_sol_2):.2f}')\n",
    "ax.axhline(y=0, color=PITT_GOLD, linewidth=1, linestyle='--', alpha=0.5)\n",
    "ax.scatter([g_sol_1, g_sol_2], [g(g_sol_1), g(g_sol_2)], color='black', s=100, zorder=5)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('g(x)')\n",
    "ax.set_title('Local vs. Global Optima')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Solutions: The Challenge\n",
    "\n",
    "When minimizing a function that is not single-peaked/troughed, gradient methods can get stuck at local solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wavy function with many local optima\n",
    "def wavy_f(x):\n",
    "    return ((-(x-2)**2) * np.cos(x-2) - \n",
    "            90 * np.cos(20*(x-2)) * np.exp(-30*(x-2)**4) + \n",
    "            np.exp((x-2)**2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(0, 4, 500)\n",
    "ax.plot(x, wavy_f(x), color=PITT_BLUE, linewidth=2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('A Function with Many Local Optima')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different starting points lead to different solutions\n",
    "starts = [0, 1.6, 2.05, 2.1, 2.3]\n",
    "for x0 in starts:\n",
    "    sol = newton_raphson_fullnumeric(wavy_f, x0, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional Optimization\n",
    "\n",
    "Consider the function:\n",
    "$$ f(x_1,x_2)=2 x_1^2+2 x_2^2-2 x_1 x_2 - 5x_1+x_2$$\n",
    "\n",
    "The first-order conditions give us:\n",
    "$$\\begin{array}{rccl} \n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_1} & = & 4 x_1- 2 x_2 -5&=0 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_2} & = & 4 x_2- 2 x_1 +1 &=0 \n",
    "\\end{array}$$\n",
    "\n",
    "The unique minimizing solution is: $x_1^\\star=\\tfrac{3}{2}$ and $x_2^\\star=\\tfrac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function of 2 inputs\n",
    "def f2(x):\n",
    "    return 2*x[0]**2 + 2*x[1]**2 - 2*x[0]*x[1] - 5*x[0] + x[1]\n",
    "\n",
    "print(f\"f2([0, 0]) = {f2([0, 0])}\")\n",
    "print(f\"f2([1.5, 0.5]) = {f2([1.5, 0.5])}\")\n",
    "print(f\"f2([1.6, 0.4]) = {f2([1.6, 0.4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional Second-Order Conditions\n",
    "\n",
    "For multi-dimensional problems, we need:\n",
    "* The **gradient vector** of partial derivatives: $\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1}, \\ldots ,\\frac{\\partial f}{\\partial x_n}\\right)^T$\n",
    "* The **Hessian matrix** of second-order derivatives\n",
    "\n",
    "The second-order conditions check whether the Hessian is:\n",
    "* **Positive definite** (all eigenvalues > 0) → local **minimum**\n",
    "* **Negative definite** (all eigenvalues < 0) → local **maximum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization in Python with SciPy\n",
    "\n",
    "Python's `scipy.optimize` module provides optimization routines similar to R's `optim()`.\n",
    "\n",
    "**Key mapping:**\n",
    "| R | Python |\n",
    "|---|--------|\n",
    "| `optim(x0, f, method=\"BFGS\")` | `optimize.minimize(f, x0, method='BFGS')` |\n",
    "| `optim(..., hessian=TRUE)` | `optimize.minimize(..., options={'return_hess': True})` |\n",
    "| `optimize(f, interval=c(a,b))` | `optimize.minimize_scalar(f, bounds=(a,b), method='bounded')` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the 2D function using BFGS\n",
    "# R: optim(c(0,0), f2, method=\"BFGS\", hessian=TRUE)\n",
    "# Python:\n",
    "result = optimize.minimize(f2, [0, 0], method='BFGS')\n",
    "\n",
    "print(\"Optimization Result:\")\n",
    "print(f\"  Optimal x: {result.x}\")\n",
    "print(f\"  Optimal value: {result.fun}\")\n",
    "print(f\"  Function evaluations: {result.nfev}\")\n",
    "print(f\"  Gradient evaluations: {result.njev}\")\n",
    "print(f\"  Success: {result.success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the Hessian, we can compute it numerically\n",
    "hess = utils.numerical_hessian(f2, result.x)\n",
    "\n",
    "print(\"Hessian at optimal point:\")\n",
    "print(hess)\n",
    "\n",
    "# Check if positive definite (all eigenvalues > 0 means minimum)\n",
    "eigenvalues = np.linalg.eigvals(hess)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"Is positive definite (minimum)? {np.all(eigenvalues > 0)}\")\n",
    "print(f\"Is negative definite (maximum)? {np.all(eigenvalues < 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Solutions in Multiple Dimensions\n",
    "\n",
    "The problem of getting stuck at local solutions doesn't go away with more dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D version of our wavy function\n",
    "def wavy_f_2d(x):\n",
    "    return (wavy_f(x[0]) + 90) * (1 + (x[1] - 3)**2) / 100\n",
    "\n",
    "# BFGS may find a local minimum\n",
    "result_bfgs = optimize.minimize(wavy_f_2d, [0, 0], method='BFGS')\n",
    "\n",
    "print(\"BFGS result:\")\n",
    "print(f\"  Found: {result_bfgs.x}\")\n",
    "print(f\"  Value: {result_bfgs.fun}\")\n",
    "\n",
    "# Actual global minimum\n",
    "print(f\"\\nGlobal minimum at [2, 3]: {wavy_f_2d([2, 3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Gradient Methods: Nelder-Mead\n",
    "\n",
    "The default method in R's `optim()` (Nelder-Mead) does not use gradients at all. Instead, it creates a simplex mesh.\n",
    "\n",
    "A nice visualization: https://www.benfrederickson.com/numerical-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nelder-Mead in Python\n",
    "# R: optim(c(0,0), wavy.f.dim2)\n",
    "# Python:\n",
    "result_nm = optimize.minimize(wavy_f_2d, [0, 0], method='Nelder-Mead')\n",
    "\n",
    "print(\"Nelder-Mead result:\")\n",
    "print(f\"  Found: {result_nm.x}\")\n",
    "print(f\"  Value: {result_nm.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1D optimization, use minimize_scalar\n",
    "# R: optimize(g, interval=c(-10, 5))\n",
    "# Python:\n",
    "result_1d = optimize.minimize_scalar(g, bounds=(-10, 5), method='bounded')\n",
    "\n",
    "print(\"1D optimization result:\")\n",
    "print(f\"  Minimum at x = {result_1d.x:.6f}\")\n",
    "print(f\"  Value: {result_1d.fun:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a local minimum in the wavy function\n",
    "result_wavy = optimize.minimize_scalar(wavy_f, bounds=(1.5, 2.5), method='bounded')\n",
    "\n",
    "print(f\"Wavy function minimum: x = {result_wavy.x:.6f}\")\n",
    "print(f\"Value: {result_wavy.fun:.6f}\")\n",
    "print(f\"Compare to wavy_f(2) = {wavy_f(2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing and Global Optimization\n",
    "\n",
    "When we're concerned about reaching global optima, **simulated annealing** is a stochastic method that can help escape local minima.\n",
    "\n",
    "In scipy, we can use `optimize.dual_annealing()` or `optimize.basinhopping()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual annealing for global optimization\n",
    "# R: optim(c(0,0), wavy.f.dim2, method=\"SANN\")\n",
    "# Python:\n",
    "result_da = optimize.dual_annealing(wavy_f_2d, bounds=[(-5, 10), (-5, 10)])\n",
    "\n",
    "print(\"Dual Annealing result:\")\n",
    "print(f\"  Found: {result_da.x}\")\n",
    "print(f\"  Value: {result_da.fun}\")\n",
    "print(f\"  Function evaluations: {result_da.nfev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basin-hopping: combines local optimization with random restarts\n",
    "result_bh = optimize.basinhopping(wavy_f_2d, [0, 0], niter=100)\n",
    "\n",
    "print(\"Basin-Hopping result:\")\n",
    "print(f\"  Found: {result_bh.x}\")\n",
    "print(f\"  Value: {result_bh.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining approaches: global search followed by local refinement\n",
    "result_global = optimize.dual_annealing(wavy_f_2d, bounds=[(-5, 10), (-5, 10)])\n",
    "result_refined = optimize.minimize(wavy_f_2d, result_global.x, method='BFGS')\n",
    "\n",
    "print(\"Combined approach:\")\n",
    "print(f\"  After global search: {result_global.x}, value = {result_global.fun}\")\n",
    "print(f\"  After refinement:    {result_refined.x}, value = {result_refined.fun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-uniqueness and Identification\n",
    "\n",
    "Sometimes the objective does not have a unique maximizer. Consider:\n",
    "$$ f(x_1,x_2)=-(x_1+x_2-3)^2$$\n",
    "\n",
    "Any value of $(x_1,x_2)$ that sums to 3 will minimize it. This relates to **identification problems** in estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-unique optimizer\n",
    "def f_saddle(x):\n",
    "    return (x[0] + x[1] - 3)**2 + (x[0]**2) * 0.000001\n",
    "\n",
    "result = optimize.minimize(f_saddle, [0, 0], method='BFGS')\n",
    "print(f\"Found: {result.x}\")\n",
    "print(f\"Value: {result.fun}\")\n",
    "print(f\"Sum of x: {sum(result.x)}\")\n",
    "\n",
    "# Check the Hessian\n",
    "hess = utils.numerical_hessian(f_saddle, result.x)\n",
    "print(f\"\\nHessian:\\n{hess}\")\n",
    "print(f\"Eigenvalues: {np.linalg.eigvals(hess)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: R to Python Optimization Mapping\n",
    "\n",
    "| R Function | Python Equivalent |\n",
    "|------------|-------------------|\n",
    "| `optim(x0, f, method=\"BFGS\")` | `scipy.optimize.minimize(f, x0, method='BFGS')` |\n",
    "| `optim(x0, f, method=\"Nelder-Mead\")` | `scipy.optimize.minimize(f, x0, method='Nelder-Mead')` |\n",
    "| `optim(x0, f, method=\"SANN\")` | `scipy.optimize.dual_annealing(f, bounds)` |\n",
    "| `optim(x0, f, method=\"Brent\", lower=a, upper=b)` | `scipy.optimize.minimize_scalar(f, bounds=(a,b), method='bounded')` |\n",
    "| `optimize(f, interval=c(a,b))` | `scipy.optimize.minimize_scalar(f, bounds=(a,b), method='bounded')` |\n",
    "| `is.positive.definite(H)` | `np.all(np.linalg.eigvals(H) > 0)` |\n",
    "\n",
    "**Key Notes:**\n",
    "- R's `optim()` minimizes by default; to maximize, multiply function by -1\n",
    "- Python's `scipy.optimize.minimize` also minimizes by default\n",
    "- For maximization in Python: `minimize(lambda x: -f(x), x0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Maximizing g(x) to find the maximum\n",
    "result_max = optimize.minimize(lambda x: -g(x), 20)  # Negate for maximization\n",
    "\n",
    "print(f\"Maximum of g(x):\")\n",
    "print(f\"  x = {result_max.x[0]:.6f}\")\n",
    "print(f\"  g(x) = {g(result_max.x[0]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Shared Utils Module\n",
    "\n",
    "The `utils.py` module provides several helpful functions for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical gradient (for multivariate functions)\n",
    "grad = utils.numerical_gradient(f2, [1.5, 0.5])\n",
    "print(f\"Gradient at optimal: {grad}\")\n",
    "print(f\"(Should be close to [0, 0] at the optimum)\")\n",
    "\n",
    "# Numerical Hessian\n",
    "hess = utils.numerical_hessian(f2, [1.5, 0.5])\n",
    "print(f\"\\nHessian at optimal:\\n{hess}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
