{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Censoring, and Selection (Python Version)\n",
    "---\n",
    "We now focus on using maximum likelihood techniques to understanding some additional violations of the standard model that can cause bias.\n",
    "* **Censoring**: whereby data above or below a certain value is measured only at the limit\n",
    "* **Selection**: where the participants in our dataset are non-randomly selected, and so may be unrepresentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import optimize, stats\n",
    "import utils\n",
    "\n",
    "# Set up plotting style\n",
    "utils.set_pitt_style()\n",
    "PITT_BLUE = utils.PITT_BLUE\n",
    "PITT_GOLD = utils.PITT_GOLD\n",
    "PITT_GRAY = utils.PITT_GRAY\n",
    "PITT_LGRAY = utils.PITT_LGRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Censoring\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "Suppose that we are modelling the effect of variable $x$ on outcome $y$, where we'll use the classical linear model:\n",
    "$$y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i$$\n",
    "where $\\epsilon_i\\sim \\mathcal{N}(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "So this would be the standard regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "So the values we want to recover here are: $\\beta_0=5000$ and $\\beta_1=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: x <- 100*rchisq(10000, df=2)\n",
    "# Python: np.random.chisquare(df, size) * 100\n",
    "np.random.seed(42)\n",
    "\n",
    "beta0 = 5000\n",
    "beta1 = 2\n",
    "n = 10000\n",
    "\n",
    "x = 100 * np.random.chisquare(df=2, size=n)\n",
    "epsilon = np.random.normal(0, 400, size=n)\n",
    "y = beta0 + beta1 * x + epsilon\n",
    "\n",
    "data = pd.DataFrame({'y': y, 'x': x})\n",
    "\n",
    "# OLS regression\n",
    "X_ols = sm.add_constant(data['x'])\n",
    "ols_result = sm.OLS(data['y'], X_ols).fit()\n",
    "print(ols_result.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data with fitted line\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.68))\n",
    "ax.scatter(data['x'], data['y'], s=0.5, color=PITT_GOLD, alpha=0.7)\n",
    "\n",
    "# Add OLS fit line\n",
    "x_range = np.linspace(data['x'].min(), data['x'].max(), 100)\n",
    "y_fit = ols_result.params[0] + ols_result.params[1] * x_range\n",
    "ax.plot(x_range, y_fit, color=PITT_BLUE, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Standard OLS Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "So here we do pretty well at estimating the effect of the variable $x$ on $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "However, suppose that due to data collection limits, we could only measure $y$ up to $\\overline{y}=6000$, and for values above this they are *top-coded* at this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: data['y.censored'] <- ifelse(y > y.upper, y.upper, y)\n",
    "# Python: np.where(condition, value_if_true, value_if_false)\n",
    "y_upper = 6000\n",
    "data['y_censored'] = np.where(data['y'] > y_upper, y_upper, data['y'])\n",
    "\n",
    "# OLS on censored data\n",
    "ols_censored = sm.OLS(data['y_censored'], X_ols).fit()\n",
    "print(ols_censored.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "Now we've substantially underestimated the relationship between y and x, where we can see why when we consider the effect of the truncation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10/1.68))\n",
    "\n",
    "# Plot censored data\n",
    "uncensored_mask = data['y'] <= y_upper\n",
    "ax.scatter(data.loc[uncensored_mask, 'x'], data.loc[uncensored_mask, 'y_censored'],\n",
    "           s=0.5, color=PITT_GOLD, alpha=0.7)\n",
    "ax.scatter(data.loc[~uncensored_mask, 'x'], data.loc[~uncensored_mask, 'y_censored'],\n",
    "           s=0.5, color='red', alpha=0.7)\n",
    "\n",
    "# OLS on censored data (red line)\n",
    "y_fit_censored = ols_censored.params[0] + ols_censored.params[1] * x_range\n",
    "ax.plot(x_range, y_fit_censored, color='red', linewidth=2, label='OLS on censored')\n",
    "\n",
    "# Original OLS (blue line)\n",
    "y_fit_orig = ols_result.params[0] + ols_result.params[1] * x_range\n",
    "ax.plot(x_range, y_fit_orig, color=PITT_BLUE, linewidth=2, label='OLS on original')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y (censored)')\n",
    "ax.set_title('Effect of Top-Coding on OLS')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "The violation of the assumption here is that if the true relationship is:\n",
    "$$ y=\\beta_0+\\beta_1 x+\\epsilon$$\n",
    "and we use the censored variable $y_C$ then we effectively have the relationship:\n",
    "$$y_C=\\begin{cases}\n",
    " \\beta_0+\\beta_1\\cdot x_U+\\epsilon & \\text{if }\\epsilon<\\overline{y}-\\beta_0-\\beta_1 x, \\\\\n",
    " \\overline{y} & \\text{otherwise.}\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "If we wanted to estimate $\\beta_1$ without bias, we would require that the expected error conditional on all values of $x$ is zero, but that can't be true here as the expected error on the top-coded variable is positive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "One way to proceed here would be to remove all of the top-coded observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS excluding censored observations\n",
    "subset = data['y_censored'] < y_upper\n",
    "ols_truncated = sm.OLS(data.loc[subset, 'y_censored'],\n",
    "                       sm.add_constant(data.loc[subset, 'x'])).fit()\n",
    "print(ols_truncated.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "But here we're still making a mistake, as the *expected* error is now negative (increasingly so as $x$ gets larger) as we're truncating the sample to remove data points with large errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10/1.68))\n",
    "\n",
    "ax.scatter(data['x'], data['y_censored'], s=0.5, color=PITT_GOLD, alpha=0.7)\n",
    "\n",
    "# OLS on truncated sample (red line)\n",
    "y_fit_trunc = ols_truncated.params[0] + ols_truncated.params[1] * x_range\n",
    "ax.plot(x_range, y_fit_trunc, color='red', linewidth=2, label='OLS (drop censored)')\n",
    "\n",
    "# OLS on full censored data (blue line)\n",
    "ax.plot(x_range, y_fit_censored, color=PITT_BLUE, linewidth=2, label='OLS (all censored)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y (censored)')\n",
    "ax.set_title('Dropping Censored Observations Still Biased')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "The other option we have is to model the censoring. The starting point for much of this is what's called a Tobit regression. We will make the classical linear model assumption that the errors are normally distributed. However, we will now recognize the fact that data points at the upper limit are top coded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "The density on points which are uncensored are therefore given by the standard normal PDF:\n",
    "$$\\phi\\left(\\frac{y_i-\\beta_0-\\beta_1 x_i}{\\sigma}\\right)$$\n",
    "This is the exactly the same as the max-likelihood version of the classical linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "The difference is that values at the upper limit $\\overline{y}$ we recognize that all values of $\\epsilon$ larger than a cutoff could have caused this. The likelihood for these data points is therefore given by the upper tail of the normal via:\n",
    "$$\\left(1-\\Phi\\left( \\frac{\\overline{y}-\\beta_0-\\beta_1 x_i}{\\sigma} \\right)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "Similarly, if we had bottom-coded variables, so left-censored at some point $\\underline{y}$, then we would model the left tail of normal via:\n",
    "$$ \\Phi\\left( \\frac{\\underline{y}-\\beta_0-\\beta_1 x_i}{\\sigma} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "Given left and right censoring at $\\underline{y}$ and $\\overline{y}$, respectively, the log-likelihood for the entire data is given by:\n",
    "$$\\begin{eqnarray}\\log \\mathcal{L}(\\beta_0,\\beta_1,\\sigma;y,x)=\n",
    "\\sum_{\\underline{y}<y_i<\\overline{y}}\\log\\phi\\left(\\frac{y_i-\\beta_0-\\beta_1 x_i}{\\sigma}\\right)+\\\\\n",
    "\\sum_{\\underline{y}\\geq y_i}\\log \\Phi\\left(\\frac{y_i-\\beta_0-\\beta_1 x_i}{\\sigma}\\right)+\\\\\n",
    "\\sum_{y_i \\geq  \\overline{y} }\\log\\left( 1-\\Phi\\left(\\frac{y_i-\\beta_0-\\beta_1 x_i}{\\sigma}\\right)\\right),\\end{eqnarray}\n",
    "$$\n",
    "and we could find the max-likelihood estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ using the previous methods (and the standard deviation $\\hat{\\sigma}$ and standard errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Tobit Model: Custom MLE in Python\n",
    "\n",
    "In R, we used `tobit()` from the `AER` package. In Python, there is no standard built-in Tobit, so we implement it ourselves using `scipy.optimize`.\n",
    "\n",
    "**Key R to Python mappings:**\n",
    "| R | Python |\n",
    "|---|--------|\n",
    "| `tobit(y ~ x, left=L, right=R)` | Custom MLE with `scipy.optimize.minimize` |\n",
    "| `pnorm(x)` | `scipy.stats.norm.cdf(x)` |\n",
    "| `dnorm(x)` | `scipy.stats.norm.pdf(x)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tobit_loglik(params, y, X, left=-np.inf, right=np.inf):\n",
    "    \"\"\"\n",
    "    Log-likelihood for the Tobit model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array-like\n",
    "        [beta_0, beta_1, ..., beta_k, log_sigma]\n",
    "        Note: we estimate log(sigma) to ensure sigma > 0\n",
    "    y : array-like\n",
    "        Dependent variable (possibly censored)\n",
    "    X : ndarray\n",
    "        Design matrix (including constant)\n",
    "    left : float\n",
    "        Left censoring point (-np.inf for no left censoring)\n",
    "    right : float\n",
    "        Right censoring point (np.inf for no right censoring)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    beta = params[:-1]\n",
    "    sigma = np.exp(params[-1])  # ensure positive\n",
    "    xb = X @ beta\n",
    "    \n",
    "    ll = 0.0\n",
    "    \n",
    "    # Uncensored observations\n",
    "    uncensored = (y > left) & (y < right)\n",
    "    if np.any(uncensored):\n",
    "        # R: dnorm((y - xb) / sigma) / sigma  =  norm.logpdf(y, xb, sigma)\n",
    "        ll += np.sum(stats.norm.logpdf(y[uncensored], xb[uncensored], sigma))\n",
    "    \n",
    "    # Left-censored observations\n",
    "    left_censored = (y <= left)\n",
    "    if np.any(left_censored):\n",
    "        # R: pnorm((left - xb) / sigma)\n",
    "        ll += np.sum(np.log(np.maximum(stats.norm.cdf((left - xb[left_censored]) / sigma), 1e-300)))\n",
    "    \n",
    "    # Right-censored observations\n",
    "    right_censored = (y >= right)\n",
    "    if np.any(right_censored):\n",
    "        # R: 1 - pnorm((right - xb) / sigma)\n",
    "        ll += np.sum(np.log(np.maximum(1 - stats.norm.cdf((right - xb[right_censored]) / sigma), 1e-300)))\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tobit(y, X, left=-np.inf, right=np.inf):\n",
    "    \"\"\"\n",
    "    Fit a Tobit model via maximum likelihood.\n",
    "    \n",
    "    R equivalent: tobit(y ~ x, left=L, right=R)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        Dependent variable\n",
    "    X : ndarray\n",
    "        Design matrix (including constant)\n",
    "    left : float\n",
    "        Left censoring point\n",
    "    right : float\n",
    "        Right censoring point\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Estimation results\n",
    "    \"\"\"\n",
    "    # Initial values from OLS\n",
    "    ols = sm.OLS(y, X).fit()\n",
    "    init_beta = ols.params\n",
    "    init_log_sigma = np.log(np.std(ols.resid))\n",
    "    x0 = np.concatenate([init_beta, [init_log_sigma]])\n",
    "    \n",
    "    # Maximize log-likelihood (minimize negative log-likelihood)\n",
    "    result = optimize.minimize(\n",
    "        lambda p: -tobit_loglik(p, y, X, left, right),\n",
    "        x0,\n",
    "        method='BFGS'\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    params = result.x\n",
    "    beta = params[:-1]\n",
    "    log_sigma = params[-1]\n",
    "    sigma = np.exp(log_sigma)\n",
    "    loglik = -result.fun\n",
    "    \n",
    "    # Standard errors from inverse Hessian\n",
    "    se = utils.mle_standard_errors(\n",
    "        lambda p: tobit_loglik(p, y, X, left, right), params\n",
    "    )\n",
    "    \n",
    "    # Count observations\n",
    "    n_left = np.sum(y <= left) if left > -np.inf else 0\n",
    "    n_right = np.sum(y >= right) if right < np.inf else 0\n",
    "    n_uncensored = len(y) - n_left - n_right\n",
    "    \n",
    "    return {\n",
    "        'beta': beta,\n",
    "        'log_sigma': log_sigma,\n",
    "        'sigma': sigma,\n",
    "        'se': se,\n",
    "        'loglik': loglik,\n",
    "        'n_total': len(y),\n",
    "        'n_left': n_left,\n",
    "        'n_uncensored': n_uncensored,\n",
    "        'n_right': n_right,\n",
    "        'converged': result.success\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tobit_summary(result, var_names=None):\n",
    "    \"\"\"\n",
    "    Print a summary table similar to R's tobit output.\n",
    "    \"\"\"\n",
    "    if var_names is None:\n",
    "        var_names = [f'beta_{i}' for i in range(len(result['beta']))]\n",
    "    \n",
    "    print(\"Tobit Model (MLE)\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Observations:\")\n",
    "    print(f\"  Total: {result['n_total']}  Left-censored: {result['n_left']}  \"\n",
    "          f\"Uncensored: {result['n_uncensored']}  Right-censored: {result['n_right']}\")\n",
    "    print()\n",
    "    print(f\"{'':>15s} {'Estimate':>12s} {'Std. Error':>12s} {'z value':>10s} {'Pr(>|z|)':>10s}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    all_params = np.concatenate([result['beta'], [result['log_sigma']]])\n",
    "    all_names = var_names + ['Log(sigma)']\n",
    "    all_se = result['se']\n",
    "    \n",
    "    for name, est, se in zip(all_names, all_params, all_se):\n",
    "        z = est / se if se > 0 else np.nan\n",
    "        p = 2 * (1 - stats.norm.cdf(abs(z))) if not np.isnan(z) else np.nan\n",
    "        stars = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "        print(f\"{name:>15s} {est:12.4f} {se:12.4f} {z:10.3f} {p:10.4f} {stars}\")\n",
    "    \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"Scale (sigma): {result['sigma']:.4f}\")\n",
    "    print(f\"Log-likelihood: {result['loglik']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "Now let's estimate the Tobit model with right-censoring at 6000:\n",
    "\n",
    "```r\n",
    "# R version:\n",
    "tobit(y.censored ~ x, data=data, left=-Inf, right=6000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Tobit with right-censoring at 6000\n",
    "# R: tobit(y.censored ~ x, data=data, left=-Inf, right=6000)\n",
    "tobit_result = fit_tobit(data['y_censored'].values, X_ols.values,\n",
    "                         left=-np.inf, right=y_upper)\n",
    "\n",
    "print_tobit_summary(tobit_result, var_names=['(Intercept)', 'x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Note that the error standard deviation is given by its log, where we should be expecting $\\sigma=400$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: log(400)\n",
    "print(f\"log(400) = {np.log(400):.6f}\")\n",
    "print(f\"Estimated log(sigma) = {tobit_result['log_sigma']:.6f}\")\n",
    "print(f\"Estimated sigma = {tobit_result['sigma']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "If we had censoring on both sides we could specify both the lower and upper censoring points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double censoring\n",
    "y_lower = 4500\n",
    "data['y_dbl_censored'] = np.clip(data['y_censored'].values, y_lower, y_upper)\n",
    "\n",
    "# R: tobit(y.dbl.censored ~ x, data=data, left=y.lower, right=y.upper)\n",
    "tobit_result_2 = fit_tobit(data['y_dbl_censored'].values, X_ols.values,\n",
    "                           left=y_lower, right=y_upper)\n",
    "\n",
    "print_tobit_summary(tobit_result_2, var_names=['(Intercept)', 'x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "Tobit models are therefore a useful technique for removing the effects of boundary observations. For example, if you were trying to understand the effects of characteristics on wages, you would encounter many individuals at minimum wages (dictated at either the state or federal level), a Tobit might be your first step to understanding how this censoring might be affecting your inference. Note though, that the normal distribution is doing a lot of the heavy lifting here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "There are other methods that weaken the strong distributional assumptions in the Tobit, but they're beyond the scope of this course! (keywords: *non-parametric* or *semi-parametric* censored regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## Interval Regression\n",
    "\n",
    "The other type of censored data you might encounter is interval censored data. That is, instead of observing the true value $y$ you instead see that it lies in the interval $[\\underline{y}_i,\\overline{y}_i]$. For example, from a survey, a respondent might select a household income category of:\n",
    "$$ \\$50{,}000 \\text{ to } \\$60{,}000$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interval-censored data\n",
    "# Each data point is in an interval of width 250, except for lower and upper tails\n",
    "# R: floor(data$y/250)*250\n",
    "data['y_int_lower'] = np.where(data['y'] > 4500, np.floor(data['y'] / 250) * 250, -np.inf)\n",
    "data['y_int_upper'] = np.where(data['y'] < 6000, np.ceil(data['y'] / 250) * 250, np.inf)\n",
    "data['y_mid'] = (data['y_int_upper'] + data['y_int_lower']) / 2\n",
    "\n",
    "print(data[['y_int_lower', 'y_mid', 'y_int_upper', 'x']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "We can define the negative log-likelihood of the interval data using the Tobit-like Normal assumption on the error. For an observation in interval $[\\underline{y}_i, \\overline{y}_i]$, the likelihood is:\n",
    "\n",
    "$$\\Phi\\left(\\frac{\\overline{y}_i - \\beta_0 - \\beta_1 x_i}{\\sigma}\\right) - \\Phi\\left(\\frac{\\underline{y}_i - \\beta_0 - \\beta_1 x_i}{\\sigma}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_intReg(params, y_lower, y_upper, x):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for interval regression.\n",
    "    \n",
    "    R equivalent:\n",
    "    nll.intReg <- function(beta) {\n",
    "        -sum(log(pnorm((y.upper - beta[1] - beta[2]*x) / beta[3]) -\n",
    "                 pnorm((y.lower - beta[1] - beta[2]*x) / beta[3])))\n",
    "    }\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array-like\n",
    "        [beta_0, beta_1, sigma]\n",
    "    y_lower, y_upper : array-like\n",
    "        Lower and upper interval bounds\n",
    "    x : array-like\n",
    "        Predictor variable\n",
    "    \"\"\"\n",
    "    b0, b1, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return 1e10\n",
    "    \n",
    "    # R: pnorm((y.upper - b0 - b1*x) / sigma)\n",
    "    prob = (stats.norm.cdf((y_upper - b0 - b1 * x) / sigma) -\n",
    "            stats.norm.cdf((y_lower - b0 - b1 * x) / sigma))\n",
    "    \n",
    "    prob = np.maximum(prob, 1e-300)  # avoid log(0)\n",
    "    return -np.sum(np.log(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum likelihood!\n",
    "# R: optim(c(5500, 3, 300), nll.intReg)\n",
    "int_result = optimize.minimize(\n",
    "    nll_intReg,\n",
    "    x0=[5500, 3, 300],\n",
    "    args=(data['y_int_lower'].values, data['y_int_upper'].values, data['x'].values),\n",
    "    method='Nelder-Mead'\n",
    ")\n",
    "\n",
    "print(f\"Interval Regression Results:\")\n",
    "print(f\"  beta_0 (Intercept): {int_result.x[0]:.4f}\")\n",
    "print(f\"  beta_1 (x):         {int_result.x[1]:.4f}\")\n",
    "print(f\"  sigma:              {int_result.x[2]:.4f}\")\n",
    "print(f\"  Neg. Log-Lik:       {int_result.fun:.4f}\")\n",
    "print(f\"  Converged:          {int_result.success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Selection Models\n",
    "While in some cases we observe values which are either top or bottom-coded, sometimes we just don't have data on some observations at all. The canonical example in Economics is on wages, and labor force participation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "When we look at the wages of a number of workers, implicitly we are examining a selection for the workers who were willing to work at the offered wages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "So, there is a hidden binary variable:\n",
    "* work at the offered wages\n",
    "* don't work at the offered wages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "Conditional on accepting the offer, we then observe the wages, and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "So, if we're trying to investigate what leads to higher/lower wages, we would want to understand both how characteristics are mapped into wage offers **and** whether those offers are accepted or declined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "The Heckman selection model uses a pair of limited dependent variables to understand the effects. \n",
    "* A wage-offer equation: $$ w_i^\\star= x_i^T \\beta +\\epsilon^{\\text{Ofr}}_i$$\n",
    "* A selection equation: $$ u_i^\\star= z_i^T\\delta  +\\epsilon^{\\text{Sel}}_i$$ \n",
    "\n",
    "where $x_i$ and $z_i$ are possibly overlapping sets of predictors for person $i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "As the analyst, we do not perfectly observe the latent variables $u^\\star_i$ (the relative happiness for working) and $w_i^\\star$ (the wage *offer*), we instead see the two limited variables:\n",
    "*  The *currently employed* variable: $$ u_i = \\begin{cases}1 & \\text{if }u^\\star_i \\geq 0 \\\\ 0 & \\text{if }u^\\star_i < 0\\end{cases}$$\n",
    "*  The *accepted-wage* variable:\n",
    "$$ w_i = \\begin{cases}w^\\star_i & \\text{if }u_i =1 \\\\ 0 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "If for example we made the assumption that:\n",
    "$$\\left(\\begin{array}{c}\\epsilon^{\\text{Ofr}}_i\\\\ \\epsilon^{\\text{Sel}}_i\\end{array}\\right) \\sim \\mathcal{N}\\left(0,\\Sigma\\right)$$\n",
    "then the employed variable in the selection equation would be very similar to a Probit estimate, while the wage variable would be similar to a Tobit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "In addition, the variance-covariance matrix $\\Sigma$ allows there to be correlation across the two errors, so something unobserved by the analyst that causes someone to have low wage offers might also cause them to be more/less picky with those offers.  \n",
    "\n",
    "Because scale is not identified for the binary decision, the variance covariance matrix we are looking for is given by:\n",
    "$$\\Sigma=\\left[\\begin{array}{cc} 1 & \\rho\\\\ \\rho & \\sigma^2 \\end{array}\\right] $$\n",
    "where $\\sigma^2$ tells us the variability of the wages and $\\rho$ gives us the correlation between the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "### Heckman Two-Step Estimator in Python\n",
    "\n",
    "In R, we used `selection()` from the `sampleSelection` package. In Python, we implement Heckman's two-step procedure:\n",
    "\n",
    "1. **Step 1**: Estimate a Probit model for the selection equation\n",
    "2. **Step 2**: Compute the **inverse Mills ratio** (IMR): $\\lambda_i = \\frac{\\phi(Z_i\\hat{\\delta})}{\\Phi(Z_i\\hat{\\delta})}$\n",
    "3. **Step 3**: Run OLS on the outcome equation, including the IMR as an additional regressor\n",
    "\n",
    "**Key R to Python mappings:**\n",
    "| R | Python |\n",
    "|---|--------|\n",
    "| `selection(selection=..., outcome=...)` | Two-step Heckman (below) |\n",
    "| `pnorm(x)` | `stats.norm.cdf(x)` |\n",
    "| `dnorm(x)` | `stats.norm.pdf(x)` |\n",
    "| IMR = `dnorm(Xb)/pnorm(Xb)` | `utils.inverse_mills_ratio(Xb)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "Let's look at the canonical version of this: data from 1975 on married women's labor-force participation decisions and wages, examined in [Mroz (1987)](https://doi.org/10.2307/1911029)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mroz87 dataset\n",
    "# This is available from statsmodels or we can load it from the web\n",
    "try:\n",
    "    import statsmodels.datasets\n",
    "    mroz = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/sampleSelection/Mroz87.csv')\n",
    "except:\n",
    "    # Alternative: create synthetic data matching the structure\n",
    "    print(\"Loading data from web...\")\n",
    "    mroz = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/sampleSelection/Mroz87.csv')\n",
    "\n",
    "print(mroz.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: summary(Mroz87[, c('lfp', 'age', 'faminc', 'educ', 'exper', 'city')])\n",
    "print(mroz[['lfp', 'age', 'faminc', 'educ', 'exper', 'city']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: Mroz87$has.children <- (Mroz87$kids5 + Mroz87$kids618 > 0)\n",
    "mroz['has_children'] = ((mroz['kids5'] + mroz['kids618']) > 0).astype(int)\n",
    "\n",
    "# Create age squared and experience squared\n",
    "mroz['age_sq'] = mroz['age'] ** 2\n",
    "mroz['exper_sq'] = mroz['exper'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "We specify the two equations as:\n",
    "* **Selection** $\\Pr\\left\\{\\text{In Labor force}\\right\\}=\\delta_0 + \\delta_1\\cdot\\text{age} +\\delta_2\\cdot \\text{age}^2 +\\delta_3\\cdot \\text{fam.income}+\\delta_4\\cdot \\text{has.children}+\\delta_5\\cdot \\text{educ}$\n",
    "* **Outcome** $ \\mathbb{E}\\text{Wage}|x = \\beta_0+\\beta_1\\cdot\\text{exper} +\\beta_2\\cdot\\text{exper}^2+\\beta_3\\cdot\\text{educ}+\\beta_4\\cdot\\text{city}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heckman_two_step(selection_y, selection_X, outcome_y, outcome_X,\n",
    "                     selection_names=None, outcome_names=None):\n",
    "    \"\"\"\n",
    "    Heckman two-step selection model.\n",
    "    \n",
    "    R equivalent:\n",
    "    selection(selection = lfp ~ age + I(age^2) + faminc + has.children + educ,\n",
    "              outcome = wage ~ exper + I(exper^2) + educ + city,\n",
    "              data = Mroz87)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    selection_y : array-like\n",
    "        Binary selection variable (1 = selected/observed)\n",
    "    selection_X : ndarray\n",
    "        Design matrix for selection equation (with constant)\n",
    "    outcome_y : array-like\n",
    "        Outcome variable (only for selected observations)\n",
    "    outcome_X : ndarray\n",
    "        Design matrix for outcome equation (with constant, selected obs only)\n",
    "    selection_names : list, optional\n",
    "        Variable names for selection equation\n",
    "    outcome_names : list, optional\n",
    "        Variable names for outcome equation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Estimation results including probit, OLS, and IMR coefficient\n",
    "    \"\"\"\n",
    "    # Step 1: Probit for selection equation\n",
    "    # R: glm(lfp ~ ..., family=binomial(link='probit'))\n",
    "    probit = sm.Probit(selection_y, selection_X).fit(disp=0)\n",
    "    \n",
    "    # Step 2: Compute inverse Mills ratio\n",
    "    # R: lambda_i = dnorm(Z %*% delta) / pnorm(Z %*% delta)\n",
    "    # Python: utils.inverse_mills_ratio(probit.fittedvalues)\n",
    "    z_hat = probit.fittedvalues  # Z * delta_hat (linear index)\n",
    "    imr = utils.inverse_mills_ratio(z_hat)\n",
    "    \n",
    "    # Step 3: OLS on selected sample with IMR as additional regressor\n",
    "    selected = selection_y == 1\n",
    "    imr_selected = imr[selected]\n",
    "    \n",
    "    # Augment outcome design matrix with IMR\n",
    "    outcome_X_aug = np.column_stack([outcome_X, imr_selected])\n",
    "    \n",
    "    ols = sm.OLS(outcome_y, outcome_X_aug).fit()\n",
    "    \n",
    "    # Extract results\n",
    "    sigma = ols.params[-1]  # coefficient on IMR = rho * sigma\n",
    "    rho_sigma = ols.params[-1]\n",
    "    \n",
    "    # Estimate sigma from residuals (approximate)\n",
    "    sigma_hat = np.sqrt(np.mean(ols.resid**2) + rho_sigma**2 * np.mean(imr_selected * (imr_selected + z_hat[selected])))\n",
    "    rho_hat = rho_sigma / sigma_hat if sigma_hat > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'probit': probit,\n",
    "        'ols': ols,\n",
    "        'imr': imr,\n",
    "        'delta': probit.params,\n",
    "        'beta': ols.params[:-1],\n",
    "        'rho_sigma': rho_sigma,\n",
    "        'sigma': sigma_hat,\n",
    "        'rho': rho_hat,\n",
    "        'selection_names': selection_names,\n",
    "        'outcome_names': outcome_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_heckman_summary(result):\n",
    "    \"\"\"\n",
    "    Print Heckman model summary similar to R's sampleSelection output.\n",
    "    \"\"\"\n",
    "    print(\"Heckman Selection Model (Two-Step)\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    probit = result['probit']\n",
    "    ols = result['ols']\n",
    "    \n",
    "    sel_names = result['selection_names'] or [f'z_{i}' for i in range(len(probit.params))]\n",
    "    out_names = (result['outcome_names'] or [f'x_{i}' for i in range(len(ols.params) - 1)]) + ['IMR (lambda)']\n",
    "    \n",
    "    n_total = len(result['imr'])\n",
    "    n_selected = int(probit.model.endog.sum())\n",
    "    print(f\"{n_total} observations ({n_total - n_selected} censored and {n_selected} observed)\")\n",
    "    print()\n",
    "    \n",
    "    # Selection equation\n",
    "    print(\"Probit selection equation:\")\n",
    "    print(f\"{'':>20s} {'Estimate':>12s} {'Std. Error':>12s} {'z value':>10s} {'Pr(>|z|)':>10s}\")\n",
    "    print(\"-\" * 65)\n",
    "    for name, est, se, z, p in zip(sel_names, probit.params, probit.bse,\n",
    "                                    probit.tvalues, probit.pvalues):\n",
    "        stars = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "        print(f\"{name:>20s} {est:12.6f} {se:12.6f} {z:10.3f} {p:10.6f} {stars}\")\n",
    "    print()\n",
    "    \n",
    "    # Outcome equation\n",
    "    print(\"Outcome equation:\")\n",
    "    print(f\"{'':>20s} {'Estimate':>12s} {'Std. Error':>12s} {'t value':>10s} {'Pr(>|t|)':>10s}\")\n",
    "    print(\"-\" * 65)\n",
    "    for name, est, se, t, p in zip(out_names, ols.params, ols.bse,\n",
    "                                    ols.tvalues, ols.pvalues):\n",
    "        stars = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "        print(f\"{name:>20s} {est:12.6f} {se:12.6f} {t:10.3f} {p:10.6f} {stars}\")\n",
    "    print()\n",
    "    \n",
    "    # Error terms\n",
    "    print(\"Error terms:\")\n",
    "    print(f\"  sigma: {result['sigma']:.4f}\")\n",
    "    print(f\"  rho:   {result['rho']:.4f}\")\n",
    "    print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the Heckman model\n",
    "\n",
    "# Selection equation: lfp ~ age + age^2 + faminc + has_children + educ\n",
    "selection_y = mroz['lfp'].values\n",
    "selection_X = sm.add_constant(\n",
    "    mroz[['age', 'age_sq', 'faminc', 'has_children', 'educ']].values\n",
    ")\n",
    "selection_names = ['(Intercept)', 'age', 'I(age^2)', 'faminc', 'has_children', 'educ']\n",
    "\n",
    "# Outcome equation: wage ~ exper + exper^2 + educ + city\n",
    "# Only for selected (lfp == 1) observations\n",
    "selected = mroz['lfp'] == 1\n",
    "outcome_y = mroz.loc[selected, 'wage'].values\n",
    "outcome_X = sm.add_constant(\n",
    "    mroz.loc[selected, ['exper', 'exper_sq', 'educ', 'city']].values\n",
    ")\n",
    "outcome_names = ['(Intercept)', 'exper', 'I(exper^2)', 'educ', 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Heckman model\n",
    "# R: selection(selection = lfp ~ age + I(age^2) + faminc + has.children + educ,\n",
    "#              outcome = wage ~ exper + I(exper^2) + educ + city,\n",
    "#              data = Mroz87, iterlim = 500)\n",
    "heckman_result = heckman_two_step(\n",
    "    selection_y, selection_X,\n",
    "    outcome_y, outcome_X,\n",
    "    selection_names=selection_names,\n",
    "    outcome_names=outcome_names\n",
    ")\n",
    "\n",
    "print_heckman_summary(heckman_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "Breaking the different equation estimates out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: delta <- coef(lfp.est)[1:6]\n",
    "delta = heckman_result['delta']\n",
    "print(\"Selection equation (delta):\")\n",
    "for name, val in zip(selection_names, delta):\n",
    "    print(f\"  {name}: {val:.6f}\")\n",
    "\n",
    "# R: beta <- coef(lfp.est)[7:11]\n",
    "beta = heckman_result['beta']\n",
    "print(\"\\nOutcome equation (beta):\")\n",
    "for name, val in zip(outcome_names, beta):\n",
    "    print(f\"  {name}: {val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "Again, we need to be careful about how we use these estimates. For a 30 year-old urban woman with five-years work experience, a high-school education and an average family income the effect of having children on the participation decision is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: pnorm(-(delta[1]+delta[2]*30 + delta[3]*30*30 + delta[4]*mean(faminc) + delta[5] + delta[6]*12))\n",
    "# Python: stats.norm.cdf is pnorm\n",
    "mean_faminc = mroz['faminc'].mean()\n",
    "\n",
    "# Linear index with children\n",
    "z_with = (delta[0] + delta[1]*30 + delta[2]*30**2 +\n",
    "          delta[3]*mean_faminc + delta[4]*1 + delta[5]*12)\n",
    "\n",
    "# Linear index without children\n",
    "z_without = (delta[0] + delta[1]*30 + delta[2]*30**2 +\n",
    "             delta[3]*mean_faminc + delta[4]*0 + delta[5]*12)\n",
    "\n",
    "# R: 1 - pnorm(-z) = pnorm(z) = Pr(u* >= 0) = Pr(in labor force)\n",
    "prob_with = stats.norm.cdf(z_with)\n",
    "prob_without = stats.norm.cdf(z_without)\n",
    "\n",
    "print(f\"Probability of LFP with children:    {prob_with:.4f}\")\n",
    "print(f\"Probability of LFP without children: {prob_without:.4f}\")\n",
    "print(f\"Difference:                          {prob_with - prob_without:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-64",
   "metadata": {},
   "source": [
    "Expected Wage Offer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: Mean.Offer <- beta[1]+beta[2]*5+beta[3]*25+beta[4]*12+beta[5]\n",
    "mean_offer = beta[0] + beta[1]*5 + beta[2]*25 + beta[3]*12 + beta[4]*1\n",
    "print(f\"Expected wage offer (30yo, urban, 5yr exp, HS): {mean_offer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-66",
   "metadata": {},
   "source": [
    "Because of the model we can look at what the *offer* distribution looks like separately from the observed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted wages: unconditional (the offer) and conditional on selection\n",
    "# For all observations, compute the linear prediction from the outcome equation\n",
    "all_outcome_X = sm.add_constant(\n",
    "    mroz[['exper', 'exper_sq', 'educ', 'city']].values\n",
    ")\n",
    "\n",
    "# Unconditional prediction: X * beta\n",
    "offers_all = all_outcome_X @ heckman_result['beta']\n",
    "\n",
    "# Conditional prediction adjusts for selection:\n",
    "# E[w | selected] = X*beta + rho*sigma*IMR\n",
    "imr_all = heckman_result['imr']\n",
    "rho_sigma = heckman_result['rho_sigma']\n",
    "\n",
    "# For those IN the labor force, the conditional wage is adjusted downward by IMR\n",
    "# For those OUT, we predict what they would have earned\n",
    "offers_cond_in = offers_all[selected] + rho_sigma * imr_all[selected]\n",
    "offers_cond_out = offers_all[~selected] + rho_sigma * imr_all[~selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-68",
   "metadata": {},
   "source": [
    "Here we can visualize the effect coming from $\\rho$, that conditional on the person being in or out of labor force, we can think through what those wages *might have been*. So from this we're realizing that a lot of the women who select out of the labor force would have counterfactually had higher wages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot comparing conditional wage distributions\n",
    "pred_data = pd.DataFrame({\n",
    "    'value': np.concatenate([offers_cond_out, offers_cond_in]),\n",
    "    'key': (['Out'] * len(offers_cond_out)) + (['In'] * len(offers_cond_in))\n",
    "})\n",
    "\n",
    "# Filter to reasonable range\n",
    "pred_data = pred_data[(pred_data['value'] > 0) & (pred_data['value'] < 20)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.68))\n",
    "\n",
    "# Create violin plot\n",
    "parts_out = ax.violinplot(pred_data.loc[pred_data['key'] == 'Out', 'value'].values,\n",
    "                          positions=[0], showmeans=True, showmedians=True)\n",
    "parts_in = ax.violinplot(pred_data.loc[pred_data['key'] == 'In', 'value'].values,\n",
    "                         positions=[1], showmeans=True, showmedians=True)\n",
    "\n",
    "# Color the violins\n",
    "for pc in parts_out['bodies']:\n",
    "    pc.set_facecolor(PITT_BLUE)\n",
    "    pc.set_alpha(0.7)\n",
    "for pc in parts_in['bodies']:\n",
    "    pc.set_facecolor(PITT_GOLD)\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Out of LF', 'In LF'])\n",
    "ax.set_ylabel('Predicted Wage')\n",
    "ax.set_title('Conditional Wage Distributions by Labor Force Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-70",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "Alongside this Tobit *type 2* model, there are other options. For example, the other common model would be a Tobit *type 5* model where people select into one of two options $A$ or $B$ (think of different careers say), and then we observe the outcome wage within each of those careers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-71",
   "metadata": {},
   "source": [
    "So the equations would be\n",
    "*  The *selection equation*: $$ u_i = \\begin{cases}A & \\text{if }u^\\star_i \\geq 0 \\\\ B & \\text{if }u^\\star_i < 0\\end{cases}$$\n",
    "*  The *accepted-wage* variable:\n",
    "$$ w_i = \\begin{cases}w^\\star_{(A,i)} & \\text{if }u_i =A \\\\ w^\\star_{(B,i)} & \\text{if }u_i=B\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-72",
   "metadata": {},
   "source": [
    "We would then write down models for the three latent variables \n",
    "* $u^\\star_i$: $i$'s relative preference for $A$ over $B$\n",
    "* $w^\\star_{(A,i)}$: The wage $i$ would get in $A$\n",
    "* $w^\\star_{(B,i)}$: The wage $i$ would get in $B$\n",
    "\n",
    "(Again, the identification conditions for the three equations are not trivial, and you'll need some separate variables to use as instruments in the selection equation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-73",
   "metadata": {},
   "source": [
    "### Tobit Type 5 (Switching Regression) Example\n",
    "\n",
    "Making some simulated data to illustrate the model estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated switching regression data\n",
    "np.random.seed(123)\n",
    "\n",
    "beta0_A, beta1_A, beta2_A = 20000, 5000, 2000  # Career A\n",
    "beta0_B, beta1_B, beta2_B = 50000, 500, 0       # Career B\n",
    "\n",
    "delta_0, delta_1, delta_2, delta_3 = -3, 1.5, 1/8, -1\n",
    "\n",
    "Nsim = 5000\n",
    "educ = np.random.poisson(12, Nsim)\n",
    "exper = 2 * np.random.poisson(3, Nsim) + np.random.poisson(1, Nsim)\n",
    "\n",
    "# Type is correlated with education\n",
    "type_var = (np.random.uniform(size=Nsim) + educ / 20 > 0.6).astype(int)\n",
    "male = (np.random.uniform(size=Nsim) > 0.5).astype(int)\n",
    "\n",
    "eps_S = np.random.normal(size=Nsim)\n",
    "eps_A = np.random.normal(0, 5000, Nsim)\n",
    "eps_B = np.random.normal(0, 1500, Nsim)\n",
    "\n",
    "u_sel = delta_0 + type_var * delta_1 + delta_2 * educ + delta_3 * male + eps_S\n",
    "selection_A = (u_sel >= 0).astype(int)\n",
    "\n",
    "y_star_A = beta0_A + beta1_A * educ + beta2_A * exper + eps_A\n",
    "y_star_B = beta0_B + beta1_B * educ + beta2_B * exper + eps_B\n",
    "\n",
    "income = np.where(u_sel >= 0, y_star_A, y_star_B)\n",
    "\n",
    "data_sim = pd.DataFrame({\n",
    "    'income': income,\n",
    "    'selection_A': selection_A,\n",
    "    'educ': educ,\n",
    "    'exper': exper,\n",
    "    'type': type_var,\n",
    "    'male': male\n",
    "})\n",
    "\n",
    "print(data_sim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-75",
   "metadata": {},
   "source": [
    "We can implement the switching regression as two separate Heckman models (one for each regime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Probit for selection into A vs B\n",
    "sel_X = sm.add_constant(data_sim[['type', 'educ', 'male']].values)\n",
    "probit_AB = sm.Probit(data_sim['selection_A'].values, sel_X).fit(disp=0)\n",
    "\n",
    "print(\"Probit Selection Equation:\")\n",
    "print(f\"{'':>15s} {'Estimate':>12s} {'Std. Error':>12s} {'z value':>10s}\")\n",
    "print(\"-\" * 55)\n",
    "for name, est, se, z in zip(['(Intercept)', 'type', 'educ', 'male'],\n",
    "                             probit_AB.params, probit_AB.bse, probit_AB.tvalues):\n",
    "    print(f\"{name:>15s} {est:12.6f} {se:12.6f} {z:10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: IMR for both regimes\n",
    "z_hat_AB = probit_AB.fittedvalues\n",
    "\n",
    "# For regime A (selected): IMR = phi(z)/Phi(z)\n",
    "imr_A = utils.inverse_mills_ratio(z_hat_AB)\n",
    "\n",
    "# For regime B (not selected): IMR = -phi(z)/(1-Phi(z)) = phi(-z)/Phi(-z)\n",
    "# This uses the fact that for the NOT selected, we need the \"negative\" Mills ratio\n",
    "imr_B = -stats.norm.pdf(z_hat_AB) / (1 - stats.norm.cdf(z_hat_AB))\n",
    "\n",
    "# Step 3: OLS for each regime with IMR\n",
    "mask_A = data_sim['selection_A'] == 1\n",
    "mask_B = data_sim['selection_A'] == 0\n",
    "\n",
    "# Regime A (selected into A)\n",
    "out_X_A = sm.add_constant(data_sim.loc[mask_A, ['educ', 'exper']].values)\n",
    "out_X_A_aug = np.column_stack([out_X_A, imr_A[mask_A]])\n",
    "ols_A = sm.OLS(data_sim.loc[mask_A, 'income'].values, out_X_A_aug).fit()\n",
    "\n",
    "# Regime B (selected into B)\n",
    "out_X_B = sm.add_constant(data_sim.loc[mask_B, ['educ', 'exper']].values)\n",
    "out_X_B_aug = np.column_stack([out_X_B, imr_B[mask_B]])\n",
    "ols_B = sm.OLS(data_sim.loc[mask_B, 'income'].values, out_X_B_aug).fit()\n",
    "\n",
    "print(f\"\\nOutcome Equation A (Career A, n={mask_A.sum()}):\")\n",
    "print(f\"{'':>15s} {'Estimate':>12s} {'Std. Error':>12s}\")\n",
    "print(\"-\" * 45)\n",
    "for name, est, se in zip(['(Intercept)', 'educ', 'exper', 'IMR'],\n",
    "                          ols_A.params, ols_A.bse):\n",
    "    print(f\"{name:>15s} {est:12.4f} {se:12.4f}\")\n",
    "\n",
    "print(f\"\\nOutcome Equation B (Career B, n={mask_B.sum()}):\")\n",
    "print(f\"{'':>15s} {'Estimate':>12s} {'Std. Error':>12s}\")\n",
    "print(\"-\" * 45)\n",
    "for name, est, se in zip(['(Intercept)', 'educ', 'exper', 'IMR'],\n",
    "                          ols_B.params, ols_B.bse):\n",
    "    print(f\"{name:>15s} {est:12.4f} {se:12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimated vs true parameters\n",
    "print(\"Parameter Comparison:\")\n",
    "print(f\"{'':>20s} {'True':>10s} {'Estimated':>10s}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(\"\\nSelection Equation:\")\n",
    "true_delta = [delta_0, delta_1, delta_2, delta_3]\n",
    "for name, true, est in zip(['(Intercept)', 'type', 'educ', 'male'],\n",
    "                            true_delta, probit_AB.params):\n",
    "    print(f\"{name:>20s} {true:10.4f} {est:10.4f}\")\n",
    "\n",
    "print(\"\\nOutcome A:\")\n",
    "true_A = [beta0_A, beta1_A, beta2_A]\n",
    "for name, true, est in zip(['(Intercept)', 'educ', 'exper'],\n",
    "                            true_A, ols_A.params[:3]):\n",
    "    print(f\"{name:>20s} {true:10.4f} {est:10.4f}\")\n",
    "\n",
    "print(\"\\nOutcome B:\")\n",
    "true_B = [beta0_B, beta1_B, beta2_B]\n",
    "for name, true, est in zip(['(Intercept)', 'educ', 'exper'],\n",
    "                            true_B, ols_B.params[:3]):\n",
    "    print(f\"{name:>20s} {true:10.4f} {est:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-79",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "* The log-likelihood functions here can be non-concave, which leads to potentially many local maxima. As such, you need to ensure you have pretty good initial conditions.\n",
    "* Most of the procedures here use two-step estimators that figure out the selection probabilities, and then use these as instruments in the second stage. These estimates are then used as initial conditions for the max likelihood stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-80",
   "metadata": {},
   "source": [
    "* Back in the day (1970s) the two step estimators were used as they didn't really have the computational power to figure out the max likelihood parameters. From the R `sampleSelection` Vignette:\n",
    "\n",
    "> The original article suggests using the two-step solution for exploratory work and as initial values for ML estimation, since in those days the cost of the two-step solution was \\$15 while that of the maximum-likelihood solution was \\$700.\n",
    "\n",
    "* While the computation power is there now for max likelihood, in the academic literature people have moved away from some of the stronger parametric assumptions in max likelihood (here the Normal distribution is doing slightly more heavy lifting that we would like)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-81",
   "metadata": {},
   "source": [
    "* The more modern approach is on **control functions** to model the effects of selection directly in the main equation. Again, this is not quite in the scope of this course, but I provide it as a keyword for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-82",
   "metadata": {},
   "source": [
    "## Summary: R to Python Mappings for Censoring and Selection\n",
    "\n",
    "| R Function | Python Equivalent |\n",
    "|------------|-------------------|\n",
    "| `tobit(y ~ x, left=L, right=R)` | Custom `fit_tobit()` with `scipy.optimize.minimize` |\n",
    "| `selection(selection=..., outcome=...)` | Custom `heckman_two_step()` (Probit + OLS with IMR) |\n",
    "| `pnorm(x)` | `scipy.stats.norm.cdf(x)` |\n",
    "| `dnorm(x)` | `scipy.stats.norm.pdf(x)` |\n",
    "| `log(x)` | `np.log(x)` |\n",
    "| `optim(x0, f)` | `scipy.optimize.minimize(f, x0)` |\n",
    "| IMR: `dnorm(z)/pnorm(z)` | `utils.inverse_mills_ratio(z)` |\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Tobit**: Model censored data by combining the normal PDF (for uncensored) and CDF (for censored tails) in the likelihood\n",
    "- **Heckman**: Correct for selection bias by (1) estimating selection via Probit, (2) computing the inverse Mills ratio, and (3) including the IMR in the outcome regression\n",
    "- **Interval regression**: Likelihood based on the difference of two CDF evaluations at the interval bounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}