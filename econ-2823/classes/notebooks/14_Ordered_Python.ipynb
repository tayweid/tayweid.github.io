{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered Multinomial Choice (Python Version)\n",
    "---\n",
    "Here we will look at extensions of the binary-choice model from the last class to incorporate multiple possible outcomes. However, we will do this under the assumption that we know that the choices are fully ordered.\n",
    "\n",
    "This ordering is known by the researcher/analyst, so that the ordinal choice across the outcomes can be written as an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you've sent out a survey to your customers on their satisfaction, and you included a five-point *likert* scale on their likelihood of recommending your product to a friend:\n",
    "1.  Strongly Disagree\n",
    "2. Disagree\n",
    "3. Neutral\n",
    "4. Agree\n",
    "5. Strongly Agree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're trying to figure out which characteristics/experiences create strong valence to your product. One way of doing this is to take the category you care most about and make it binary!\n",
    "\n",
    "So for example, you can code *Agree* and *Strongly Agree* as 1, and *Strongly Disagree* through *Neutral* as a zero. Then you can use a probit or a logit as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you want to understand the differences across the categories, then you would use the fact that you know the outcomes are ordered to generate an estimation using a latent variable as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will again model a linear predictor $\\eta_i=x^T_i\\beta$, where\n",
    "$$y^\\star_i=x^T_i\\beta+\\epsilon_i,$$\n",
    "and $\\epsilon_i$ will have a fixed distribution (typically logistic or Normal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, on top of this if we have $m$ different ordered outcomes we also model a threshold quantity between each of the ordered outcomes:\n",
    "* $\\zeta_{0,1}$ for the threshold between choices 0 and 1\n",
    "* $\\zeta_{1,2}$ for the threshold between choices 1 and 2\n",
    "* $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that our firm has three levels of service and we examine the ordered outcome for each potential customer\n",
    "* No purchase (Option 0)\n",
    "* Basic package (Option 1)\n",
    "* Upgrade package (Option 2)\n",
    "* Deluxe package (Option 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limited-dependent variable representation of this choice would be:\n",
    "$$y=\\begin{cases}\n",
    "3 \\text{ (Deluxe)} & \\text{ if }x_i^T\\beta +\\epsilon_i \\geq \\zeta_{2,3} \\\\\n",
    "2 \\text{ (Upgrade)} & \\text{ if }\\zeta_{23}\\geq x_i^T\\beta +\\epsilon_i \\geq \\zeta_{1,2} \\\\\n",
    "1 \\text{ (Basic)} & \\text{ if }\\zeta_{12}\\geq x_i^T\\beta +\\epsilon_i \\geq \\zeta_{0,1} \\\\\n",
    "0 \\text{ (No purchase)} & \\text{ otherwise (so }\\zeta_{01}\\geq x_i^T\\beta +\\epsilon_i \\text{.)}\n",
    "\\end{cases}$$\n",
    "Based on the constants $\\zeta_{0,1}<\\zeta_{1,2}<\\zeta_{2,3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, someone with observable characteristics given by $x_i$ would have a variable level effect of $x_i^T\\beta$ (note, no intercept in here) has a probability of selecting each option governed by the likelihood the error is in the shaded regions:\n",
    "![Model](https://alistairjwilson.github.io/MQE_AW/i/OrderedLogit.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As  we then shift the characteristics given by $x_i$ (and so moving the modified $x_i^T\\beta$ up and down), the effect is to modify the size of each region:\n",
    "![Animation](https://alistairjwilson.github.io/MQE_AW/i/OrderedLogit.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is estimated via maximum likelihood using the assumed distribution for the error $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if there were no other covariates and we were just estimating the crossing points and we had:\n",
    "* 50 who don't purchase ($y=0$)\n",
    "* 100 who purchase a basic product ($y=1$)\n",
    "* 15 who purchase an upgraded package ($y=2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the assumption that the error is logistic, with CDF $\\frac{e^x}{1+e^x}$, the log-likelihood of the data is then:\n",
    "$$ 50 \\log\\left( \\frac{e^{\\zeta_{01}}}{1+e^{\\zeta_{01}}} \\right) +100\\log\\left(\n",
    "\\frac{e^{\\zeta_{12}}}{1+e^{\\zeta_{12}}}-\\frac{e^{\\zeta_{01}}}{1+e^{\\zeta_{01}}}\n",
    "\\right)+15\\log\\left(1-\\frac{e^{\\zeta_{12}}}{1+e^{\\zeta_{12}}}\\right).$$\n",
    "\n",
    "Which is maximized at $\\hat{\\zeta}_{01}=-0.833$ and $\\hat{\\zeta}_{12}=2.303$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the assumption that the error is Normal, with CDF $\\Phi(\\cdot)$, the log-likelihood of the data is then:\n",
    "$$ 50 \\log\\left(\\Phi(\\zeta_{0,1})\\right) +100\\log\\left(\\Phi(\\zeta_{1,2})-\\Phi(\\zeta_{0,1})\\right)+15\\log\\left(1-\\Phi(\\zeta_{1,2})\\right).$$\n",
    "\n",
    "Which is maximized at $\\hat{\\zeta}_{01}=-0.516$ and $\\hat{\\zeta}_{12}=1.335$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the seemingly large differences in the numbers though, when you plug these estimates back into the relevant distributions, the inferences are identical. For example, consider the probability of purchasing a basic product:\n",
    "![Probit vs Logit](http://alistairjwilson.github.io/MQE_AW/i/OLogitVOProbit.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are no other covariates here, the model in each case is setting the intercept parameters to ensure that the probability of lying  in the relevant region is exactly the empirical incidence (so 100/165 for the *basic* purchases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying threshold estimates with custom MLE\n",
    "\n",
    "Let's verify those threshold numbers ourselves by writing the log-likelihood and optimizing it with `scipy.optimize`. This is a simple case with no covariates -- just intercept-only thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, optimize\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import utils\n",
    "\n",
    "# Set up plotting style\n",
    "utils.set_pitt_style()\n",
    "PITT_BLUE = utils.PITT_BLUE\n",
    "PITT_GOLD = utils.PITT_GOLD\n",
    "PITT_GRAY = utils.PITT_GRAY\n",
    "PITT_LGRAY = utils.PITT_LGRAY\n",
    "PITT_DGRAY = utils.PITT_DGRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept-only ordered logit: 50 no-purchase, 100 basic, 15 upgrade\n",
    "# Log-likelihood as a function of two thresholds (zeta_01, zeta_12)\n",
    "\n",
    "def loglik_ordered_logit_simple(zeta):\n",
    "    \"\"\"Log-likelihood for intercept-only ordered logit.\"\"\"\n",
    "    z01, z12 = zeta\n",
    "    # CDF is the logistic function\n",
    "    F01 = stats.logistic.cdf(z01)\n",
    "    F12 = stats.logistic.cdf(z12)\n",
    "    # Probabilities for each category\n",
    "    p0 = F01                   # P(y=0)\n",
    "    p1 = F12 - F01             # P(y=1)\n",
    "    p2 = 1 - F12               # P(y=2)\n",
    "    # Clip to avoid log(0)\n",
    "    p0, p1, p2 = np.clip([p0, p1, p2], 1e-15, None)\n",
    "    return 50 * np.log(p0) + 100 * np.log(p1) + 15 * np.log(p2)\n",
    "\n",
    "def loglik_ordered_probit_simple(zeta):\n",
    "    \"\"\"Log-likelihood for intercept-only ordered probit.\"\"\"\n",
    "    z01, z12 = zeta\n",
    "    F01 = stats.norm.cdf(z01)\n",
    "    F12 = stats.norm.cdf(z12)\n",
    "    p0 = F01\n",
    "    p1 = F12 - F01\n",
    "    p2 = 1 - F12\n",
    "    p0, p1, p2 = np.clip([p0, p1, p2], 1e-15, None)\n",
    "    return 50 * np.log(p0) + 100 * np.log(p1) + 15 * np.log(p2)\n",
    "\n",
    "# Maximize (minimize the negative)\n",
    "res_logit = optimize.minimize(lambda z: -loglik_ordered_logit_simple(z),\n",
    "                              x0=[0.0, 1.0], method='BFGS')\n",
    "res_probit = optimize.minimize(lambda z: -loglik_ordered_probit_simple(z),\n",
    "                               x0=[0.0, 1.0], method='BFGS')\n",
    "\n",
    "print(\"Ordered Logit thresholds (intercept-only):\")\n",
    "print(f\"  zeta_01 = {res_logit.x[0]:.3f}  (expected: -0.833)\")\n",
    "print(f\"  zeta_12 = {res_logit.x[1]:.3f}  (expected:  2.303)\")\n",
    "print()\n",
    "print(\"Ordered Probit thresholds (intercept-only):\")\n",
    "print(f\"  zeta_01 = {res_probit.x[0]:.3f}  (expected: -0.516)\")\n",
    "print(f\"  zeta_12 = {res_probit.x[1]:.3f}  (expected:  1.335)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: implied probabilities are the same despite different thresholds\n",
    "p_logit = [\n",
    "    stats.logistic.cdf(res_logit.x[0]),\n",
    "    stats.logistic.cdf(res_logit.x[1]) - stats.logistic.cdf(res_logit.x[0]),\n",
    "    1 - stats.logistic.cdf(res_logit.x[1])\n",
    "]\n",
    "p_probit = [\n",
    "    stats.norm.cdf(res_probit.x[0]),\n",
    "    stats.norm.cdf(res_probit.x[1]) - stats.norm.cdf(res_probit.x[0]),\n",
    "    1 - stats.norm.cdf(res_probit.x[1])\n",
    "]\n",
    "\n",
    "print(\"Implied probabilities:\")\n",
    "print(f\"  Empirical:  {50/165:.4f}  {100/165:.4f}  {15/165:.4f}\")\n",
    "print(f\"  Logit:      {p_logit[0]:.4f}  {p_logit[1]:.4f}  {p_logit[2]:.4f}\")\n",
    "print(f\"  Probit:     {p_probit[0]:.4f}  {p_probit[1]:.4f}  {p_probit[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data example\n",
    "Here I'm using data from the 2020 [National Youth Tobacco Survey](https://www.cdc.gov/tobacco/data_statistics/surveys/nyts/data/index.html) on \"eCig\" (vapes, etc) usage.\n",
    "\n",
    "Technically I'm joining together two variables, one on being a current user, and another for non-users on the curiosity, where I ranked/labeled the data outcomes via:\n",
    "\n",
    "```r\n",
    "factor(eCig$eCigUse, ordered=TRUE, labels=\n",
    "c(\"User\",\"Definitely.Try\",\"Probably.Try\",\"Probably.Not.Try\",'Definitely.Not.Try'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eCig data from R's .rdata format\n",
    "# R: load(file='eCig/eCig.rdata')\n",
    "# Python: use pyreadr to load .rdata files\n",
    "\n",
    "ecig_data = utils.load_rda('eCig/eCig.rdata')\n",
    "eCigUse = ecig_data['eCigUse'].copy()\n",
    "print(eCigUse.head())\n",
    "print(f\"\\nShape: {eCigUse.shape}\")\n",
    "print(f\"\\nColumn types:\\n{eCigUse.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rankings of the outcomes here are:\n",
    "\n",
    "0. Have used an e-Cigarette/Vape\n",
    "1. Have not used, but stated would *Definitely Try*\n",
    "2. *Probably Try*\n",
    "3. *Probably Not Try*\n",
    "4. *Definitely Not Try*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the outcome variable\n",
    "# R: head(eCigUse$eCigUse)\n",
    "# The R factor has levels: User < Definitely.Try < Probably.Try < Probably.Not.Try < Definitely.Not.Try\n",
    "\n",
    "print(\"Unique values:\")\n",
    "print(eCigUse['eCigUse'].value_counts().sort_index())\n",
    "print(f\"\\nAge summary:\")\n",
    "print(eCigUse['Age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for ordered models\n",
    "# Drop rows with missing values (matching R behavior)\n",
    "df = eCigUse.dropna().copy()\n",
    "\n",
    "# The outcome needs to be an ordered categorical\n",
    "# R's factor ordering: User < Definitely.Try < Probably.Try < Probably.Not.Try < Definitely.Not.Try\n",
    "category_order = ['User', 'Definitely.Try', 'Probably.Try', \n",
    "                  'Probably.Not.Try', 'Definitely.Not.Try']\n",
    "\n",
    "# Convert eCigUse to ordered categorical\n",
    "# The R data may store this as numeric codes -- let's check and convert appropriately\n",
    "if df['eCigUse'].dtype in ['float64', 'int64']:\n",
    "    # Map numeric codes to labels\n",
    "    code_to_label = {i: cat for i, cat in enumerate(category_order)}\n",
    "    df['eCigUse_cat'] = df['eCigUse'].map(code_to_label)\n",
    "else:\n",
    "    df['eCigUse_cat'] = df['eCigUse']\n",
    "\n",
    "df['eCigUse_ordered'] = pd.Categorical(df['eCigUse_cat'], \n",
    "                                        categories=category_order, \n",
    "                                        ordered=True)\n",
    "\n",
    "# Convert boolean columns to int for modeling\n",
    "for col in ['female', 'black', 'hispanic']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Create age dummies (R uses as.factor(Age) which creates dummies with Age=9 as reference)\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "age_dummies = pd.get_dummies(df['Age'], prefix='Age', drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"Sample size after dropping NAs: {len(df)}\")\n",
    "print(f\"\\nOutcome distribution:\")\n",
    "print(df['eCigUse_ordered'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Logit\n",
    "First, we'll estimate an Ordered Logit (the standard) where the errors are distributed according to a logistic distribution.\n",
    "\n",
    "**R equivalent:**\n",
    "```r\n",
    "library(MASS)\n",
    "vape.ologit <- polr(eCigUse ~ female + black + hispanic + as.factor(Age), data=eCigUse)\n",
    "```\n",
    "\n",
    "**Python:** We use `statsmodels.miscmodels.ordinal_model.OrderedModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "# Build the design matrix: female, black, hispanic, age dummies\n",
    "X = pd.concat([df[['female', 'black', 'hispanic']].reset_index(drop=True), \n",
    "               age_dummies.reset_index(drop=True)], axis=1)\n",
    "y = df['eCigUse_ordered'].reset_index(drop=True)\n",
    "\n",
    "# R: polr(eCigUse ~ female+black+hispanic+as.factor(Age), data=eCigUse)\n",
    "# Python: OrderedModel with distr='logit'\n",
    "# Note: polr() uses the logit link by default\n",
    "\n",
    "vape_ologit = OrderedModel(y, X, distr='logit')\n",
    "res_ologit = vape_ologit.fit(method='bfgs', disp=False)\n",
    "print(res_ologit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:** The coefficients above match the R output from `polr()`. The parameters include:\n",
    "- **Coefficients** (`female`, `black`, `hispanic`, `Age_*`): the $\\beta$ vector\n",
    "- **Thresholds** (the cutpoints): the $\\zeta$ parameters that partition the latent variable into observed categories\n",
    "\n",
    "Note: `statsmodels` reports thresholds directly in the parameter table, while R's `polr()` separates them into `$coefficients` and `$zeta` (called \"Intercepts\" in the R summary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we had a black female 14-year-old, the model would specify an outcome of:\n",
    "$$\\eta_i= 0.1153 +0.6066 +1.2458 =1.9677$$\n",
    "While a hispanic male 18-year-old:\n",
    "$$\\eta_i=-0.2034+0.2763=0.0729$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these observables, using the model we can illustrate the probabilities of the modal category as:\n",
    "\n",
    "![Animation](https://alistairjwilson.github.io/MQE_AW/i/eCigUse.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic distribution we can read in the probabilities of the shaded regions in the above graph as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual probability calculations\n",
    "# R: c( 1-plogis(1.1347 - 1.9677), plogis(0.145-0.0729), 1-plogis(1.13475-0.0729) )\n",
    "\n",
    "# P(Definitely Not Try | black female 14yr) = 1 - F(zeta_34 - eta)\n",
    "p_defnot_bf14 = 1 - stats.logistic.cdf(1.1347 - 1.9677)\n",
    "\n",
    "# P(User | hispanic male 18yr) = F(zeta_01 - eta)\n",
    "p_user_hm18 = stats.logistic.cdf(0.145 - 0.0729)\n",
    "\n",
    "# P(Definitely Not Try | hispanic male 18yr) = 1 - F(zeta_34 - eta)\n",
    "p_defnot_hm18 = 1 - stats.logistic.cdf(1.13475 - 0.0729)\n",
    "\n",
    "print(\"Probabilities from logistic distribution:\")\n",
    "print(f\"  P(Def Not Try | black female 14):     {p_defnot_bf14:.4f}\")\n",
    "print(f\"  P(User | hispanic male 18):           {p_user_hm18:.4f}\")\n",
    "print(f\"  P(Def Not Try | hispanic male 18):    {p_defnot_hm18:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Probit\n",
    "\n",
    "We can also estimate the model using the assumption that the error terms are Normally distributed, in which case we specify that we are using a probit formulation.\n",
    "\n",
    "**R equivalent:**\n",
    "```r\n",
    "vape.oprobit <- polr(eCigUse ~ female+black+hispanic+as.factor(Age), data=eCigUse, method = \"probit\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: polr(..., method = \"probit\")\n",
    "# Python: OrderedModel with distr='probit'\n",
    "\n",
    "vape_oprobit = OrderedModel(y, X, distr='probit')\n",
    "res_oprobit = vape_oprobit.fit(method='bfgs', disp=False)\n",
    "print(res_oprobit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model here actually does slightly better at organizing the data (using the AIC output), though the fundamental probabilities are not too distinct. Using the stored coefficients and the intercepts (stored as `zeta`) let's assemble the probabilities for:\n",
    "* A black female 14 year old being \"Definitely Not\"\n",
    "* A hispanic male 18 year old being \"Has used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients and thresholds from the probit model\n",
    "# R: vape.oprobit$coefficients and vape.oprobit$zeta\n",
    "# Python: res_oprobit.params contains both coefficients and thresholds\n",
    "\n",
    "all_params = res_oprobit.params\n",
    "print(\"All estimated parameters:\")\n",
    "print(all_params)\n",
    "\n",
    "# In statsmodels OrderedModel, the first len(X.columns) entries are betas,\n",
    "# and the remaining are the threshold parameters\n",
    "n_betas = X.shape[1]\n",
    "betas = all_params[:n_betas]\n",
    "thresholds = all_params[n_betas:]\n",
    "\n",
    "print(\"\\n--- Coefficients (betas) ---\")\n",
    "print(betas)\n",
    "print(\"\\n--- Thresholds (zeta) ---\")\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble probabilities using the probit model\n",
    "# R: vape.oprobit$coefficients and vape.oprobit$zeta\n",
    "\n",
    "# Extract named coefficients\n",
    "b_female = betas['female']\n",
    "b_black = betas['black']\n",
    "b_hispanic = betas['hispanic']\n",
    "\n",
    "# Find the Age_14 and Age_18 coefficient names\n",
    "age14_col = [c for c in betas.index if '14' in str(c)][0]\n",
    "age18_col = [c for c in betas.index if '18' in str(c)][0]\n",
    "b_age14 = betas[age14_col]\n",
    "b_age18 = betas[age18_col]\n",
    "\n",
    "# Get the threshold between Probably.Not.Try and Definitely.Not.Try\n",
    "# This is the last threshold (zeta_34)\n",
    "zeta_last = thresholds.iloc[-1]\n",
    "# And the first threshold (between User and Definitely.Try) -- zeta_01\n",
    "zeta_first = thresholds.iloc[0]\n",
    "\n",
    "# Black female 14: eta = b_female + b_black + b_age14\n",
    "eta_bf14 = b_female + b_black + b_age14\n",
    "# P(Definitely Not Try) = 1 - Phi(zeta_34 - eta)\n",
    "p_defnot_bf14_probit = 1 - stats.norm.cdf(zeta_last - eta_bf14)\n",
    "\n",
    "# Hispanic male 18: eta = b_hispanic + b_age18\n",
    "eta_hm18 = b_hispanic + b_age18\n",
    "# P(User) = Phi(zeta_01 - eta)\n",
    "p_user_hm18_probit = stats.norm.cdf(zeta_first - eta_hm18)\n",
    "\n",
    "print(\"Probit model probabilities:\")\n",
    "print(f\"  P(Def Not Try | black female 14):  {p_defnot_bf14_probit:.4f}  (R: 0.6909)\")\n",
    "print(f\"  P(User | hispanic male 18):        {p_user_hm18_probit:.4f}  (R: 0.5134)\")\n",
    "\n",
    "print(\"\\nLogit model probabilities (for comparison):\")\n",
    "print(f\"  P(Def Not Try | black female 14):  {1 - stats.logistic.cdf(1.1347 - 1.9677):.4f}\")\n",
    "print(f\"  P(User | hispanic male 18):        {stats.logistic.cdf(0.145 - 0.0729):.4f}\")\n",
    "\n",
    "print(\"\\nSome, but not major differences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Probabilities (Fitted Values)\n",
    "\n",
    "The one other term that is probably worth diving into a little here is the `fitted.values` -- a matrix of probability for being in each category for each data point.\n",
    "\n",
    "**R equivalent:**\n",
    "```r\n",
    "head(vape.oprobit$fitted.values)\n",
    "```\n",
    "\n",
    "**Python:** `model.predict()` returns the predicted probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: head(vape.oprobit$fitted.values)\n",
    "# Python: model.predict() returns probabilities for each category\n",
    "\n",
    "pred_probs = res_oprobit.predict()\n",
    "pred_probs_df = pd.DataFrame(pred_probs, columns=category_order)\n",
    "\n",
    "print(\"Predicted probabilities (first 6 observations):\")\n",
    "print(pred_probs_df.head(6).to_string(float_format='{:.6f}'.format))\n",
    "print(f\"\\nTotal observations: {len(pred_probs_df)}\")\n",
    "\n",
    "# Verify rows sum to 1\n",
    "print(f\"\\nRow sums (should all be 1.0): {pred_probs_df.sum(axis=1).unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Predicted Probabilities\n",
    "\n",
    "Let's visualize how the predicted probabilities vary with age, holding other characteristics fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities by profile\n",
    "# Create profiles for different age groups, holding demographics fixed\n",
    "\n",
    "ages = sorted(df['Age'].unique())\n",
    "ref_age = ages[0]  # reference category (dropped in dummies)\n",
    "\n",
    "# Profile: non-hispanic, non-black, male across ages\n",
    "profiles = []\n",
    "for age in ages:\n",
    "    row = {'female': 0, 'black': 0, 'hispanic': 0}\n",
    "    for a in ages[1:]:  # skip reference age\n",
    "        col_name = f'Age_{a}'\n",
    "        row[col_name] = 1 if age == a else 0\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles)\n",
    "# Ensure columns match X\n",
    "profile_df = profile_df.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Predict probabilities for these profiles\n",
    "pred_by_age = res_oprobit.model.predict(res_oprobit.params, exog=profile_df)\n",
    "pred_by_age_df = pd.DataFrame(pred_by_age, columns=category_order, index=ages)\n",
    "\n",
    "# Plot stacked area\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = [PITT_BLUE, PITT_GOLD, PITT_DGRAY, '#E87722', '#4CAF50']\n",
    "\n",
    "pred_by_age_df.plot.bar(stacked=True, color=colors, ax=ax, width=0.8)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Predicted Probability')\n",
    "ax.set_title('Predicted Category Probabilities by Age\\n(non-Hispanic, non-Black male)')\n",
    "ax.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MLE Implementation\n",
    "\n",
    "If `statsmodels.OrderedModel` is insufficient for your needs (e.g., you need custom constraints, different parameterizations, or want to understand the mechanics), you can write the ordered logit/probit log-likelihood from scratch and optimize with `scipy.optimize`.\n",
    "\n",
    "The key insight: for $K$ ordered categories with thresholds $\\zeta_1 < \\zeta_2 < \\ldots < \\zeta_{K-1}$ and linear predictor $\\eta_i = x_i^T\\beta$:\n",
    "\n",
    "$$P(y_i = k) = F(\\zeta_k - \\eta_i) - F(\\zeta_{k-1} - \\eta_i)$$\n",
    "\n",
    "where $\\zeta_0 = -\\infty$ and $\\zeta_K = +\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_model_loglik(params, X, y_codes, n_categories, distr='logit'):\n",
    "    \"\"\"\n",
    "    Log-likelihood for ordered logit/probit model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        First len(X.columns) entries are beta coefficients,\n",
    "        remaining (n_categories - 1) entries are threshold parameters.\n",
    "    X : ndarray\n",
    "        Design matrix (n x p)\n",
    "    y_codes : ndarray\n",
    "        Integer-coded outcome variable (0, 1, ..., K-1)\n",
    "    n_categories : int\n",
    "        Number of ordered categories K\n",
    "    distr : str\n",
    "        'logit' or 'probit'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    n_vars = X.shape[1]\n",
    "    beta = params[:n_vars]\n",
    "    # Use cumulative sum to enforce ordering of thresholds\n",
    "    raw_thresholds = params[n_vars:]\n",
    "    thresholds = np.cumsum(np.concatenate([[raw_thresholds[0]], \n",
    "                                           np.exp(raw_thresholds[1:])]))\n",
    "    \n",
    "    eta = X @ beta  # linear predictor\n",
    "    \n",
    "    # CDF function\n",
    "    if distr == 'logit':\n",
    "        F = stats.logistic.cdf\n",
    "    else:\n",
    "        F = stats.norm.cdf\n",
    "    \n",
    "    # Compute probabilities for each observation\n",
    "    ll = 0.0\n",
    "    for k in range(n_categories):\n",
    "        mask = (y_codes == k)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        if k == 0:\n",
    "            prob = F(thresholds[0] - eta[mask])\n",
    "        elif k == n_categories - 1:\n",
    "            prob = 1 - F(thresholds[-1] - eta[mask])\n",
    "        else:\n",
    "            prob = F(thresholds[k] - eta[mask]) - F(thresholds[k-1] - eta[mask])\n",
    "        prob = np.clip(prob, 1e-15, 1 - 1e-15)\n",
    "        ll += np.sum(np.log(prob))\n",
    "    \n",
    "    return ll\n",
    "\n",
    "print(\"Custom log-likelihood function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the custom ordered probit model\n",
    "X_arr = X.values.astype(float)\n",
    "y_codes = df['eCigUse_ordered'].cat.codes.values\n",
    "n_cats = len(category_order)\n",
    "\n",
    "# Initial values: zeros for betas, evenly spaced thresholds\n",
    "# For the reparameterized thresholds: first is raw, rest are log-gaps\n",
    "init_betas = np.zeros(X_arr.shape[1])\n",
    "init_thresh = np.array([0.0, 0.0, 0.0, 0.0])  # will become cumsum with exp\n",
    "x0_custom = np.concatenate([init_betas, init_thresh])\n",
    "\n",
    "# Maximize (minimize negative)\n",
    "res_custom = optimize.minimize(\n",
    "    lambda p: -ordered_model_loglik(p, X_arr, y_codes, n_cats, distr='probit'),\n",
    "    x0=x0_custom,\n",
    "    method='BFGS',\n",
    "    options={'maxiter': 5000, 'disp': False}\n",
    ")\n",
    "\n",
    "# Extract and display results\n",
    "n_vars = X_arr.shape[1]\n",
    "custom_betas = res_custom.x[:n_vars]\n",
    "raw_thresh = res_custom.x[n_vars:]\n",
    "custom_thresholds = np.cumsum(np.concatenate([[raw_thresh[0]], np.exp(raw_thresh[1:])]))\n",
    "\n",
    "print(\"Custom MLE Ordered Probit Results\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, val in zip(X.columns, custom_betas):\n",
    "    print(f\"  {name:20s} {val:10.5f}\")\n",
    "\n",
    "print(\"\\nThresholds:\")\n",
    "threshold_names = [f\"{category_order[i]}|{category_order[i+1]}\" \n",
    "                   for i in range(n_cats - 1)]\n",
    "for name, val in zip(threshold_names, custom_thresholds):\n",
    "    print(f\"  {name:40s} {val:10.5f}\")\n",
    "\n",
    "print(f\"\\nLog-likelihood: {-res_custom.fun:.2f}\")\n",
    "print(f\"AIC: {2 * res_custom.fun + 2 * len(res_custom.x):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare custom MLE to statsmodels results\n",
    "print(\"Comparison: Custom MLE vs statsmodels OrderedModel (Probit)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Parameter':30s} {'Custom':>12s} {'statsmodels':>12s}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "sm_params = res_oprobit.params\n",
    "\n",
    "# Print beta coefficients\n",
    "for i, name in enumerate(X.columns):\n",
    "    print(f\"{name:30s} {custom_betas[i]:12.5f} {sm_params.iloc[i]:12.5f}\")\n",
    "\n",
    "# Print thresholds\n",
    "for i, tname in enumerate(threshold_names):\n",
    "    print(f\"{tname:30s} {custom_thresholds[i]:12.5f} {sm_params.iloc[n_vars + i]:12.5f}\")\n",
    "\n",
    "print(f\"\\n{'Log-Likelihood':30s} {-res_custom.fun:12.2f} {res_oprobit.llf:12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Errors via Numerical Hessian\n",
    "\n",
    "For the custom MLE, we can compute standard errors using the inverse of the observed Fisher information (negative Hessian of the log-likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard errors from the inverse Hessian\n",
    "# scipy stores the inverse Hessian approximation from BFGS\n",
    "\n",
    "if hasattr(res_custom, 'hess_inv'):\n",
    "    # BFGS returns an approximation to the inverse Hessian\n",
    "    if hasattr(res_custom.hess_inv, 'todense'):\n",
    "        hess_inv = res_custom.hess_inv.todense()\n",
    "    else:\n",
    "        hess_inv = res_custom.hess_inv\n",
    "    custom_se = np.sqrt(np.diag(hess_inv))\n",
    "    \n",
    "    print(\"Standard Errors (Custom MLE via BFGS Hessian inverse):\")\n",
    "    print(f\"{'Parameter':30s} {'Estimate':>10s} {'Std Error':>10s} {'t-value':>10s}\")\n",
    "    print(\"-\" * 65)\n",
    "    for i, name in enumerate(X.columns):\n",
    "        t_val = custom_betas[i] / custom_se[i] if custom_se[i] > 0 else np.nan\n",
    "        print(f\"{name:30s} {custom_betas[i]:10.4f} {custom_se[i]:10.4f} {t_val:10.4f}\")\n",
    "else:\n",
    "    print(\"Hessian inverse not available from optimizer.\")\n",
    "    print(\"Computing numerically...\")\n",
    "    se = utils.mle_standard_errors(\n",
    "        lambda p: ordered_model_loglik(p, X_arr, y_codes, n_cats, distr='probit'),\n",
    "        res_custom.x\n",
    "    )\n",
    "    for i, name in enumerate(X.columns):\n",
    "        print(f\"{name:30s} {custom_betas[i]:10.4f} {se[i]:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Effects for Ordered Models\n",
    "\n",
    "Unlike in binary models where there is a single marginal effect, in ordered models each covariate has a marginal effect **for each category**. A positive coefficient $\\beta_j$ means:\n",
    "- Increasing $x_j$ **decreases** the probability of the lowest category\n",
    "- Increasing $x_j$ **increases** the probability of the highest category\n",
    "- The effect on middle categories is ambiguous\n",
    "\n",
    "For continuous variables with a probit link:\n",
    "$$\\frac{\\partial P(y=k | x)}{\\partial x_j} = \\left[\\phi(\\zeta_{k-1} - x^T\\beta) - \\phi(\\zeta_k - x^T\\beta)\\right] \\beta_j$$\n",
    "\n",
    "For the logit link, replace $\\phi$ with the logistic PDF $f(z) = \\frac{e^z}{(1+e^z)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_effects_ordered(params, X, n_categories, var_names, cat_names,\n",
    "                              distr='probit', at='mean'):\n",
    "    \"\"\"\n",
    "    Compute marginal effects for an ordered logit/probit model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        Model parameters [betas, thresholds]\n",
    "    X : ndarray\n",
    "        Design matrix\n",
    "    n_categories : int\n",
    "        Number of ordered categories\n",
    "    var_names : list\n",
    "        Names of explanatory variables\n",
    "    cat_names : list\n",
    "        Names of outcome categories\n",
    "    distr : str\n",
    "        'logit' or 'probit'\n",
    "    at : str\n",
    "        'mean' for marginal effects at means, 'average' for average marginal effects\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Marginal effects matrix (variables x categories)\n",
    "    \"\"\"\n",
    "    n_vars = len(var_names)\n",
    "    beta = params[:n_vars]\n",
    "    thresholds = params[n_vars:]\n",
    "    \n",
    "    if distr == 'logit':\n",
    "        pdf = stats.logistic.pdf\n",
    "    else:\n",
    "        pdf = stats.norm.pdf\n",
    "    \n",
    "    if at == 'mean':\n",
    "        X_eval = X.mean(axis=0).values.reshape(1, -1) if hasattr(X, 'values') else X.mean(axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        X_eval = X.values if hasattr(X, 'values') else X\n",
    "    \n",
    "    eta = X_eval @ beta  # (n, ) or (1,)\n",
    "    \n",
    "    me_matrix = np.zeros((n_vars, n_categories))\n",
    "    \n",
    "    for k in range(n_categories):\n",
    "        if k == 0:\n",
    "            # dP(y=0)/dx_j = -pdf(zeta_0 - eta) * beta_j\n",
    "            density = -pdf(thresholds[0] - eta)\n",
    "        elif k == n_categories - 1:\n",
    "            # dP(y=K-1)/dx_j = pdf(zeta_{K-2} - eta) * beta_j\n",
    "            density = pdf(thresholds[-1] - eta)\n",
    "        else:\n",
    "            # dP(y=k)/dx_j = [pdf(zeta_{k-1} - eta) - pdf(zeta_k - eta)] * beta_j\n",
    "            density = pdf(thresholds[k-1] - eta) - pdf(thresholds[k] - eta)\n",
    "        \n",
    "        if at == 'average':\n",
    "            avg_density = density.mean()\n",
    "        else:\n",
    "            avg_density = density[0]\n",
    "        \n",
    "        for j in range(n_vars):\n",
    "            me_matrix[j, k] = avg_density * beta[j]\n",
    "    \n",
    "    return pd.DataFrame(me_matrix, index=var_names, columns=cat_names)\n",
    "\n",
    "print(\"Marginal effects function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average marginal effects for the ordered probit model\n",
    "me_probit = marginal_effects_ordered(\n",
    "    res_oprobit.params.values, X, n_cats,\n",
    "    var_names=list(X.columns),\n",
    "    cat_names=category_order,\n",
    "    distr='probit',\n",
    "    at='average'\n",
    ")\n",
    "\n",
    "print(\"Average Marginal Effects (Ordered Probit)\")\n",
    "print(\"=\" * 80)\n",
    "print(me_probit.to_string(float_format='{:.5f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal effects at means\n",
    "me_at_means = marginal_effects_ordered(\n",
    "    res_oprobit.params.values, X, n_cats,\n",
    "    var_names=list(X.columns),\n",
    "    cat_names=category_order,\n",
    "    distr='probit',\n",
    "    at='mean'\n",
    ")\n",
    "\n",
    "print(\"Marginal Effects at Means (Ordered Probit)\")\n",
    "print(\"=\" * 80)\n",
    "print(me_at_means.to_string(float_format='{:.5f}'.format))\n",
    "\n",
    "print(\"\\nNote: Each row sums to approximately zero (probability must sum to 1):\")\n",
    "print(me_at_means.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize marginal effects for key variables\n",
    "key_vars = ['female', 'black', 'hispanic']\n",
    "me_subset = me_probit.loc[key_vars]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "colors = [PITT_BLUE, PITT_GOLD, PITT_DGRAY, '#E87722', '#4CAF50']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    ax = axes[i]\n",
    "    vals = me_subset.loc[var]\n",
    "    bar_colors = [colors[j] for j in range(len(vals))]\n",
    "    ax.bar(range(len(vals)), vals.values, color=bar_colors)\n",
    "    ax.set_xticks(range(len(vals)))\n",
    "    ax.set_xticklabels(['User', 'Def Try', 'Prob Try', \n",
    "                         'Prob Not', 'Def Not'], \n",
    "                        rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_title(f'AME: {var}')\n",
    "    ax.set_ylabel('Marginal Effect')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('Average Marginal Effects by Category (Ordered Probit)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Ordered Logit vs Ordered Probit\n",
    "\n",
    "Let's compare the two models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"Model Comparison: Ordered Logit vs Ordered Probit\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'':30s} {'Logit':>15s} {'Probit':>15s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "logit_params = res_ologit.params\n",
    "probit_params = res_oprobit.params\n",
    "\n",
    "for name in logit_params.index:\n",
    "    print(f\"{name:30s} {logit_params[name]:15.5f} {probit_params[name]:15.5f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Log-Likelihood':30s} {res_ologit.llf:15.2f} {res_oprobit.llf:15.2f}\")\n",
    "print(f\"{'AIC':30s} {res_ologit.aic:15.2f} {res_oprobit.aic:15.2f}\")\n",
    "print(f\"{'Observations':30s} {res_ologit.nobs:15.0f} {res_oprobit.nobs:15.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: R to Python Ordered Model Mapping\n",
    "\n",
    "| R (MASS) | Python (statsmodels) | Notes |\n",
    "|----------|---------------------|-------|\n",
    "| `polr(y ~ x, method=\"logistic\")` | `OrderedModel(y, X, distr='logit').fit(method='bfgs')` | Default in R is logit |\n",
    "| `polr(y ~ x, method=\"probit\")` | `OrderedModel(y, X, distr='probit').fit(method='bfgs')` | Normal errors |\n",
    "| `model$coefficients` | `result.params[:n_betas]` | Beta coefficients |\n",
    "| `model$zeta` | `result.params[n_betas:]` | Threshold/cutpoint parameters |\n",
    "| `model$fitted.values` | `result.predict()` | Predicted probability matrix |\n",
    "| `summary(model)` | `result.summary()` | Full output with std errors |\n",
    "| `AIC(model)` | `result.aic` | Akaike Information Criterion |\n",
    "| `plogis(x)` | `stats.logistic.cdf(x)` | Logistic CDF |\n",
    "| `pnorm(x)` | `stats.norm.cdf(x)` | Normal CDF |\n",
    "\n",
    "**Key differences:**\n",
    "- R's `polr()` stores coefficients and thresholds separately; `statsmodels` puts them all in `params`\n",
    "- R uses `as.factor()` for categorical variables; Python uses `pd.get_dummies()` or `pd.Categorical()`\n",
    "- For custom MLE, use `scipy.optimize.minimize` with the negative log-likelihood\n",
    "- The `utils` module provides `maximize_likelihood()` and `mle_standard_errors()` for custom implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}