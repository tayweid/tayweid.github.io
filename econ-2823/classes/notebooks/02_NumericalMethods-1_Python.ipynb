{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods (Python Version)\n",
    "___\n",
    "Adding some tools to our toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this unit will be to help us understand how we go from thinking about things fully theoretically, to making them numeric.\n",
    "\n",
    "* In this way we'll lose some generality, as we won't be able to think about things abstractly\n",
    "* But we'll also gain a whole bunch of concrete detail\n",
    "\n",
    "In general, as part of my process, I tend to always start out with fully analytical models:\n",
    "* This is where you figure out the moving parts\n",
    "* What are inputs/outputs\n",
    "* What's the objective here, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at some point the model either gets too complicated to solve, or I'm trying to move the model to data to estimate things concretely, at which point we move from having a theoretical model on a piece of paper, to trying to get this model to engage with numbers.\n",
    "\n",
    "Today we'll go through ways to mirror some of the analysis skills we've used in the theory sections, but within the computer.\n",
    "\n",
    "Our aims is to go over three key numerical tools:\n",
    "1. Numerical derivatives\n",
    "2. Numerical solutions to equations\n",
    "3. Numerical Optimization (next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils  # Our shared utility module\n",
    "\n",
    "# Set up plotting style\n",
    "utils.set_pitt_style()\n",
    "\n",
    "# Define Pitt colors for plotting\n",
    "PITT_BLUE = utils.PITT_BLUE\n",
    "PITT_GOLD = utils.PITT_GOLD\n",
    "PITT_GRAY = utils.PITT_GRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Equations\n",
    "\n",
    "Suppose that we have a simple system of equations:\n",
    "$$ \\begin{array}{rcl} 2 x_1+ 3 x_2 & = & 6 \\\\  x_1+ x_2 & = & 2 \\end{array}$$\n",
    "\n",
    "You probably solved this type of equation in high school, and the unique solution is:\n",
    "$$ \\begin{array}{rcl} x_1 & = & 0 \\\\  x_2 & = & 2 \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent this as a linear system:\n",
    "$$\\begin{array}{rcl} 2 x_1+ 3 x_2 & = & 6 \\\\  x_1+ x_2 & = & 2 \\end{array} \\Rightarrow \\left[ \\begin{array}{cc}\n",
    " 2 & 3 \\\\ \n",
    " 1 & 1 \\end{array}\\right] \n",
    " \\left(\\begin{array}{c}x_1\\\\x_2\\end{array}\\right)= \\left(\\begin{array}{c}6\\\\2\\end{array}\\right)\n",
    " $$\n",
    " \n",
    "So the linear equation system is $\\mathbf{A}\\mathbf{x}=\\mathbf{c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In R: A <- rbind(c(2,3), c(1,1))\n",
    "# In Python: Use numpy arrays\n",
    "A = np.array([[2, 3], \n",
    "              [1, 1]])\n",
    "\n",
    "c = np.array([6, 2])\n",
    "\n",
    "# Solve using np.linalg.solve (more numerically stable than using inverse)\n",
    "# R: solve(A) %*% c\n",
    "# Python:\n",
    "x = np.linalg.solve(A, c)\n",
    "print(\"Solution:\")\n",
    "print(f\"x1 = {x[0]}\")\n",
    "print(f\"x2 = {x[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, using matrix inverse (R's solve(A) returns the inverse)\n",
    "A_inv = np.linalg.inv(A)\n",
    "x_via_inv = A_inv @ c  # @ is matrix multiplication in Python\n",
    "print(\"Via inverse:\")\n",
    "print(x_via_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this example we could have done more quickly by hand, once the number of equations becomes large, linear algebra becomes very useful.\n",
    "\n",
    "Implicitly, you've been using the benefits of this when minimizing the total sum of squares for a regression model:\n",
    "$$ \\sum_i\\left(y_{i}-(\\beta_0+\\beta_1 x_{i,1}+\\ldots +\\beta_k x_{i,k})\\right)^2 $$\n",
    "\n",
    "The OLS solution is:\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Derivatives\n",
    "\n",
    "## Analytical Derivatives\n",
    "\n",
    "### Easy Ones\n",
    "Sometimes it is trivial for us to take a derivative:\n",
    "$$ f(x)=a x^2+b x+c \\quad \\Rightarrow \\quad f^\\prime(x)=2 a x+b $$\n",
    "\n",
    "### Medium Ones\n",
    "Other times, we can probably solve something, it just takes us a bit longer:\n",
    "$$ g(x)=(a x^2+b x+c)\\cdot e^{-\\lambda x}$$\n",
    "$$ f^\\prime(x)=-\\lambda e^{-\\lambda x}(a x^2 +bx +c)+e^{-\\lambda x}(2ax+b)$$\n",
    "\n",
    "### Hurt-Your-Head Ones\n",
    "And still other times, it's just not worth really thinking about the analytical formula..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients & Derivatives\n",
    "\n",
    "### Discrete changes\n",
    "A lot of the time when we're thinking about changes, it's usually a discrete quantity change:\n",
    "$$x\\mapsto x+\\Delta x$$ \n",
    "\n",
    "The underlying gradient is the ratio of the movements:\n",
    "$$ \\frac{\\Delta y}{\\Delta x}=\\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$\n",
    "\n",
    "### Derivatives\n",
    "As we make the change in $x$ smaller and smaller, this gradient approaches the derivative:\n",
    "$$\\tfrac{d}{dx}y= \\lim_{\\Delta x\\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding it up\n",
    "\n",
    "Let's go back to our simplest example $f(x)=a x^2+b x+c$,\n",
    "but where we make the problem exact by setting $a=2$, $b=-10$ and $c=3$\n",
    "\n",
    "So:\n",
    "* $f(x)=2 x^2- 10 x+3$\n",
    "* $f^\\prime(x)=4 x-10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a numerical derivative function\n",
    "# R: num.deriv <- function(f, x, eps) { (f(x+eps)-f(x))/eps }\n",
    "def num_deriv(f, x, eps):\n",
    "    \"\"\"Forward difference numerical derivative.\"\"\"\n",
    "    return (f(x + eps) - f(x)) / eps\n",
    "\n",
    "# Define the function we're interested in taking derivatives of\n",
    "def easy_f(x):\n",
    "    \"\"\"f(x) = 2x^2 - 10x + 3\"\"\"\n",
    "    return 2*x**2 - 10*x + 3\n",
    "\n",
    "# Define the actual (analytical) derivative\n",
    "def d_easy_f(x):\n",
    "    \"\"\"f'(x) = 4x - 10\"\"\"\n",
    "    return 4*x - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Numerical Derivatives!\n",
    "eps = 1e-7\n",
    "nd = num_deriv(easy_f, 3, eps)\n",
    "ad = d_easy_f(3)\n",
    "err = nd - ad\n",
    "\n",
    "print(f\"Numerical derivative:  {nd:.10f}\")\n",
    "print(f\"Analytical derivative: {ad:.10f}\")\n",
    "print(f\"Error:                 {err:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "\n",
    "Let's try our medium-hard example where we'll just take the product of our previous quadratic function with an exponential function:\n",
    "\n",
    "$$\\left(2 x^2- 10 x+3 \\right)e^{-x/10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the medium-hard function\n",
    "def med_f(x):\n",
    "    \"\"\"f(x) = (2x^2 - 10x + 3) * exp(-x/10)\"\"\"\n",
    "    return easy_f(x) * np.exp(-x/10)\n",
    "\n",
    "# Analytical derivative using product rule\n",
    "def d_med_f(x):\n",
    "    \"\"\"f'(x) = -0.1*exp(-0.1x)*(2x^2-10x+3) + exp(-0.1x)*(4x-10)\"\"\"\n",
    "    return -0.1 * np.exp(-0.1*x) * easy_f(x) + np.exp(-0.1*x) * (4*x - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Numerical Derivatives!\n",
    "eps = 1e-5\n",
    "nd = num_deriv(med_f, 3, eps)\n",
    "ad = d_med_f(3)\n",
    "err = nd - ad\n",
    "\n",
    "print(f\"Numerical derivative:  {nd:.10f}\")\n",
    "print(f\"Analytical derivative: {ad:.10f}\")\n",
    "print(f\"Error:                 {err:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a graph of the function\n",
    "x = np.linspace(0, 100, 200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x, med_f(x), color=PITT_BLUE, linewidth=2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Function Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical derivatives for every x from 0 to 100\n",
    "x_range = np.arange(0, 101)\n",
    "nd_med_f = np.array([num_deriv(med_f, xi, 1e-5) for xi in x_range])\n",
    "\n",
    "# Draw both analytical and numerical derivatives\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, d_med_f(x_range), color=PITT_BLUE, linewidth=2, label='Analytical')\n",
    "ax.scatter(x_range, nd_med_f, color=PITT_GOLD, s=30, zorder=5, label='Numerical')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x)\")\n",
    "ax.set_title('Derivative Graph')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the errors (numerical - actual)\n",
    "nd_med_f_error = nd_med_f - d_med_f(x_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x_range, nd_med_f_error, color=PITT_GOLD, s=30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_title('Errors (Forward Difference)')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor Expansion and Better Formulas\n",
    "\n",
    "One way to think through this procedure is to consider the **Taylor expansion** of the function around the point $x$ for any deviation $\\Delta x$:\n",
    "\n",
    "$$f(x+\\Delta x)=f(x)+\\frac{f^\\prime(x)}{1!}\\Delta x+\\frac{f^{\\prime\\prime}(x)}{2!}\\Delta x^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\Delta x^3+\\ldots$$\n",
    "\n",
    "So our forward difference formula gives us:\n",
    "$$\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}=f^\\prime(x) + \\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon + \\ldots = f^\\prime(x) + O(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered Difference\n",
    "\n",
    "If we use both a forward and a backward increment to generate a *centered difference* equation:\n",
    "$$\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}=f^\\prime(x)+O(\\epsilon^2)$$\n",
    "\n",
    "This is a more accurate approximation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centered numerical derivative (O(eps^2) accuracy)\n",
    "def c_num_deriv(f, x, eps):\n",
    "    \"\"\"Centered difference numerical derivative.\"\"\"\n",
    "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "# This is also available in our utils module:\n",
    "# utils.numerical_derivative(f, x, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare forward vs centered difference\n",
    "eps = 1e-4\n",
    "c_nd_med_f = np.array([c_num_deriv(med_f, xi, eps) for xi in x_range])\n",
    "nd_med_f = np.array([num_deriv(med_f, xi, eps) for xi in x_range])\n",
    "\n",
    "c_nd_med_f_error = c_nd_med_f - d_med_f(x_range)\n",
    "nd_med_f_error = nd_med_f - d_med_f(x_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x_range, nd_med_f_error, color=PITT_BLUE, s=30, label='Forward Difference', alpha=0.7)\n",
    "ax.scatter(x_range, c_nd_med_f_error, color=PITT_GOLD, s=30, label='Centered Difference', alpha=0.7)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x) - (Derivative Approximation)\")\n",
    "ax.set_title('Errors: Forward vs Centered Difference')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Formulas via Linear Algebra\n",
    "\n",
    "Using Taylor expansion with 5 points, we can derive even more accurate formulas.\n",
    "\n",
    "We set up the system:\n",
    "$$ \\mathbf{f}=\\mathbf{A}\\mathbf{d} $$\n",
    "\n",
    "where $\\mathbf{f}$ contains function evaluations at $x-2\\epsilon, x-\\epsilon, x, x+\\epsilon, x+2\\epsilon$, and $\\mathbf{d}$ contains $D_0f = f(x)$, $D_1f = f'(x)\\epsilon$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Taylor coefficient matrix\n",
    "# R: A=matrix(1:35, nrow = 5, ncol = 5) then loop\n",
    "A = np.zeros((5, 5))\n",
    "for i, offset in enumerate([-2, -1, 0, 1, 2]):\n",
    "    for j in range(5):\n",
    "        A[i, j] = (offset ** j) / np.math.factorial(j)\n",
    "\n",
    "# Reorder rows to match R notebook: [f(x+2e), f(x+e), f(x), f(x-e), f(x-2e)]\n",
    "A = A[[4, 3, 2, 1, 0], :]\n",
    "\n",
    "print(\"Taylor coefficient matrix A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weights for first derivative (D_1 f)\n",
    "b = np.array([0, 1, 0, 0, 0])  # Select D_1 f (the first derivative term)\n",
    "A_inv = np.linalg.inv(A)\n",
    "w1 = b @ A_inv\n",
    "\n",
    "print(\"Weights for first derivative (O(eps^4) method):\")\n",
    "labels = ['f(x+2d)', 'f(x+d)', 'f(x)', 'f(x-d)', 'f(x-2d)']\n",
    "for label, weight in zip(labels, w1):\n",
    "    print(f\"  {label}: {weight:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weights for second derivative (D_2 f)\n",
    "b = np.array([0, 0, 1, 0, 0])  # Select D_2 f (the second derivative term)\n",
    "w2 = b @ A_inv\n",
    "\n",
    "print(\"Weights for second derivative (O(eps^3) method):\")\n",
    "for label, weight in zip(labels, w2):\n",
    "    print(f\"  {label}: {weight:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-order first derivative formula (O(eps^4) accuracy)\n",
    "def n_deriv_1(f, x, eps):\n",
    "    \"\"\"Five-point stencil first derivative (O(eps^4) accuracy).\"\"\"\n",
    "    return (w1[0]*f(x+2*eps) + w1[1]*f(x+eps) + w1[2]*f(x) + \n",
    "            w1[3]*f(x-eps) + w1[4]*f(x-2*eps)) / eps\n",
    "\n",
    "# Higher-order second derivative formula (O(eps^3) accuracy)\n",
    "def n_deriv_2(f, x, eps):\n",
    "    \"\"\"Five-point stencil second derivative (O(eps^3) accuracy).\"\"\"\n",
    "    return (w2[0]*f(x+2*eps) + w2[1]*f(x+eps) + w2[2]*f(x) + \n",
    "            w2[3]*f(x-eps) + w2[4]*f(x-2*eps)) / eps**2\n",
    "\n",
    "# Test on easy function\n",
    "print(f\"n_deriv_1(med_f, 2, 1e-5) = {n_deriv_1(med_f, 2, 1e-5):.6f}\")\n",
    "print(f\"Actual d_med_f(2) = {d_med_f(2):.6f}\")\n",
    "print(f\"\\nn_deriv_2(easy_f, 2, 1e-4) = {n_deriv_2(easy_f, 2, 1e-4):.6f}\")\n",
    "print(f\"Actual (2nd deriv of 2x^2-10x+3 = 4)\")\n",
    "\n",
    "# Test on sin function\n",
    "print(f\"\\nn_deriv_1(sin, 0.2, 0.001) = {n_deriv_1(np.sin, 0.2, 0.001):.6f}\")\n",
    "print(f\"Actual cos(0.2) = {np.cos(0.2):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare errors: centered vs high-order formula\n",
    "high_nd_med_f = np.array([n_deriv_1(med_f, xi, 1e-6) for xi in x_range])\n",
    "high_nd_med_f_error = high_nd_med_f - d_med_f(x_range)\n",
    "\n",
    "c_nd_med_f_error = np.array([c_num_deriv(med_f, xi, 1e-6) for xi in x_range]) - d_med_f(x_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x_range, c_nd_med_f_error, color=PITT_BLUE, s=30, label='Centered (O(eps^2))', alpha=0.7)\n",
    "ax.scatter(x_range, high_nd_med_f_error, color=PITT_GOLD, s=30, label='Five-point (O(eps^4))', alpha=0.7)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x) - (Derivative Approximation)\")\n",
    "ax.set_title('Errors: Centered vs Five-Point Formula')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Solutions to an Equation\n",
    "\n",
    "## Non-linear Equations\n",
    "\n",
    "Linear equations have well-defined solutions:\n",
    "* We know *if* we can solve a system based on the rank of the matrix\n",
    "* If it's solvable, there's a unique solution\n",
    "* And we know the exact formula for this solution\n",
    "\n",
    "Non-linear equations are tougher! Some are easily invertible:\n",
    "$$ x^4= 12\\Rightarrow x = 12^{1/4} $$\n",
    "\n",
    "Others are not:\n",
    "$$ x\\cdot e^{x^2+3x}= 3$$\n",
    "\n",
    "And non-linear equations can admit multiple solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson Method\n",
    "\n",
    "The core method is the Newton-Raphson iteration to find the root of a continuous function $f(x)=0$:\n",
    "\n",
    "**Method Procedure:**\n",
    "1. Guess an initial solution $x_0$\n",
    "2. Figure out the value of the function at this point $f(x_0)$\n",
    "3. Figure out the derivative of the function here $f^\\prime(x_0)$\n",
    "\n",
    "Then output a new guess:\n",
    "$$ x_{n+1}= x_n - \\dfrac{f(x_n)}{f^\\prime(x_n)} $$\n",
    "\n",
    "Stop whenever $|x_{n+1}-x_n|$ is small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson(f, x0, tol=1e-8, eps=1e-6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Newton-Raphson root finding algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function for which to find root\n",
    "    x0 : float\n",
    "        Initial guess\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    eps : float\n",
    "        Step size for numerical derivative\n",
    "    max_iter : int\n",
    "        Maximum iterations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Approximate root\n",
    "    \"\"\"\n",
    "    xx = x0\n",
    "    fval = f(xx)\n",
    "    error = abs(fval)\n",
    "    ii = 1\n",
    "    \n",
    "    while error > tol and ii < max_iter:\n",
    "        fd = n_deriv_1(f, xx, eps)  # numerical derivative\n",
    "        if abs(fd) < 1e-12:\n",
    "            print(\"Derivative too small\")\n",
    "            break\n",
    "        xx = xx - fval / fd  # Newton-Raphson iteration\n",
    "        fval = f(xx)\n",
    "        error = abs(fval)\n",
    "        ii += 1\n",
    "    \n",
    "    if ii >= max_iter:\n",
    "        print(\"Exited due to non-convergence\")\n",
    "    \n",
    "    print(f\"{ii} steps\")\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual roots from quadratic formula for 2x^2 - 10x + 3 = 0\n",
    "actual_roots = {\n",
    "    'Low Root': (10 - np.sqrt(100 - 4*2*3)) / (2*2),\n",
    "    'High Root': (10 + np.sqrt(100 - 4*2*3)) / (2*2)\n",
    "}\n",
    "print(\"Actual roots:\")\n",
    "for name, val in actual_roots.items():\n",
    "    print(f\"  {name}: {val:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Newton-Raphson starting from x0=4\n",
    "print(\"\\nStarting from x0=4:\")\n",
    "root = newton_raphson(easy_f, 4)\n",
    "print(f\"Found root: {root:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from x0=1 (closer to the low root)\n",
    "print(\"Starting from x0=1:\")\n",
    "root = newton_raphson(easy_f, 1)\n",
    "print(f\"Found root: {root:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function and its roots\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(-1, 6, 200)\n",
    "ax.plot(x, easy_f(x), color=PITT_BLUE, linewidth=2, label='f(x) = 2xÂ² - 10x + 3')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Mark the roots\n",
    "for name, root in actual_roots.items():\n",
    "    ax.scatter([root], [0], color=PITT_GOLD, s=100, zorder=5)\n",
    "    ax.annotate(f'{name}\\n{root:.3f}', (root, 0), textcoords='offset points', \n",
    "                xytext=(0, 15), ha='center')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Function with Roots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the medium function\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(0, 10, 200)\n",
    "ax.plot(x, med_f(x), color=PITT_BLUE, linewidth=2)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Medium Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find roots of medium function\n",
    "print(\"Starting from x0=7.5:\")\n",
    "root1 = newton_raphson(med_f, 7.5)\n",
    "print(f\"Found root: {root1:.8f}\")\n",
    "\n",
    "print(\"\\nStarting from x0=0.5:\")\n",
    "root2 = newton_raphson(med_f, 0.5)\n",
    "print(f\"Found root: {root2:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SciPy for Root Finding\n",
    "\n",
    "Obviously, we shouldn't try to reinvent the wheel. SciPy provides many robust solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve, brentq, newton\n",
    "\n",
    "# Using scipy.optimize.newton (similar to our Newton-Raphson)\n",
    "print(\"Using scipy.optimize.newton:\")\n",
    "root_scipy = newton(easy_f, 4)\n",
    "print(f\"Found root: {root_scipy:.8f}\")\n",
    "\n",
    "# Using fsolve (more general solver)\n",
    "print(\"\\nUsing scipy.optimize.fsolve:\")\n",
    "root_fsolve = fsolve(easy_f, 4)[0]\n",
    "print(f\"Found root: {root_fsolve:.8f}\")\n",
    "\n",
    "# Using brentq (bracketing method - needs interval containing root)\n",
    "print(\"\\nUsing scipy.optimize.brentq:\")\n",
    "root_brent = brentq(easy_f, 0, 1)  # Search between 0 and 1\n",
    "print(f\"Found root: {root_brent:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we covered:\n",
    "\n",
    "1. **Numerical Derivatives**\n",
    "   - Forward difference: $O(\\epsilon)$ accuracy\n",
    "   - Centered difference: $O(\\epsilon^2)$ accuracy  \n",
    "   - Five-point stencil: $O(\\epsilon^4)$ accuracy\n",
    "   - Using Taylor expansion to derive formulas\n",
    "\n",
    "2. **Root Finding**\n",
    "   - Newton-Raphson method\n",
    "   - Importance of starting point\n",
    "   - SciPy alternatives\n",
    "\n",
    "In the next notebook, we'll cover **Numerical Optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Shared Utils Module\n",
    "\n",
    "Many of these functions are also available in our `utils.py` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical derivative (centered difference)\n",
    "print(f\"utils.numerical_derivative(med_f, 3): {utils.numerical_derivative(med_f, 3):.8f}\")\n",
    "print(f\"Actual d_med_f(3): {d_med_f(3):.8f}\")\n",
    "\n",
    "# Five-point stencil\n",
    "print(f\"\\nutils.numerical_derivative_high_order(med_f, 3): {utils.numerical_derivative_high_order(med_f, 3):.8f}\")\n",
    "\n",
    "# Newton-Raphson\n",
    "root, iters, converged = utils.newton_raphson(easy_f, 4)\n",
    "print(f\"\\nutils.newton_raphson(easy_f, 4): root={root:.8f}, iterations={iters}, converged={converged}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
