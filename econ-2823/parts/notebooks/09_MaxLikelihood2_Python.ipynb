{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, optimize\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import utils\n",
    "\n",
    "# Set up plotting style\n",
    "utils.set_pitt_style()\n",
    "PITT_BLUE = utils.PITT_BLUE\n",
    "PITT_GOLD = utils.PITT_GOLD\n",
    "PITT_GRAY = utils.PITT_GRAY\n",
    "PITT_LGRAY = utils.PITT_LGRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Maximum Likelihood 2: Inference\n",
    "---\n",
    "Here we'll try to do two things:\n",
    "1. Outline some properties of Maximum Likelihood estimates\n",
    "2. Examine how we can construct standard-errors and tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "properties-cell",
   "metadata": {},
   "source": [
    "## Properties of the Maximum Likelihood estimates:\n",
    "---\n",
    "Under some technical assumptions, the MLE estimator $\\hat{\\boldsymbol{\\theta}}$ of an identifiable parameter vector $\\boldsymbol{\\theta}_0$  has the following properties:\n",
    "1. Consistency: $\\hat{\\boldsymbol{\\theta}}$ converges in probability to the true parameter  $\\boldsymbol{\\theta}_0$\n",
    "2. Asymptotic Normality: $\\sqrt{n}\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0\\right)$ converges in distribution to $\\mathcal{N}(0,\\Sigma)$\n",
    "3. The estimator is efficient, where the asymptotic variance-covariance matrix $\\Sigma$ is the smallest possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-sample-cell",
   "metadata": {},
   "source": [
    "Note that the above results are all large sample results\n",
    "* We've already seen that the maximum-likelihood estimators can be **biased** in finite samples!\n",
    "* Similarly, all of our inference will rely on the sample being *big*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proviso-cell",
   "metadata": {},
   "source": [
    "With that proviso, let's dive into how we can compute the variance-covariance matrix $\\Sigma$, and from this derive:\n",
    "* standard errors\n",
    "* confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-theory-cell",
   "metadata": {},
   "source": [
    "## Theory for the Variance-Covariance of MLE estimators\n",
    "---\n",
    "To find an estimator for $\\Sigma$ we need to define a few other terms first.\n",
    "\n",
    "First, we define the **score vector** assessed with any possible parameter $\\boldsymbol{\\theta}$ value as:\n",
    "$$ \\text{score vector}: \\mathbf{s}(\\boldsymbol{\\theta}) =\\frac{\\partial\\log L(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$$\n",
    "\n",
    "So this is a vector of partial derivatives for the log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chain-rule-cell",
   "metadata": {},
   "source": [
    "But from the chain rule, we know that this will be given by:\n",
    "$$\\mathbf{s}(\\boldsymbol{\\theta})=\\frac{1}{L(\\boldsymbol{\\theta})}\\frac{\\partial L(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-info-cell",
   "metadata": {},
   "source": [
    "Second, we define the **Fisher Information Matrix** assessed at the **true** parameter value $\\boldsymbol{\\theta}_0$ as:\n",
    "$$\\text{Information matrix: }\\mathbf{I}_{\\boldsymbol{\\theta}_0}=\\mathbb{E}\\left[ \\mathbf{s}(\\boldsymbol{\\theta}_0) \\mathbf{s}(\\boldsymbol{\\theta}_0)^T \\right]= -\\mathbb{E}\\left[ \\frac{\\partial\\log L(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^T} \\right] $$\n",
    "The expectations here are over the realizations of our data which will be distributed according to $y$ (or $y|\\mathbf{X}$ if we have covariates), so while the average value might have zero score, this will look a lot like the square of a mean-zero error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cramer-rao-cell",
   "metadata": {},
   "source": [
    "## Cramer-Rao Inequality\n",
    "A fundamental result in statistics is that under some technical restrictions on the underlying distributions:\n",
    "\n",
    "If we have a vector of observations $\\mathbf{z}$ which has a density given by $f(\\mathbf{z};\\boldsymbol{\\theta}_0)$ for a finite set of parameters $\\boldsymbol{\\theta}_0$. Given the likelihood function $L(\\tilde{\\boldsymbol{\\theta}})=f(\\mathbf{z};\\tilde{\\boldsymbol{\\theta}})$,  any unbiased estimator $\\hat{\\boldsymbol{\\theta}}(\\mathbf{z})$ of $\\boldsymbol{\\theta}_0$ satisfies:\n",
    "\n",
    "$$\\text{Var}\\left(\\hat{\\boldsymbol{\\theta}}(\\mathbf{z})\\right) \\geq \\tfrac{1}{n}\\mathbf{I}(\\boldsymbol{\\theta}_0)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficiency-cell",
   "metadata": {},
   "source": [
    "So, because the maximum likelihood estimator's asymptotic variance is exactly this, that means that the ML-estimator is asymptotically efficient!\n",
    "\n",
    "If you have the right probability distribution, and a large amount of data, there is no better method than ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "poisson-theory-cell",
   "metadata": {},
   "source": [
    "## Poisson Example (Theory)\n",
    "---\n",
    "Let's take our simple Poisson model for count data as an example, where the data is as series of counts $(k_1,k_2,\\ldots,k_n)$ and there is a solitary parameter $\\lambda$. The Likelihood function is given by:\n",
    "$$ L(\\lambda)= \\prod_{i=1}^N \\frac{\\lambda^{k_i} e^{-\\lambda}}{k_i !}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log-likelihood-cell",
   "metadata": {},
   "source": [
    "So the Log-likelihood is given by:\n",
    "$$ l(\\lambda)= \\sum_{i=1}^N  \\left(k_i\\cdot\\log(\\lambda)-\\lambda -\\log(k_i !)\\right)$$\n",
    "\n",
    "which means the score is????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-deriv-cell",
   "metadata": {},
   "source": [
    "The score is the derivative of the log-likelihood with respect to the parameters, which here is the scalar $\\lambda$, so we have:\n",
    "$$ s(\\lambda)=\\frac{\\partial l(\\lambda)}{\\partial \\lambda}= \\sum^{N}_{i=1} \\frac{k_i}{\\lambda}-N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-formula-cell",
   "metadata": {},
   "source": [
    "One formula we have for the Fisher Information matrix is given by:\n",
    "$$ \\mathbf{I}(\\lambda)=\\mathbb{E}\\left[s(\\lambda)^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-expand-cell",
   "metadata": {},
   "source": [
    "which simplifies to:\n",
    "$$\\mathbf{I}(\\lambda)=\\mathbb{E}\\left[\\left( \\sum^{N}_{i=1} \\frac{k_i}{\\lambda}-N  \\right)^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-expand2-cell",
   "metadata": {},
   "source": [
    "We can expand this to...\n",
    "$$\\mathbf{I}(\\lambda) = \\mathbb{E}\\left[ N^2 -\\frac{2 N}{\\lambda}\\sum^{N}_{i=1} k_i + \\frac{1}{\\lambda^2}\\left(\\sum^{N}_{i=1} k_i\\right)^2 \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-simplify-cell",
   "metadata": {},
   "source": [
    "Take out $N$ as a common element and write $\\tfrac{1}{N}\\sum_i k_i=\\overline{k}$ to get\n",
    "$$\\mathbf{I}(\\lambda) = \\frac{N^2}{\\lambda^2}\\mathbb{E}\\left[ \\lambda^2 -2\\lambda \\overline{k} +\\overline{k}^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expectations-cell",
   "metadata": {},
   "source": [
    "Moving the expectations inside the square bracket we get:\n",
    "$$\\mathbf{I}(\\lambda)=\\frac{N^2}{\\lambda^2}\\left[ \\lambda^2 -2\\lambda \\mathbb{E}\\overline{k} +\\mathbb{E}\\overline{k}^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iid-props-cell",
   "metadata": {},
   "source": [
    "We know that each of the data draws $k_i$ is an *iid* draw from a Poisson$(\\lambda)$, so the data draws have the property that:\n",
    "* $\\mathbb{E}k_i=\\lambda$, so that means $\\mathbb{E}\\overline{k}=\\lambda$\n",
    "* $\\text{Var}(k_i)=\\lambda$, which means that $\\mathbb{E}k_i^2=\\lambda(1+\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-result-cell",
   "metadata": {},
   "source": [
    "So we can simplify this expression to: \n",
    "$$\\mathbf{I}(\\lambda)=\\frac{N^2}{\\lambda^2}\\left[ -\\lambda^2 + \\tfrac{N(N-1)}{N^2}\\cdot \\lambda^2 +\\tfrac{1}{N}\\lambda\\cdot(1+\\lambda) \\right]$$\n",
    "\n",
    "which if you multiply through you would find that:\n",
    "$$\\mathbf{I}(\\lambda)= \\frac{N}{\\lambda} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hessian-shortcut-cell",
   "metadata": {},
   "source": [
    "In this particular case, the other formula for the Information matrix would have been far quicker!!\n",
    "$$ \\mathbf{I}(\\lambda)=-\\mathbb{E}\\left[\\frac{\\partial^2 \\log L(\\lambda)}{\\partial \\lambda^2}\\right]=-\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\lambda} s(\\lambda)\\right]$$\n",
    "which is\n",
    "$$\\mathbf{I}(\\lambda)=-\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\lambda}\\left( \\sum^{N}_{i=1} \\frac{k_i}{\\lambda}-N \\right)\\right]= \\left[\\sum^{N}_{i=1} \\frac{\\mathbb{E} k_i}{\\lambda^2}\\right]=\\frac{N}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identity-cell",
   "metadata": {},
   "source": [
    "... and so our identity holds true!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "poisson-conclusion-cell",
   "metadata": {},
   "source": [
    "## Poisson conclusion:\n",
    "---\n",
    "So the Cramer-Rao inequality tells us that every possible unbiased estimators for $\\lambda$ has a variance of at least: \n",
    "$$\\mathbf{I}(\\lambda)^{-1}=\\tfrac{\\lambda}{N}$$\n",
    "and that asymptotically our MLE estimator will have this variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcov-heading-cell",
   "metadata": {},
   "source": [
    "## Calculation of variance-covariance matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcov-theory-cell",
   "metadata": {},
   "source": [
    "The theory tells us that the asymptotic variance-covariance matrix for $\\sqrt{n}\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0\\right)$ is given by $\\mathbf{I}_{\\boldsymbol{\\theta}_0}^{-1}$. \n",
    "\n",
    "If we knew the true values for the parameters we could calculate these things as in the above example. But typically we *don't* know the true parameters, we have an estimate of them,  and we're trying to get a sense for how inaccurate our estimator is through the theoretical variance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach-cell",
   "metadata": {},
   "source": [
    "We'll proceed in a similar way then to how we constructed *standard errors* in OLS:\n",
    "* Using our estimates of the parameters, we'll assemble an *estimator* of the variance-covariance matrix\n",
    "* Using this estimated variance, we then create an estimator for the variability of our parameter estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "two-options-cell",
   "metadata": {},
   "source": [
    "From the above we have two options for computing an estimator for $\\mathbf{I}_{\\boldsymbol{\\theta}_0}$:\n",
    "1. Calculate the score vector $s(\\hat{\\boldsymbol{\\theta}})$ *at each data point* and then take averages to get an approximation for the expectation at $\\boldsymbol{\\theta}_0$\n",
    "2. Figure out the Hessian matrix $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ (the second derivatives) for the log-likelihood (again, at each data point), and average across these data points to get an approximation of the expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-vs-hessian-cell",
   "metadata": {},
   "source": [
    "While the Hessian can sometimes be easier in *analytical* calculations (per the above), the score is typically less computationally intensive if we're relying on numerical methods.\n",
    "\n",
    "However we'll again do both for the simple Poisson example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "poisson-estimator-cell",
   "metadata": {},
   "source": [
    "## Poisson Estimator\n",
    "---\n",
    "The MLE estimator for the simple Poisson case was that $ \\hat{\\lambda}=\\overline{k}$, the average in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pois-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: rpois(400, 2)\n",
    "# Python: np.random.poisson(2, 400)\n",
    "np.random.seed(42)\n",
    "pois_data = np.random.poisson(2, 400)  # true value of lambda is 2\n",
    "lambda_hat = np.mean(pois_data)\n",
    "print(f\"lambda_hat = {lambda_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-obs-cell",
   "metadata": {},
   "source": [
    "The score for any particular observation is given by:\n",
    "$$s(k_j)=\\frac{\\partial}{\\partial \\lambda}\\log L(\\lambda;k_j)=\\frac{k_j}{\\lambda}-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-fisher-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: pois.score.est <- function(kj) ( kj / lambda.hat - 1 )\n",
    "# Python: vectorized\n",
    "def pois_score_est(kj):\n",
    "    return kj / lambda_hat - 1\n",
    "\n",
    "# R: scores <- sapply(pois.data, pois.score.est)\n",
    "# Python: vectorized over numpy array\n",
    "scores = pois_score_est(pois_data)\n",
    "\n",
    "# Take the average of the square of the scores to get Fisher Information\n",
    "# R: Fisher.Inf <- sum(scores**2) / length(scores)\n",
    "Fisher_Inf = np.mean(scores**2)\n",
    "\n",
    "# The variance estimator is then the reciprocal of this\n",
    "EstVar = 1.0 / Fisher_Inf\n",
    "print(f\"Estimated asymptotic variance: {EstVar:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "se-theory-cell",
   "metadata": {},
   "source": [
    "But the theory here tells us that $\\sqrt{n}(\\hat{\\lambda}-\\lambda )$ converges in distribution to a mean-zero normal with this variance, so our standard error on the estimate of lambda is given by:\n",
    "$$\\frac{\\sqrt{\\hat{\\text{Var}}}}{\\sqrt{n}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "estimate-se-cell",
   "metadata": {},
   "source": [
    "Our estimate and standard error are therefore given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-est-se-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(pois_data)\n",
    "se_lambda = np.sqrt(EstVar) / np.sqrt(n)\n",
    "\n",
    "est_pois = pd.Series({'Estimate': lambda_hat, 'Std. Error': se_lambda})\n",
    "print(est_pois.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "estimating-sd-cell",
   "metadata": {},
   "source": [
    "So given our estimator, we're estimating the standard deviation of our estimator as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "se-value-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_lambda = np.sqrt(EstVar / 400)\n",
    "print(f\"Estimated SE: {se_lambda:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "true-se-cell",
   "metadata": {},
   "source": [
    "when the reality is that in this case we know it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "true-se-value-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True SE: {np.sqrt(2 / 400):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illustrating-cell",
   "metadata": {},
   "source": [
    "## Illustrating this:\n",
    "---\n",
    "What does this mean about the potential for the noise in our estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-normal-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5, 0.5, 300)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Actual distribution for the error (gold)\n",
    "ax.plot(x, stats.norm.pdf(x, 0, np.sqrt(2 / 400)),\n",
    "        color=PITT_GOLD, linewidth=3, label='Actual distribution')\n",
    "# Estimated distribution for the error (blue)\n",
    "ax.plot(x, stats.norm.pdf(x, 0, se_lambda),\n",
    "        color=PITT_BLUE, linewidth=1, label='Estimated distribution')\n",
    "ax.set_xlabel(r'$\\hat{\\lambda} - \\lambda$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Actual vs Estimated Sampling Distribution')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hessian-cell",
   "metadata": {},
   "source": [
    "Similarly, we could have estimated this using the second derivative of the log-likelihood (the Hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hessian-calc-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: pois.hess.est <- function(kj) ( -kj / lambda.hat^2 )\n",
    "# Python: vectorized\n",
    "def pois_hess_est(kj):\n",
    "    return -kj / lambda_hat**2\n",
    "\n",
    "# R: pois.hess <- sapply(pois.data, pois.hess.est)\n",
    "pois_hess = pois_hess_est(pois_data)\n",
    "\n",
    "# Take the negative of the mean to get I, then invert\n",
    "EstVar_H = 1.0 / np.mean(-pois_hess)\n",
    "se_hessian = np.sqrt(EstVar_H / 400)\n",
    "print(f\"SE from Hessian method: {se_hessian:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hessian-note-cell",
   "metadata": {},
   "source": [
    "Note that this is not identical to the score calculation! The identity only holds at the true value $\\lambda$ under the expectation... However, if we're close to the true value, the approximation is still good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-heading-cell",
   "metadata": {},
   "source": [
    "## Numerical Approximations\n",
    "In many situations it may be a bit too much for us to calculate the derivatives analytically, in which case, we could rely on our numerical approximations.\n",
    "\n",
    "Here I'll use: $$\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}=f^\\prime(x)+O(\\epsilon^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "num-deriv-func-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_log_likelihood_poisson(kj, lam, eps):\n",
    "    \"\"\"\n",
    "    Numerical derivative of Poisson log-likelihood at a single observation.\n",
    "    \n",
    "    R equivalent:\n",
    "        d.log.likelihood.poisson <- function(kj, lambda, eps) {\n",
    "            (dpois(kj, lambda=lambda+eps, log=TRUE) - \n",
    "             dpois(kj, lambda=lambda-eps, log=TRUE)) / (2*eps)\n",
    "        }\n",
    "    \n",
    "    Python: stats.poisson.logpmf(kj, mu) is equivalent to dpois(kj, lambda, log=TRUE)\n",
    "    \"\"\"\n",
    "    return (stats.poisson.logpmf(kj, lam + eps) - \n",
    "            stats.poisson.logpmf(kj, lam - eps)) / (2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "num-deriv-compare-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our analytical scores squared and averaged\n",
    "print(f\"Analytical: {np.mean(scores**2):.7f}\")\n",
    "\n",
    "# Same thing, but with numerical derivatives\n",
    "# R: sapply(pois.data, d.log.likelihood.poisson, lambda=lambda.hat, eps=1e-4)\n",
    "# Python: vectorized or list comprehension\n",
    "num_scores = np.array([d_log_likelihood_poisson(kj, lambda_hat, 1e-4) \n",
    "                       for kj in pois_data])\n",
    "print(f\"Numerical:  {np.mean(num_scores**2):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "num-se-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_lambda_numerical = np.sqrt(1.0 / np.mean(num_scores**2) / 400)\n",
    "print(f\"SE (numerical): {se_lambda_numerical:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-vs-hessian-scaling-cell",
   "metadata": {},
   "source": [
    "Note that here we're taking a numerical derivative for every entry. In this case, the score/versus Hessian would be about the same in terms of complication to compute\n",
    "\n",
    "In general though if we have $k$-parameters to estimate, the variance covariance matrix we're looking for will be a $k\\times k$ matrix. When we're using the score method this means computing $n$ different $k$-dimensional numerical derivatives to get an approximation for $\\mathbb{E}\\left[\\mathbf{s}(\\boldsymbol{\\theta})\\mathbf{s}(\\boldsymbol{\\theta})^T\\right]$ where we average across the $n$ points.\n",
    "\n",
    "In contrast, if we wanted to calculate things with the second derivatives, each Hessian matrix is a $k\\times k $ matrix, so we'd have to do $n\\cdot k^2$ numerical derivatives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "two-groups-heading-cell",
   "metadata": {},
   "source": [
    "For example, suppose we have two observable groups and we're trying to estimate the two means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "two-groups-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: df.two.groups <- data.frame(Group = ifelse(runif(400) > 1/2, 1, 2))\n",
    "np.random.seed(123)\n",
    "n_obs = 400\n",
    "groups = np.where(np.random.uniform(size=n_obs) > 0.5, 1, 2)\n",
    "\n",
    "# R: sapply(df.two.groups$Group, function(x) if(x==1) rpois(1,1.5) else rpois(1,2.5))\n",
    "k_vals = np.where(groups == 1, \n",
    "                  np.random.poisson(1.5, n_obs), \n",
    "                  np.random.poisson(2.5, n_obs))\n",
    "\n",
    "df_two_groups = pd.DataFrame({'Group': groups, 'k': k_vals})\n",
    "print(df_two_groups.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "two-groups-means-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_hat_1 = df_two_groups.loc[df_two_groups['Group'] == 1, 'k'].mean()\n",
    "lambda_hat_2 = df_two_groups.loc[df_two_groups['Group'] == 2, 'k'].mean()\n",
    "print(f\"lambda1 = {lambda_hat_1:.6f}\")\n",
    "print(f\"lambda2 = {lambda_hat_2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "two-groups-score-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "nn = len(df_two_groups)\n",
    "\n",
    "# Initialize score matrix (N x 2)\n",
    "score_matrix = np.zeros((nn, 2))\n",
    "\n",
    "for i in range(nn):\n",
    "    grp = df_two_groups.iloc[i]['Group']\n",
    "    kj = df_two_groups.iloc[i]['k']\n",
    "    if grp == 1:  # Group 1: d/d lambda1 is non-zero\n",
    "        score_matrix[i, 0] = (stats.poisson.logpmf(kj, lambda_hat_1 + eps) - \n",
    "                              stats.poisson.logpmf(kj, lambda_hat_1 - eps)) / (2 * eps)\n",
    "    else:  # Group 2: d/d lambda2 is non-zero\n",
    "        score_matrix[i, 1] = (stats.poisson.logpmf(kj, lambda_hat_2 + eps) - \n",
    "                              stats.poisson.logpmf(kj, lambda_hat_2 - eps)) / (2 * eps)\n",
    "\n",
    "# Fisher Information: (S^T S) / n, then invert for Sigma\n",
    "# R: Sigma <- solve(t(scoreVector) %*% scoreVector / nn)\n",
    "Sigma = np.linalg.inv(score_matrix.T @ score_matrix / nn)\n",
    "print(\"Variance-covariance matrix (Sigma):\")\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-cell",
   "metadata": {},
   "source": [
    "As should be obvious, our estimators for the two groups are entirely independent as they're separate samples,  where:\n",
    "$$\\sqrt{N}\\left(\\begin{array}{c} \\hat{\\lambda_1}-\\lambda_1 \\\\ \\hat{\\lambda_2}-\\lambda_2 \\end{array}\\right)\\rightarrow^D \\mathcal{N}\\left\\{  \n",
    "0,\\hat{\\boldsymbol{\\Sigma}}\n",
    "\\right\\}$$\n",
    "\n",
    "Where the lack of any correlation in the estimates means that we get:\n",
    "$$\\hat{\\boldsymbol{\\Sigma}}=\\left[\\begin{array}{cc}\n",
    "\\hat{\\sigma}^2_{\\lambda_1} & 0 \\\\\n",
    "0 & \\hat{\\sigma}^2_{\\lambda_2}\n",
    "\\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "se-groups-heading-cell",
   "metadata": {},
   "source": [
    "So we can calculate the standard errors as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "se-groups-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: data.frame(group=c(1,2), mean=c(lambda.hat.1,lambda.hat.2), se=sqrt(diag(Sigma)/nn))\n",
    "# diag() picks out the diagonal elements of the Sigma matrix!\n",
    "se_df = pd.DataFrame({\n",
    "    'group': [1, 2],\n",
    "    'mean': [lambda_hat_1, lambda_hat_2],\n",
    "    'se': np.sqrt(np.diag(Sigma) / nn)\n",
    "})\n",
    "print(se_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alt-param-cell",
   "metadata": {},
   "source": [
    "Note that another way of setting up this model would have been to say group 1's average was $\\lambda_1$ and to define group 2's average as $\\lambda_1+\\lambda_2$. \n",
    "\n",
    "It's pretty easy to show that our estimates would then be given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-param-values-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lambda_hat_1 = lambda_hat_1\n",
    "alt_lambda_hat_2 = lambda_hat_2 - lambda_hat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alt-note-cell",
   "metadata": {},
   "source": [
    "So now $\\lambda_1$ appears in all the rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alt-score-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "a_score_matrix = np.zeros((nn, 2))\n",
    "\n",
    "for i in range(nn):\n",
    "    grp = df_two_groups.iloc[i]['Group']\n",
    "    kj = df_two_groups.iloc[i]['k']\n",
    "    if grp == 1:  # Group 1: derivative only over lambda1\n",
    "        a_score_matrix[i, 0] = (stats.poisson.logpmf(kj, alt_lambda_hat_1 + eps) - \n",
    "                                stats.poisson.logpmf(kj, alt_lambda_hat_1 - eps)) / (2 * eps)\n",
    "    else:  # Group 2: derivative over both lambda1 *and* lambda2\n",
    "        a_score_matrix[i, 0] = (stats.poisson.logpmf(kj, alt_lambda_hat_1 + alt_lambda_hat_2 + eps) - \n",
    "                                stats.poisson.logpmf(kj, alt_lambda_hat_1 - eps + alt_lambda_hat_2)) / (2 * eps)\n",
    "        a_score_matrix[i, 1] = (stats.poisson.logpmf(kj, alt_lambda_hat_1 + alt_lambda_hat_2 + eps) - \n",
    "                                stats.poisson.logpmf(kj, alt_lambda_hat_1 + alt_lambda_hat_2 - eps)) / (2 * eps)\n",
    "\n",
    "# R: a.Sigma <- solve(t(a.scoreVector) %*% a.scoreVector / nn)\n",
    "a_Sigma = np.linalg.inv(a_score_matrix.T @ a_score_matrix / nn)\n",
    "print(\"Alternative parameterization Sigma:\")\n",
    "print(a_Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sum-variance-cell",
   "metadata": {},
   "source": [
    "However, if we take the *sum* of the two coefficients, this should have the same variance as the second group from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delta-method-demo-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a_Sigma:\")\n",
    "print(a_Sigma)\n",
    "print()\n",
    "\n",
    "# R: t(c(1,1)) %*% a.Sigma %*% c(1,1)\n",
    "c = np.array([1, 1])\n",
    "var_sum = c @ a_Sigma @ c\n",
    "print(f\"Var(lambda1 + lambda2) = {var_sum:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delta-method-algebra-cell",
   "metadata": {},
   "source": [
    "Note that the matrix algebra above is:\n",
    "$$\\left(\\begin{array}{cc}\n",
    "1 & 1\n",
    "\\end{array} \\right)  \\left[\\begin{array}{cc}\n",
    "\\hat{\\sigma}^2_{\\lambda_1} & 0 \\\\\n",
    "0 & \\hat{\\sigma}^2_{\\lambda_2}\n",
    "\\end{array} \\right] \\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array} \\right) = \\hat{\\sigma}^2_{\\lambda_1} + \\hat{\\sigma}^2_{\\lambda_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soccer-heading-cell",
   "metadata": {},
   "source": [
    "## Soccer parameter estimates\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soccer-intro-cell",
   "metadata": {},
   "source": [
    "Going back to our soccer estimations, let's calculate the standard errors the hard way, before I show you the *easy* way. \n",
    "\n",
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soccer-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: load(\"soccer/soccerData.rda\")\n",
    "# Python: use pyreadr to load .rda files\n",
    "try:\n",
    "    import pyreadr\n",
    "    rda_data = pyreadr.read_r('soccer/soccerData.rda')\n",
    "    estimData = rda_data['estimData']\n",
    "except ImportError:\n",
    "    print(\"pyreadr not installed. Run: pip install pyreadr\")\n",
    "    print(\"Attempting to recreate data structure...\")\n",
    "    estimData = None\n",
    "\n",
    "if estimData is not None:\n",
    "    print(estimData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "estimation-heading-cell",
   "metadata": {},
   "source": [
    "Estimation Equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likelihood-func-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_function_list(theta, data):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood contribution for each match outcome.\n",
    "    Returns a vector of log-probabilities (two per match: home goals and away goals).\n",
    "    \n",
    "    Parameters:\n",
    "    theta: array of length 40\n",
    "        theta[0:38] = alpha[0], delta[0], alpha[1], delta[1], ..., alpha[18], delta[18]\n",
    "        theta[38] = mu (mean parameter)\n",
    "        theta[39] = eta (home advantage)\n",
    "    \"\"\"\n",
    "    alpha = np.zeros(20)\n",
    "    delta = np.zeros(20)\n",
    "    for i in range(19):\n",
    "        alpha[i] = theta[2 * i]\n",
    "        delta[i] = theta[2 * i + 1]\n",
    "    alpha[19] = -np.sum(alpha[:19])  # sum-to-zero constraint\n",
    "    delta[19] = -np.sum(delta[:19])  # sum-to-zero constraint\n",
    "    \n",
    "    n_matches = len(data)\n",
    "    prob = np.zeros(2 * n_matches)\n",
    "    \n",
    "    for row in range(n_matches):\n",
    "        H = int(data.iloc[row]['HomeNo'])  # Home team number\n",
    "        gH = int(data.iloc[row]['FTHG'])   # Home goals\n",
    "        A = int(data.iloc[row]['AwayNo'])  # Away team number\n",
    "        gA = int(data.iloc[row]['FTAG'])   # Away goals\n",
    "        \n",
    "        # log(lambda) for home team: mu + eta + alpha_H - delta_A\n",
    "        lambdaH = theta[38] + theta[39] + alpha[H] - delta[A]\n",
    "        # log(lambda) for away team: mu + alpha_A - delta_H\n",
    "        lambdaA = theta[38] + alpha[A] - delta[H]\n",
    "        \n",
    "        # Poisson log-likelihood: k*log(lambda) - lambda - log(k!)\n",
    "        from math import factorial, log\n",
    "        prob[2 * row] = gH * lambdaH - np.exp(lambdaH) - log(factorial(gH))\n",
    "        prob[2 * row + 1] = gA * lambdaA - np.exp(lambdaA) - log(factorial(gA))\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-estimates-cell",
   "metadata": {},
   "source": [
    "Rather than re-estimate, I'm just going to load in the saved estimates from the last class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-soccer-est-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: load(\"soccer/SoccerEst.rda\")\n",
    "try:\n",
    "    rda_est = pyreadr.read_r('soccer/SoccerEst.rda')\n",
    "    outModel = rda_est['outModel']\n",
    "    theta_out = rda_est['theta.out'].values.flatten()\n",
    "    print(outModel.head())\n",
    "    print(f\"\\ntheta_out (first 10): {theta_out[:10]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading estimates: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "score-matrix-heading-cell",
   "metadata": {},
   "source": [
    "So, we need to calculate the score vector which will be an $N\\times 40$ matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-matrix-calc-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec = likelihood_function_list(theta_out, estimData)  # Log-prob for each score\n",
    "eps = 1e-6\n",
    "n_params = len(theta_out)\n",
    "n_obs_soccer = len(pVec)\n",
    "\n",
    "# Initialize the score matrix\n",
    "score_mat = np.zeros((n_obs_soccer, n_params))\n",
    "\n",
    "for i in range(n_params):\n",
    "    # Create epsilon vector: zeros everywhere except position i\n",
    "    eps_vec = np.zeros(n_params)\n",
    "    eps_vec[i] = eps\n",
    "    # Numerical derivative: [f(theta+eps_i) - f(theta-eps_i)] / (2*eps)\n",
    "    score_mat[:, i] = (likelihood_function_list(theta_out + eps_vec, estimData) - \n",
    "                       likelihood_function_list(theta_out - eps_vec, estimData)) / (2 * eps)\n",
    "\n",
    "# R: aVCM <- solve((t(score.matrix) %*% score.matrix) / length(pVec)) / length(pVec)\n",
    "# Fisher information via score outer product, then invert and scale\n",
    "aVCM = np.linalg.inv(score_mat.T @ score_mat / n_obs_soccer) / n_obs_soccer\n",
    "\n",
    "# Standard errors: sqrt of diagonal elements\n",
    "seVector = np.sqrt(np.diag(aVCM))\n",
    "print(f\"Computed {n_params} standard errors\")\n",
    "print(f\"SE for mu (theta[38]): {seVector[38]:.4f}\")\n",
    "print(f\"SE for eta (theta[39]): {seVector[39]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "team-names-heading-cell",
   "metadata": {},
   "source": [
    "Get the team names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "team-names-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamNames = outModel.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-heading-cell",
   "metadata": {},
   "source": [
    "Fancy it up to make it easier to see what it's saying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-table-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaOut = np.zeros(20)\n",
    "deltaOut = np.zeros(20)\n",
    "alphaOutSE = np.zeros(20)\n",
    "deltaOutSE = np.zeros(20)\n",
    "\n",
    "# Vectors for computing constrained team's SE via delta method\n",
    "sumAlpha = np.zeros(40)\n",
    "sumDelta = np.zeros(40)\n",
    "\n",
    "for i in range(19):\n",
    "    sumAlpha[2 * i] = -1\n",
    "    sumDelta[2 * i + 1] = -1\n",
    "    alphaOut[i] = theta_out[2 * i]\n",
    "    alphaOutSE[i] = seVector[2 * i]\n",
    "    deltaOut[i] = theta_out[2 * i + 1]\n",
    "    deltaOutSE[i] = seVector[2 * i + 1]\n",
    "\n",
    "# The 20th team is constrained: sum-to-zero\n",
    "alphaOut[19] = -np.sum(alphaOut[:19])\n",
    "alphaOutSE[19] = np.sqrt(sumAlpha @ aVCM @ sumAlpha)\n",
    "deltaOut[19] = -np.sum(deltaOut[:19])\n",
    "deltaOutSE[19] = np.sqrt(sumDelta @ aVCM @ sumDelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-params-heading-cell",
   "metadata": {},
   "source": [
    "And look at the overall mean/home effect variables too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-params-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = pd.Series({\n",
    "    'mu': theta_out[38],\n",
    "    'se(mu)': seVector[38],\n",
    "    'eta': theta_out[39],\n",
    "    'se(eta)': seVector[39]\n",
    "})\n",
    "print(general_params.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-table-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_coded = pd.DataFrame({\n",
    "    'alpha': np.round(alphaOut, 3),\n",
    "    'alpha_se': np.round(alphaOutSE, 3),\n",
    "    'delta': np.round(deltaOut, 3),\n",
    "    'delta_se': np.round(deltaOutSE, 3)\n",
    "}, index=teamNames)\n",
    "\n",
    "print(mod_coded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glm-heading-cell",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "---\n",
    "Fortunately, and we will talk about this more as we introduce other models, if we can outline the effects on the mean as the sum of a number of linear parts, we can solve this type of model in Python using what is called a *generalized linear model*.\n",
    "\n",
    "While the linearity of the predictor is given, the form of how this predictor is used for understanding the outcome $y$ can be highly non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reshape-heading-cell",
   "metadata": {},
   "source": [
    "Here, we reshape the scoreline data to make the predictor $\\lambda$ a simple linear combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reshape-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: gather(estimData, key=\"variable\", value=\"value\", \"FTHG\", \"FTAG\")\n",
    "# Python: pd.melt()\n",
    "df = pd.melt(estimData, \n",
    "             id_vars=['HomeTeam', 'HomeNo', 'AwayTeam', 'AwayNo'],\n",
    "             value_vars=['FTHG', 'FTAG'],\n",
    "             var_name='variable', \n",
    "             value_name='value')\n",
    "\n",
    "# Create AttackTeam, DefTeam, Home columns\n",
    "df['AttackTeam'] = np.where(df['variable'] == 'FTHG', df['HomeTeam'], df['AwayTeam'])\n",
    "df['DefTeam'] = np.where(df['variable'] == 'FTHG', df['AwayTeam'], df['HomeTeam'])\n",
    "df['Home'] = (df['variable'] == 'FTHG')\n",
    "\n",
    "# Make factor variables with Man City as reference level\n",
    "# R: relevel(as.factor(...), \"Man City\")\n",
    "# Python: Categorical with specified categories (reference handled by formula)\n",
    "df['Off'] = pd.Categorical(df['AttackTeam'])\n",
    "df['Def'] = pd.Categorical(df['DefTeam'])\n",
    "df['value'] = df['value'].astype(int)\n",
    "\n",
    "print(df[['value', 'Home', 'Off', 'Def']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glm-model-heading-cell",
   "metadata": {},
   "source": [
    "We will now attempt to estimate this as a model where the log-mean of the Poisson draw for each observation (so  the $\\log(\\lambda)$) is given by:\n",
    "* a constant/intercept\n",
    "* an indicator for whether the team was at home\n",
    "* a factor variable for the attacking team\n",
    "* a factor variable for the defending team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glm-head-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glm-fit-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: glm(value ~ as.factor(Home) + Off + Def, family=poisson, data=df)\n",
    "# Python: statsmodels GLM with Poisson family\n",
    "# We use C() for categorical treatment, with Man City as reference\n",
    "glm_mod = smf.glm(\n",
    "    'value ~ C(Home) + C(Off, Treatment(reference=\"Man City\")) + C(Def, Treatment(reference=\"Man City\"))',\n",
    "    data=df,\n",
    "    family=sm.families.Poisson()\n",
    ").fit()\n",
    "\n",
    "print(glm_mod.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glm-note-cell",
   "metadata": {},
   "source": [
    "Note that the team values are different, though the normalization here is also different. However, the *home*  parameter ($\\eta$ in our previous notation) is obviously comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "home-effect-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: exp(0.15665)\n",
    "# Find the Home coefficient\n",
    "home_coef = glm_mod.params['C(Home)[T.True]']\n",
    "print(f\"Home coefficient: {home_coef:.5f}\")\n",
    "print(f\"exp(Home coef) = {np.exp(home_coef):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-teams-heading-cell",
   "metadata": {},
   "source": [
    "Let's check the parameters we're getting for two teams:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wolves-heading-cell",
   "metadata": {},
   "source": [
    "The model output here indicates the **log** of Wolves's expected goals at home against Man City is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wolves-glm-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: coef(glm.mod)[\"(Intercept)\"] + coef(glm.mod)[\"as.factor(Home)TRUE\"] + coef(glm.mod)[\"OffWolves\"]\n",
    "intercept = glm_mod.params['Intercept']\n",
    "home_param = glm_mod.params['C(Home)[T.True]']\n",
    "wolves_off = glm_mod.params['C(Off, Treatment(reference=\"Man City\"))[T.Wolves]']\n",
    "\n",
    "log_wolves_xg = intercept + home_param + wolves_off\n",
    "print(f\"log(Wolves XG at home vs Man City) = {log_wolves_xg:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp-wolves-heading-cell",
   "metadata": {},
   "source": [
    "So we can take the exponential of this to get the true mean for the expected goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp-wolves-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wolves XG at home vs Man City = {np.exp(log_wolves_xg):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hand-coded-heading-cell",
   "metadata": {},
   "source": [
    "Our model estimates calculated by the hand coded model indicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hand-coded-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = general_params['mu']\n",
    "eta = general_params['eta']\n",
    "wolves_alpha = mod_coded.loc['Wolves', 'alpha']\n",
    "mancity_delta = mod_coded.loc['Man City', 'delta']\n",
    "\n",
    "hand_coded_log = mu + eta + wolves_alpha - mancity_delta\n",
    "print(f\"Hand-coded log(XG) = {hand_coded_log:.7f}\")\n",
    "print(f\"Hand-coded XG      = {np.exp(hand_coded_log):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-diff-cell",
   "metadata": {},
   "source": [
    "**Note** there are some small differences, indicating the extent to which features of the optimization can be important, especially when there are many parameters in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "se-interpretation-cell",
   "metadata": {},
   "source": [
    "Note that all of our standard errors are constructed over the parameters, which are not directly interpretable as means. \n",
    "* The parameters are inputs into the large model\n",
    "* Reporting either the parameter or it's standard error only has meaning within the context of the model\n",
    "* We will have to come up with a way of expressing the effects in a more meaningful way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vcov-glm-cell",
   "metadata": {},
   "source": [
    "## Comparing Variance-Covariance Matrices\n",
    "---\n",
    "We can also compare our hand-coded variance-covariance matrix with the one provided by the GLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vcov-compare-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: vcov(glm.mod) gives the variance-covariance matrix\n",
    "# Python: model.cov_params()\n",
    "glm_vcov = glm_mod.cov_params()\n",
    "print(\"GLM Variance-Covariance Matrix (first 5x5 block):\")\n",
    "print(glm_vcov.iloc[:5, :5].round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delta-method-heading-cell",
   "metadata": {},
   "source": [
    "## Delta Method for Transformations\n",
    "---\n",
    "Since the parameters enter through a log-link, the expected goals for a matchup involve $\\exp(\\cdot)$ of a linear combination. To get standard errors for these transformed quantities, we use the **delta method**:\n",
    "\n",
    "If $g(\\boldsymbol{\\theta})$ is a function of parameters with variance-covariance $\\boldsymbol{\\Sigma}$, then:\n",
    "$$\\text{Var}\\left(g(\\hat{\\boldsymbol{\\theta}})\\right) \\approx \\nabla g(\\hat{\\boldsymbol{\\theta}})^T \\boldsymbol{\\Sigma} \\nabla g(\\hat{\\boldsymbol{\\theta}})$$\n",
    "\n",
    "We can use the `delta_method` function from our `utils` module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delta-method-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SE for Wolves' expected goals at home vs Man City\n",
    "# Using our hand-coded VCM\n",
    "def wolves_xg_home(theta):\n",
    "    \"\"\"Expected goals for Wolves at home vs Man City.\"\"\"\n",
    "    alpha = np.zeros(20)\n",
    "    delta = np.zeros(20)\n",
    "    for i in range(19):\n",
    "        alpha[i] = theta[2*i]\n",
    "        delta[i] = theta[2*i + 1]\n",
    "    alpha[19] = -np.sum(alpha[:19])\n",
    "    delta[19] = -np.sum(delta[:19])\n",
    "    # Wolves is team index we need to find\n",
    "    wolves_idx = teamNames.index('Wolves')\n",
    "    mancity_idx = teamNames.index('Man City')\n",
    "    return np.exp(theta[38] + theta[39] + alpha[wolves_idx] - delta[mancity_idx])\n",
    "\n",
    "xg_val, xg_se = utils.delta_method(wolves_xg_home, theta_out, aVCM)\n",
    "print(f\"Wolves XG at home vs Man City: {xg_val:.4f} (SE: {xg_se:.4f})\")\n",
    "print(f\"95% CI: [{xg_val - 1.96*xg_se:.4f}, {xg_val + 1.96*xg_se:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell",
   "metadata": {},
   "source": [
    "## Summary: R to Python Mapping\n",
    "---\n",
    "\n",
    "| Concept | R | Python |\n",
    "|---------|---|--------|\n",
    "| Poisson density (log) | `dpois(kj, lambda, log=TRUE)` | `stats.poisson.logpmf(kj, mu)` |\n",
    "| Numerical gradient | Manual loop | `utils.numerical_gradient()` or `scipy.optimize.approx_fprime()` |\n",
    "| Score outer product | `t(S) %*% S / n` | `S.T @ S / n` |\n",
    "| Matrix inverse | `solve(A)` | `np.linalg.inv(A)` |\n",
    "| Diagonal elements | `diag(A)` | `np.diag(A)` |\n",
    "| Reshape wide to long | `gather()` | `pd.melt()` |\n",
    "| GLM Poisson | `glm(..., family=poisson)` | `smf.glm(..., family=sm.families.Poisson())` |\n",
    "| Variance-covariance | `vcov(model)` | `model.cov_params()` |\n",
    "| Delta method | Manual | `utils.delta_method()` |\n",
    "| Load .rda files | `load()` | `pyreadr.read_r()` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}