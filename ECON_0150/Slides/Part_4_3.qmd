---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 4.3 | Regression Assumptions, Multiple Sample Tests*

---

## General Linear Model
<p class="subheader">... a flexible approach to run many statistical tests.</p>

**The Linear Model**: $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$

. . .

- $\beta_0$ is the intercept (value of $\bar{y}$ when x = 0)
- $\beta_1$ is the slope (change in y per unit change in x)
- $\varepsilon_i$ is the error term (random noise around the model)

. . .

**OLS Estimation**: Minimizes $\sum_{i=1}^n \varepsilon_i^2$

. . .

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, ax2 = plt.subplots(1, 1, figsize=(11, 3))

# Plot 2: Linear regression model
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept + slope * x_line
ax2.plot(x_line, y_line, 'r-', linewidth=2)

# Add vertical error lines for regression model
for i in range(0, n, 1):  # Show errors for every 5th point
    y_pred = intercept + slope * minutes_after_open[i]
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

# Use the same styling as your other plots
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

---

## One-Sample T-Test
<p class="subheader">A one-sample t-test is a horizontal line model.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seed for reproducibility
np.random.seed(42)

# Generate sample data
n = 30
data = [1.3, 2.4, 2.2, 1.3, 3.0, 2.3, 0.8, 2.7, 2.0, 2.9, 1.7, 1.9, 2.2,
       2.1, 2.4, 1.7, 1.7, 1.9, 1.7, 2.5, 2.4, 2.2, 1.9, 1.9, 2.7, 1.8,
       1.8, 2.0, 1.9, 2.1]

# Create the plot
plt.figure(figsize=(11, 3))

# Plot the data points
plt.scatter(range(1, n+1), data, alpha=0.7, label='Data Points')

# Plot the horizontal line at the mean
mean_data = np.mean(data)
plt.axhline(mean_data, color='r', linestyle='-', linewidth=2, label=f'Mean ($\\beta_0$ = {mean_data:.2f})')

# Add vertical lines for errors
for i in range(n):
    plt.plot([i+1, i+1], [data[i], mean_data], 'g--', alpha=0.4)

plt.ylabel('Temperature Difference (°C)', fontsize=16)
plt.grid(False)
plt.legend()
plt.xticks([])
sns.despine(bottom=True, trim=True)

plt.tight_layout()
```

$$Temperature = \beta_0 + \varepsilon$$

. . .

*> the intercept $\beta_0$ is the estimated mean temperature*

. . .

*> the p-value is the probability of seeing $\beta_0$ if the null is true*

---

## Relationships Between Variables
<p class="subheader">A test of relationships is a slope model.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, ax2 = plt.subplots(1, 1, figsize=(11, 3))

# Plot 2: Linear regression model
ax2.scatter(minutes_after_open, wait_times, alpha=0.7)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept + slope * x_line
ax2.plot(x_line, y_line, 'r-', linewidth=2)

# Add vertical error lines for regression model
for i in range(0, n, 1):  # Show errors for every 5th point
    y_pred = intercept + slope * minutes_after_open[i]
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.5)

ax2.set_xlabel('Minutes After Opening', fontsize=14)
ax2.set_ylabel('Wait Time', fontsize=14)
ax2.legend()

# Use the same styling as your other plots
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

$$\text{WaitTime} = \beta_0 + \beta_1 \text{MinutesAfterOpening} + \epsilon$$

. . .

*> the intercept parameter $\beta_0$ is the estimated temperature at 0 on the horizontal*

. . .

*> the slope parameter $\beta_1$ is the estimated change in y for a 1 unit change in x*

. . .

*> the p-value is the probability of seeing parameter ($\beta_0$ or $\beta_1$) if the null is true*

---

## New Setting: Two Samples
<p class="subheader">Is temperature lower with more green space?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Set seed for reproducibility
np.random.seed(42)

# Create 50 data points for each group
n = 50
low_green = 22 + np.random.normal(0, 1.5, n)  # Higher temperatures
high_green = 19 + np.random.normal(0, 1.5, n)  # Lower temperatures
jitter = np.random.uniform(-0.1,0.1,2*n)

# Combine into a dataframe
data = pd.DataFrame({
    'temperature': np.concatenate([low_green, high_green]),
    'high_green': np.concatenate([np.zeros(n), np.ones(n)])
})

# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='high_green', y='temperature', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['high_green']+jitter, data['temperature'], alpha=0.4, color='darkblue')

plt.xticks([0, 1], ['Low Green Space\n(0)', 'High Green Space\n(1)'])
plt.ylabel('Temperature (°C)', fontsize=14)

plt.xlim(-0.5,1.5)
sns.despine(trim=True)
plt.tight_layout()
```

. . .

$$Temperature = \beta_0 + \beta_1 \cdot HighGreen + \varepsilon$$

. . .

*> how would we interpret $\beta_0$ here?*

. . .

*> the average temperature at $x=0$, which is Low Green Space locations*

---

## New Setting: Two Samples
<p class="subheader">Is temperature lower with more green space?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Set seed for reproducibility
np.random.seed(42)

# Create 50 data points for each group
n = 50
low_green = 22 + np.random.normal(0, 1.5, n)  # Higher temperatures
high_green = 19 + np.random.normal(0, 1.5, n)  # Lower temperatures
jitter = np.random.uniform(-0.1,0.1,2*n)

# Combine into a dataframe
data = pd.DataFrame({
    'temperature': np.concatenate([low_green, high_green]),
    'high_green': np.concatenate([np.zeros(n), np.ones(n)])
})

# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='high_green', y='temperature', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['high_green']+jitter, data['temperature'], alpha=0.4, color='darkblue')

plt.xticks([0, 1], ['Low Green Space\n(0)', 'High Green Space\n(1)'])
plt.ylabel('Temperature (°C)', fontsize=14)

plt.xlim(-0.5,1.5)
sns.despine(trim=True)
plt.tight_layout()
```

$$Temperature = \beta_0 + \beta_1 \cdot HighGreen + \varepsilon$$

. . .

*> how would we interpret $\beta_1$ here?*

. . .

*> one unit increase in $x$, which puts us in High Green Space*

---

## New Setting: Two Samples
<p class="subheader">Is temperature lower with more green space?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Set seed for reproducibility
np.random.seed(42)

# Create 50 data points for each group
n = 50
low_green = 22 + np.random.normal(0, 1.5, n)  # Higher temperatures
high_green = 19 + np.random.normal(0, 1.5, n)  # Lower temperatures
jitter = np.random.uniform(-0.1,0.1,2*n)

# Combine into a dataframe
data = pd.DataFrame({
    'temperature': np.concatenate([low_green, high_green]),
    'high_green': np.concatenate([np.zeros(n), np.ones(n)])
})

# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='high_green', y='temperature', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['high_green']+jitter, data['temperature'], alpha=0.4, color='darkblue')

# Calculate and display the means
mean_low = np.mean(low_green)
mean_high = np.mean(high_green)
plt.plot([0, 1], [mean_low, mean_high], 'r-', linewidth=2)

plt.xticks([0, 1], ['Low Green Space\n(0)', 'High Green Space\n(1)'])
plt.ylabel('Temperature (°C)', fontsize=14)

plt.text(-0.35, mean_low, f'$\\beta_0$ = {mean_low:.2f}°C', fontsize=12, va='center')
plt.text(1.15, mean_high, f'$\\beta_0$ + $\\beta_1$ = {mean_high:.2f}°C', fontsize=12, va='center')
plt.text(0.5, (mean_low+mean_high)/2+0.5, f'$\\beta_1$ = {mean_high-mean_low:.2f}°C', fontsize=12, va='center', ha='center', rotation=-8)

plt.xlim(-0.5,1.5)
sns.despine(trim=True)
plt.tight_layout()
```

$$Temperature = \beta_0 + \beta_1 \cdot HighGreen + \varepsilon$$

. . .

*> $\beta_0$ is the mean temperature in low green space cities (22.03°C)*

. . .

*> $\beta_1$ is the temperature difference in high green space cities (-3.02°C)*

. . .

*> the t-test on $\beta_1$ tests if this difference is significant*

---

## Example: Neighborhood Income and Pollution
<p class="subheader">Do low-income neighborhoods face higher pollution levels?</p>

**Step 1: Summarize the data**

. . .

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
# Set seed for reproducibility
np.random.seed(42)
# Create 40 data points for each group
n = 40
high_income = 25 + np.random.normal(0, 5, n)  # Lower pollution
low_income = 40 + np.random.normal(0, 7, n)   # Higher pollution
jitter = np.random.uniform(-0.1,0.1,2*n)
# Combine into a dataframe
data = pd.DataFrame({
    'pollution': np.concatenate([high_income, low_income]),
    'low_income': np.concatenate([np.zeros(n), np.ones(n)])
})
# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='low_income', y='pollution', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['low_income']+jitter, data['pollution'], alpha=0.4, color='darkblue')

plt.xticks([0, 1], ['High Income\n(0)', 'Low Income\n(1)'])
plt.ylabel('Air Pollution Index', fontsize=14)

plt.xlim(-0.5,1.5)
sns.despine(trim=True)
plt.tight_layout()
```

. . .

**Step 2: Build a model**

. . .

$$Pollution = \beta_0 + \beta_1 \cdot LowIncome + \varepsilon$$

---

## Example: Neighborhood Income and Pollution
<p class="subheader">Do low-income neighborhoods face higher pollution levels?</p>

**Step 3: Estimate the model**

. . .

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
# Set seed for reproducibility
np.random.seed(42)
# Create 40 data points for each group
n = 40
high_income = 25 + np.random.normal(0, 5, n)  # Lower pollution
low_income = 40 + np.random.normal(0, 7, n)   # Higher pollution
jitter = np.random.uniform(-0.1,0.1,2*n)
# Combine into a dataframe
data = pd.DataFrame({
    'pollution': np.concatenate([high_income, low_income]),
    'low_income': np.concatenate([np.zeros(n), np.ones(n)])
})
# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='low_income', y='pollution', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['low_income']+jitter, data['pollution'], alpha=0.4, color='darkblue')
# Calculate and display the means
mean_high = np.mean(high_income)
mean_low = np.mean(low_income)
plt.plot([0, 1], [mean_high, mean_low], 'r-', linewidth=2)
plt.xticks([0, 1], ['High Income\n(0)', 'Low Income\n(1)'])
plt.ylabel('Air Pollution Index', fontsize=14)
plt.text(-0.35, mean_high, f'$\\beta_0$ = {mean_high:.1f}', fontsize=12, va='center')
plt.text(1.15, mean_low, f'$\\beta_0$ + $\\beta_1$ = {mean_low:.1f}', fontsize=12, va='center')
plt.text(0.5, (mean_high+mean_low)/2+5, f'$\\beta_1$ = {mean_low-mean_high:.1f}', fontsize=12, va='center')
plt.xlim(-0.5, 1.5)
sns.despine(trim=True)
plt.tight_layout()
```

. . .

- $\beta_0$ = Mean pollution in high-income areas (24.8)

- $\beta_1$ = Additional pollution in low-income areas (+15.0)

---

## Example: Neighborhood Income and Pollution
<p class="subheader">Do low-income neighborhoods face higher pollution levels?</p>

**Step 4: Interpret and communicate the findings**

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
# Set seed for reproducibility
np.random.seed(42)
# Create 40 data points for each group
n = 40
high_income = 25 + np.random.normal(0, 5, n)  # Lower pollution
low_income = 40 + np.random.normal(0, 7, n)   # Higher pollution
jitter = np.random.uniform(-0.1,0.1,2*n)
# Combine into a dataframe
data = pd.DataFrame({
    'pollution': np.concatenate([high_income, low_income]),
    'low_income': np.concatenate([np.zeros(n), np.ones(n)])
})
# Plot the data
plt.figure(figsize=(11, 4))
sns.boxplot(x='low_income', y='pollution', data=data, 
            color='white',  # Use white for the boxes
            width=0.2,
            zorder=-1)
plt.scatter(data['low_income']+jitter, data['pollution'], alpha=0.4, color='darkblue')
# Calculate and display the means
mean_high = np.mean(high_income)
mean_low = np.mean(low_income)
plt.plot([0, 1], [mean_high, mean_low], 'r-', linewidth=2)
plt.xticks([0, 1], ['High Income\n(0)', 'Low Income\n(1)'])
plt.ylabel('Air Pollution Index', fontsize=14)
plt.text(-0.35, mean_high, f'$\\beta_0$ = {mean_high:.1f}', fontsize=12, va='center')
plt.text(1.15, mean_low, f'$\\beta_0$ + $\\beta_1$ = {mean_low:.1f}', fontsize=12, va='center')
plt.text(0.5, (mean_high+mean_low)/2+5, f'$\\beta_1$ = {mean_low-mean_high:.1f}', fontsize=12, va='center')
plt.xlim(-0.5, 1.5)
sns.despine(trim=True)
plt.tight_layout()
```

. . .

*> A significant positive $\beta_1$ suggests environmental quality differences between neighborhoods*

---

## OLS Assumptions
<p class="subheader">Our test results are only valid when the model assumptions are valid.</p>

<br>

1. **Linearity**: The relationship between X and Y is linear
   
. . .

2. **Independence**: Observations are independent from each other

. . . 

3. **Homoskedasticity**: Equal error variance across all values of X

. . .

4. **Normality**: Errors are normally distributed

---

## Model Diagnostics: Why Check Assumptions?
<p class="subheader">Assumption violations affect our inferences</p>

<br>

**If assumptions are violated:**

- Coefficient estimates may be biased
- Standard errors may be wrong
- p-values may be misleading
- Predictions may be unreliable

---

## Checking for Linearity
<p class="subheader">The error term should be unrelated to the fitted value.</p>

*> which one of these figures shows linearity?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data
np.random.seed(42)
x = np.linspace(0, 10, 50)
y_linear = 2 + 0.5 * x + np.random.normal(0, 1, 50)
y_nonlinear = 2 + 0.5 * x + 0.2 * x**2 + np.random.normal(0, 1, 50)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))

# Linear model
from scipy import stats
slope1, intercept1, r1, p1, se1 = stats.linregress(x, y_linear)
y_pred1 = intercept1 + slope1 * x
residuals1 = y_linear - y_pred1

ax1.scatter(y_pred1, residuals1, alpha=0.7)
ax1.axhline(y=0, color='r', linestyle='-')
ax1.set_xlabel('Fitted Values ($\hat{y}$)')
ax1.set_ylabel('Residuals ($\hat{\\varepsilon}$)')

# Nonlinear model fitted with linear regression
slope2, intercept2, r2, p2, se2 = stats.linregress(x, y_nonlinear)
y_pred2 = intercept2 + slope2 * x
residuals2 = y_nonlinear - y_pred2

ax2.scatter(y_pred2, residuals2, alpha=0.7)
ax2.axhline(y=0, color='r', linestyle='-')
ax2.set_xlabel('Fitted Values ($\hat{y}$)')

sns.despine(trim=True)
plt.tight_layout()
```

. . .

*> the left one is what we want to see*

. . .

*> residual plots should show that the model is equally wrong everywhere*

---

## Checking for Homoskedasticity
<p class="subheader">Residuals should be spread out the same everywhere.</p>

*> which one of these figures shows homoskedasticity?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data
np.random.seed(42)
x = np.linspace(0, 10, 50)

# Homoskedastic errors
y_homo = 2 + 0.5 * x + np.random.normal(0, 1, 50)

# Heteroskedastic errors (variance increases with x)
y_hetero = 2 + 0.5 * x + np.random.normal(0, 0.2 + 0.3 * x, 50)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 3))

# Homoskedastic model
slope1, intercept1, r1, p1, se1 = stats.linregress(x, y_homo)
y_pred1 = intercept1 + slope1 * x
residuals1 = y_homo - y_pred1

ax1.scatter(y_pred1, residuals1, alpha=0.7)
ax1.axhline(y=0, color='r', linestyle='-')
ax1.set_xlabel('Fitted Values ($\hat{y}$)')
ax1.set_ylabel('Residuals ($\hat{\\varepsilon}$)')

# Heteroskedastic model
slope2, intercept2, r2, p2, se2 = stats.linregress(x, y_hetero)
y_pred2 = intercept2 + slope2 * x
residuals2 = y_hetero - y_pred2

ax2.scatter(y_pred2, residuals2, alpha=0.7)
ax2.axhline(y=0, color='r', linestyle='-')
ax2.set_xlabel('Fitted Values ($\hat{y}$)')

sns.despine(trim=True)
plt.tight_layout()
```

. . .

*> the left figure shows constant variability (homoskedasticity)*

. . .

*> the right one has increasing variability (heteroskedasticity)*

. . .

*> residual plots should show that the model is equally wrong everywhere*

---

## Checking for Normality
<p class="subheader">Residuals should be normally distributed</p>
```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# Sample data
np.random.seed(42)
n = 1000
# Normal errors
normal_residuals = np.random.normal(0, 1, n)
# Skewed errors
skewed_residuals = stats.skewnorm.rvs(5, size=n)
skewed_residuals = skewed_residuals - skewed_residuals.mean()  # Center at 0

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 3))

# Normal histogram with curve
sns.histplot(normal_residuals, kde=True, ax=ax1)
ax1.set_xlabel('Residuals ($\hat{\\varepsilon}$)')
ax1.set_yticks([])
ax1.set_ylabel('')

# Skewed histogram with curve
sns.histplot(skewed_residuals, kde=True, ax=ax2)
ax2.set_xlabel('Residuals ($\hat{\\varepsilon}$)')
ax2.set_yticks([])
ax2.set_ylabel('')

sns.despine(left=True, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
```

. . .

*> left shows a nice bell shape (roughly normally distributed)*

. . .

*> right shows a skewed distribution (not normally distributed)*

. . .

*> by the CLT we can still use regression without this if the sample is large*

---

## Extending to Multiple Regression
<p class="subheader">Adding control variables to isolate relationships</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

# Set seed for reproducibility
np.random.seed(42)

# Create sample data with 100 points
n = 80
income = np.random.uniform(20000, 150000, n)
density = np.random.uniform(1000, 25000, n)

# Generate pollution with both income and density effects
pollution = 70 - 0.0003 * income + 0.001 * density + np.random.normal(0, 5, n)

# Create 3D plot
fig = plt.figure(figsize=(11, 5))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
scatter = ax.scatter(income, density, pollution, c=pollution, cmap='coolwarm', alpha=0.8)

# Create grid for surface
income_grid, density_grid = np.meshgrid(np.linspace(20000, 150000, 20), 
                                         np.linspace(1000, 25000, 20))
pollution_grid = 70 - 0.0003 * income_grid + 0.001 * density_grid

# Plot surface
surface = ax.plot_surface(income_grid, density_grid, pollution_grid, 
                         alpha=0.3, cmap='coolwarm')

ax.set_xlabel('Median Income ($)')
ax.set_ylabel('Population Density')
ax.set_zlabel('Air Pollution Index')

# Improve perspective
ax.view_init(elev=25, azim=230)

plt.tight_layout()
```

. . .

$$Pollution = \beta_0 + \beta_1 \cdot Income + \beta_2 \cdot Density + \varepsilon$$


---

## Extending to Multiple Regression
<p class="subheader">Adding control variables to isolate relationships</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

# Set seed for reproducibility
np.random.seed(42)

# Create sample data with 100 points
n = 80
income = np.random.uniform(20000, 150000, n)
density = np.random.uniform(1000, 25000, n)

# Generate pollution with both income and density effects
pollution = 70 - 0.0003 * income + 0.001 * density + np.random.normal(0, 5, n)

# Create 3D plot
fig = plt.figure(figsize=(11, 4))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
scatter = ax.scatter(income, density, pollution, c=pollution, cmap='coolwarm', alpha=0.8)

# Create grid for surface
income_grid, density_grid = np.meshgrid(np.linspace(20000, 150000, 20), 
                                         np.linspace(1000, 25000, 20))
pollution_grid = 70 - 0.0003 * income_grid + 0.001 * density_grid

# Plot surface
surface = ax.plot_surface(income_grid, density_grid, pollution_grid, 
                         alpha=0.3, cmap='coolwarm')

ax.set_xlabel('Median Income ($)')
ax.set_ylabel('Population Density')
ax.set_zlabel('Air Pollution Index')

# Improve perspective
ax.view_init(elev=25, azim=230)

plt.tight_layout()
```

- $\beta_0$ = Baseline pollution level (70.0)
- $\beta_1$ = Effect of income on pollution, holding density constant (-0.0003)
- $\beta_2$ = Effect of density on pollution, holding income constant (+0.001)

---

## Key Takeaways
<p class="subheader">Regression provides a unified framework for statistical testing</p>

. . .

**One-Sample T-Test**: Continuous outcome variable ($y$) with only an intercept

$$y = \beta_0 + \varepsilon$$

. . .

**Relationships**: Continuous outcome variable ($y$) with a continuous predictor ($x$)

$$y = \beta_0 + \beta_1 x + \varepsilon$$

. . .

**Two-Sample T-Test**: Continuous outcome variable ($y$) with a dummy ($Group$)

$$y = \beta_0 + \beta_1 \cdot Group + \varepsilon$$

. . .

**Multiple Regression**: Adding control variables to isolate relationships

. . .

<br>

*> all use the same OLS framework and interpretation of coefficients and p-values*
