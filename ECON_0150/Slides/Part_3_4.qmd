---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 3.4 | Testing Hypotheses*

---

## Confidence Intervals Recap
<p class="subheader">We used the distribution of sample means to systematize the probability of "closeness" of $\bar{x}$ and $\mu$.</p>

:::: {.columns}
::: {.column width="50%"}
- The difference between $\bar{x}$ and $\mu$ follows a t distribution with SE = $\frac{s}{\sqrt{n}}$
- 95% of samples will have $\bar{x}$ no further than 1.96 standard errors from $\mu$
:::

::: {.column width="50%"}
```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns


# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

np.random.seed(123)
n_means = 1000
sample_size = 30
sample_means = [np.mean(np.random.normal(0, 2.5, sample_size)) 
                for _ in range(n_means)]

se = 2.5/np.sqrt(sample_size)

plt.figure(figsize=(5, 3))
plt.hist(sample_means, bins=25, density=True, alpha=0.5)
x = np.linspace(-2, 2, 1000)
y = stats.norm.pdf(x, 0, 2.5/np.sqrt(sample_size))
plt.plot(x, y, 'r-')
plt.axvline(x=0, color='blue', linestyle='--')
plt.axvline(x=-1.96*se, color='green', linestyle='--')
plt.axvline(x=1.96*se, color='green', linestyle='--')
plt.title('Distribution of $\\bar{x}-\mu$', fontsize=20)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)

plt.tight_layout()
plt.show()
```
:::
::::

. . .

<br><br>

*> in the wait time example, we asked "where is the true mean wait time?"*

. . .

*> but what if we want to test a specific claim about the mean?*

---

## Flipping The Question
<p class="subheader">What if we want to test a specific claim about the mean?</p>

<br>

. . .

*> "my boss claims the mean wait time is 10 minutes"*

. . .

*> is our data consistent with that specific claim?*

. . .

*> same math as last time, but a different question...*

. . .

*> instead of finding where some $\mu$ might be, we're testing a specific value of $\mu$*

---

## Example: Wait Times
<p class="subheader">If $\bar{x}=10.85$, is that consistent with $\mu_0=10$?</p>

. . .

*> let's simulate data where $\mu=10$ and see what sample means we'd get*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(42)
true_mean = 10
std_dev = 2.5
sample_size = 30
n_samples = 1000

# Generate samples and calculate their means
sample_means = [np.mean(np.random.normal(true_mean, std_dev, sample_size)) 
                for _ in range(n_samples)]

plt.figure(figsize=(10, 4))
plt.hist(sample_means, bins=30, density=True, alpha=0.6, color='#4C72B0')
x = np.linspace(8.5, 11.5, 1000)
y = stats.norm.pdf(x, true_mean, std_dev/np.sqrt(sample_size))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=true_mean, color='red', linestyle='--', linewidth=2)

# Add annotation for the observed value
observed_mean = 10.85
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

plt.title('Distribution of Sample Means if True Mean is 10', fontsize=16)
plt.xlabel('Sample Mean (minutes)', fontsize=14)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> how "surprising" would our observed $\bar{x}$ be if $\mu$ actually was 10?*

. . .

*> notice we've centered the distribution on our hypothesis: $\mu_0$*

---

## Example: Wait Times
<p class="subheader">The math to answer this question is identical to confidence intervals.</p>

:::: {.columns}
::: {.column width="60%"}
If sample standard deviation is $s = 2.5$:

$$SE = \frac{s}{\sqrt{n}}$$

$$SE = \frac{2.5}{\sqrt{30}}$$

$$SE = 0.456$$
:::

::: {.column width="40%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(5, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```
:::
::::

```python
s = 2.5
n = 30
se = s / np.sqrt(30)
```

---

## Example: Wait Times
<p class="subheader">The math to answer this question is identical to confidence intervals.</p>

:::: {.columns}
::: {.column width="60%"}
If sample standard deviation is $s = 2.5$:

$$SE = 0.456$$

If true mean is $\mu_0 = 10$:

$$\bar{x} \sim t_{29}(10, 0.456)$$

So the critical value for 95%:
$$t_{crit} = 2.045$$
:::

::: {.column width="40%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(5, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```
:::
::::

```python
stats.t.interval(0.95, df=30)
```

---

## Example: Wait Times
<p class="subheader">The math to answer this question is identical to confidence intervals.</p>

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85
df = 29  # degrees of freedom = n-1

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.t.pdf(x, df, loc=mu_0, scale=std_err)

plt.figure(figsize=(11, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
t_critical = stats.t.ppf(0.975, df)  # t-value for 95% CI
ci_lower = mu_0 - t_critical * std_err
ci_upper = mu_0 + t_critical * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

plt.yticks([])
plt.title('t-Distribution if μ=10 (df=29)')

sns.despine(left=True, bottom=False, right=True, top=True, offset=-7, trim=True)
plt.tight_layout()
plt.show()
```

A 95% confidence interval around $\mu_0$ would be: $[9.07, 10.93]$

. . .

*> our observed mean ($\bar{x} = 10.85$) is within this interval — not surprising if μ=10*

. . .

*> but if we observed $\bar{x} = 11.5$, that would be outside the interval — surprising!*

---

## The Null Hypothesis
<p class="subheader">We formalize this approach by setting up a "null hypothesis"</p>

<br>

. . .

**Null Hypothesis** ($H_0$): *The specific value or claim we're testing*

. . .

- $H_0: \mu = 10$ (wait time is 10 minutes)

. . .

**Alternative Hypothesis** ($H_1$ or $H_a$): *What we accept if we reject the null*

. . .

- $H_1: \mu \neq 10$ (wait time is not 10 minutes)

. . .

**Testing Approach**: 

- Calculate how "surprising" our data would be if $H_0$ were true
- If sufficiently surprising, we reject $H_0$

---

## Quantifying Surprise: p-values
<p class="subheader">The p-value measures how compatible our data is with the null hypothesis.</p>

<br>

. . .

**p-value**: *The probability of observing a test statistic at least as extreme as ours, if the null hypothesis were true*

. . .

<br>

**For our example:**

- Null: $\mu = 10$

. . .

- Observed: $\bar{x} = 10.85$

. . .

*> How likely is it to get $\bar{x}$ this far or farther from 10, if the true mean is 10?*

---

## Quantifying Surprise: p-values
<p class="subheader">Example cont.: What is the probability of an error as large as the observed mean?</p>

*> how likely is it to get $\bar{x}$ this far or farther from 10, if the true mean is 10?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.norm.pdf(x, mu_0, std_err)

# Calculate two-tailed p-value
z_score = (observed_mean - mu_0) / std_err
p_value = stats.t.cdf((mu_0-observed_mean)/se, df=29) * 2

plt.figure(figsize=(11, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

# Shade the area representing the p-value
x_right = np.linspace(observed_mean, mu_0+3*std_err, 500)
y_right = stats.norm.pdf(x_right, mu_0, std_err)
plt.fill_between(x_right, 0, y_right, color='red', alpha=0.3)

x_left = np.linspace(mu_0-3*std_err, mu_0-(observed_mean-mu_0), 500)
y_left = stats.norm.pdf(x_left, mu_0, std_err)
plt.fill_between(x_left, 0, y_left, color='red', alpha=0.3)

plt.annotate(f'p-value = {p_value:.3f}', xy=(mu_0, 0.4),
             xytext=(mu_0, 0.6), fontsize=14, ha='center',
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))

plt.title('Two-tailed Test for μ=10', fontsize=16)
plt.xlabel('Sample Mean (minutes)', fontsize=14)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=-5, trim=True)
plt.tight_layout()
plt.show()
```

. . .

```python
stats.t.cdf((mu_0-xbar)/se, df=n-1)) * 2
```

. . .

*> interpretation: if μ=10, we'd see $\bar{x}$ this far from 10 about 7.2% of the time*

. . .

*> often, we reject $H_0$ if p-value < 0.05 (5%)*

. . .

*> here, p-value > 0.05, so we don't reject the claim that μ=10*

---

## Test Statistic: The t-statistic
<p class="subheader">We can standardize our result for easier interpretation</p>

. . .

The **t-statistic** measures how many standard errors our sample mean is from the null value:

. . .

$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$$

. . .

Where:

- $\bar{x}$ is our sample mean (10.85)
- $\mu_0$ is our null value (10)
- $s$ is our sample standard deviation (2.5)
- $n$ is our sample size (30)

---

## Test Statistic: The t-statistic
<p class="subheader">We can standardize our result for easier interpretation</p>

The **t-statistic** measures how many standard errors our sample mean is from the null value:

$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} = \frac{10.85 - 10}{2.5/\sqrt{30}} = \frac{0.85}{0.456} = 1.86$$

Where:

- $\bar{x}$ is our sample mean (10.85)
- $\mu_0$ is our null value (10)
- $s$ is our sample standard deviation (2.5)
- $n$ is our sample size (30)

---

## The t-test
<p class="subheader">This example has become a formal hypothesis test.</p>

:::: {.columns}
::: {.column width="40%"}
**One-sample t-test:**

- $H_0: \mu = 10$  
- $H_1: \mu \neq 10$
- Test statistic: $t = 1.86$
- Degrees of freedom: 29
- p-value: 0.072

**Decision rule:**

- If p-value < 0.05, reject $H_0$
- Otherwise, fail to reject $H_0$
:::

::: {.column width="60%"}
```python
# Imports
import numpy as np
from scipy import stats
```

<br>

```python
# Sample Data
sample_mean = 10.85
pop_mean = 10    # null hypothesis
std_dev = 2.5    
sample_size = 30
```

<br>

```python
# Calculate t-statistic
t_stat = (sample_mean - pop_mean) / (std_dev / np.sqrt(sample_size))
```

<br>

```python
# Calculate p-value
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=sample_size-1))
```
:::
::::

. . .

*> t-tests are extremely common, especially in regression (coming soon!)*

---

## Statistical vs. Practical Significance
<p class="subheader">A caution about hypothesis testing</p>

<br>

. . .

**Statistical significance:**

- Formal rejection of the null hypothesis (p < 0.05)
- Only tells us if the effect is unlikely due to chance

. . .

**Practical significance:**

- Whether the effect size matters in the real world
- A statistically significant result can still be tiny

. . .

*> with large samples, even tiny differences can be statistically significant*

. . .

*> always consider the magnitude of the effect, not just the p-value*

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that $H_0$ is true

   - The p-value doesn't tell us if the null hypothesis is correct. It assumes the null is true and then calculates how surprising our result would be under that assumption.
   - *Example:* A p-value of 0.04 doesn't mean there's a 4% chance the null hypothesis is true.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that the results occurred by chance

   - All results reflect some combination of real effects and random variation. The p-value doesn't separate these components.
   - *Example:* A p-value of 0.04 doesn't mean there's a 4% chance our results are due to chance and 96% chance they're real.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

❌ **Not:** The probability that $H_1$ is true

   - The p-value doesn't directly address the alternative hypothesis or its likelihood.
   - *Example:* A p-value of 0.04 doesn't mean there's a 96% chance the alternative hypothesis is true.

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

✅ **Correct:** The probability of observing a test statistic at least as extreme as ours, if $H_0$ were true

   - It measures the compatibility between our data and the null hypothesis.
   - *Example:* A p-value of 0.04 means: "If the null hypothesis were true, we'd see results this extreme or more extreme only about 4% of the time."

*> think of it like this: The p-value answers "How surprising is this data if the null hypothesis is true?" not "Is the null hypothesis true?"*

---

## Looking Forward
<p class="subheader">The t-test framework extends to many scenarios</p>

<br>

. . .

**Next time:**

- Comparing means between two groups

. . .

**Coming soon:**

- This same framework underlies regression analysis
- Regression coefficients are tested using t-tests
- ANOVA uses the same fundamental approach

. . .

*> the hypothesis testing framework is foundational for modern science*