---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 4.4 | OLS Assumptions; Multiple Regression*

---

## OLS Assumptions
<p class="subheader">Our test results are only valid when the model assumptions are valid.</p>

<br>

1. **Linearity**: The relationship between X and Y is linear
   
. . .

2. **Independence**: Observations are independent from each other

. . . 

3. **Homoskedasticity**: Equal error variance across all values of X

. . .

4. **Normality**: Errors are normally distributed

---

## Model Diagnostics: Why Check Assumptions?
<p class="subheader">Assumption violations affect our inferences</p>

<br>

**If assumptions are violated:**

- Coefficient estimates may be biased
- Standard errors may be wrong
- p-values may be misleading
- Predictions may be unreliable

. . .

*> to test whether the model is 'specified', we can calculate the residuals and the model predictions*

---

## Example: Education and Income
<p class="subheader">Is income higher for those more highly educated?</p>

---

## Model Residuals
<p class="subheader">... we can directly examine the error of the model.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, ax2 = plt.subplots(1, 1, figsize=(11, 3))

# Plot 2: Linear regression model
ax2.scatter(minutes_after_open, wait_times, alpha=0.1)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept + slope * x_line
ax2.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.1)

# Add vertical error lines for regression model
for i in range(0, n, 1):  # Show errors for every 5th point
    y_pred = intercept + slope * minutes_after_open[i]
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=1)

# Use the same styling as your other plots
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

. . .

<br>

```python
# Calculate residuals
residuals = model.resid
residuals.hist()
```

. . .

*> this is $\varepsilon$*

---

## Model Predictions
<p class="subheader">... we can directly examine the predictions of the model.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Update Matplotlib parameters
plt.rcParams.update({
    'font.family': 'serif',              # Set the font family
    'font.serif': ['Times New Roman'],   # Use a specific serif font
    'font.style': 'italic',              # Set the font style to italic
})

# Set a seed for reproducibility
np.random.seed(42)

# Generate wait times dataset
n = 60
minutes_after_open = np.linspace(0, 600, n)  # 0 to 600 minutes
wait_times = 5 + 0.01 * minutes_after_open + np.random.normal(0, 2, n)  # Base wait + trend + noise
wait_times = np.maximum(0, wait_times)  # Ensure no negative wait times

# Fit regression models
slope, intercept, r_value, p_value, std_err = stats.linregress(minutes_after_open, wait_times)
mean_wait = np.mean(wait_times)

# Calculate MSE for both models
mse_mean = np.mean((wait_times - mean_wait)**2)
mse_regression = np.mean((wait_times - (intercept + slope * minutes_after_open))**2)

# Create a figure with two subplots
fig, ax2 = plt.subplots(1, 1, figsize=(11, 3))

# Plot 2: Linear regression model
ax2.scatter(minutes_after_open, wait_times, alpha=0.1)
x_line = np.array([min(minutes_after_open), max(minutes_after_open)])
y_line = intercept + slope * x_line
ax2.plot(x_line, y_line, 'r-', linewidth=2, alpha=1)

# Add vertical error lines for regression model
for i in range(0, n, 1):  # Show errors for every 5th point
    y_pred = intercept + slope * minutes_after_open[i]
    ax2.plot([minutes_after_open[i], minutes_after_open[i]], 
             [wait_times[i], y_pred], 'green', linestyle=':', alpha=0.1)

# Use the same styling as your other plots
sns.despine(ax=ax2, left=False, bottom=False, right=True, top=True, trim=True)
plt.tight_layout()
plt.show()
```

<br>

. . .

```python
# Calculate predictions
predictions = model.predict()
predictions.hist()
```
. . .

*> this is $\hat{y}$, the model prediction*

---

## Residual Plot
<p class="subheader">... we can directly observe the error according to the model estimates.</p>


```python
plt.scatter(predictions, residuals)
```

---

## Assumption 1: Checking for Linearity
<p class="subheader">The error term should be unrelated to the fitted value.</p>

*> which one of these figures shows linearity?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set seed for reproducibility
np.random.seed(42)

# Generate education data (years)
education = np.concatenate([np.random.randint(10, 13, 50), 
                           np.random.randint(13, 17, 70),
                           np.random.randint(17, 22, 30)])

# Generate wage data with more pronounced heteroskedasticity
base_wage = 20000
education_effect = 2000
error_scale = 3000 * np.power(education/10, 2)  # More dramatic increase in variance
wages = base_wage + education_effect * education + np.random.normal(0, error_scale, len(education))

# Sample data
np.random.seed(42)
x = np.linspace(0, 10, 50)
y_linear = 2 + 0.5 * x + np.random.normal(0, 1, 50)
y_nonlinear = 2 + 0.5 * x + 0.2 * x**2 + np.random.normal(0, 1, 50)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))

# Linear model
from scipy import stats
slope1, intercept1, r1, p1, se1 = stats.linregress(x, y_linear)
y_pred1 = intercept1 + slope1 * x
residuals1 = y_linear - y_pred1

ax1.scatter(y_pred1, residuals1, alpha=0.7)
ax1.axhline(y=0, color='r', linestyle='-')
ax1.set_xlabel('Fitted Values ($\hat{y}$)')
ax1.set_ylabel('Residuals ($\hat{\\varepsilon}$)')

# Nonlinear model fitted with linear regression
slope2, intercept2, r2, p2, se2 = stats.linregress(x, y_nonlinear)
y_pred2 = intercept2 + slope2 * x
residuals2 = y_nonlinear - y_pred2

ax2.scatter(y_pred2, residuals2, alpha=0.7)
ax2.axhline(y=0, color='r', linestyle='-')
ax2.set_xlabel('Fitted Values ($\hat{y}$)')

sns.despine(trim=True)
plt.tight_layout()
```

. . .

*> the left one is what we want to see*

. . .

*> residual plots should show that the model is equally wrong everywhere*

---

## Non-Linear Relationships
<p class="subheader">A non-linear relationship will produce non-linear residuals.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import pandas as pd

# Set seed for reproducibility
np.random.seed(42)

# Generate age data
age = np.random.uniform(18, 65, 150)

# Generate income with a STRONGLY non-linear relationship to age
# Income follows a pronounced inverted U shape with age
true_income = 20000 + 4000 * age - 70 * (age-40)**2
noise = np.random.normal(0, 15000, len(age))
income = true_income + noise

# Create DataFrame
data = pd.DataFrame({
    'age': age,
    'income': income
})

# Fit linear model
X_linear = sm.add_constant(age)
linear_model = sm.OLS(income, X_linear).fit()

# Fit quadratic model
age_squared = age**2
X_quadratic = sm.add_constant(np.column_stack((age, age_squared)))
quadratic_model = sm.OLS(income, X_quadratic).fit()

# Predictions for plotting
age_range = np.linspace(18, 65, 100)
X_linear_pred = sm.add_constant(age_range)
linear_predictions = linear_model.predict(X_linear_pred)

age_range_squared = age_range**2
X_quadratic_pred = sm.add_constant(np.column_stack((age_range, age_range_squared)))
quadratic_predictions = quadratic_model.predict(X_quadratic_pred)

# Create the plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Linear regression plot
ax1.scatter(age, income, alpha=0.6, color='#4C72B0')
ax1.plot(age_range, linear_predictions, color='red', linewidth=2, 
         label=f'Linear: RÂ² = {linear_model.rsquared:.2f}')
ax1.set_title('Linear Regression: Age vs. Income', fontsize=14)
ax1.set_xlabel('Age (years)', fontsize=12)
ax1.set_ylabel('Annual Income ($)', fontsize=12)
ax1.legend()

# Add true curve to highlight the actual relationship
ax1.plot(age_range, 20000 + 4000 * age_range - 70 * (age_range-40)**2, 
         color='green', linestyle='--', linewidth=2, label='True relationship')
ax1.legend()

# Residuals of linear model
residuals = income - linear_model.predict(X_linear)
sns.regplot(x=age, y=residuals, lowess=True, scatter_kws={'alpha': 0.4}, 
            line_kws={'color': 'red'}, ax=ax2)
ax2.axhline(y=0, color='black', linestyle='--')
ax2.set_title('Residuals vs. Age (Linear Model)', fontsize=14)
ax2.set_xlabel('Age (years)', fontsize=12)
ax2.set_ylabel('Residuals ($)', fontsize=12)
# Add annotation highlighting the pattern
ax2.annotate('Clear U-shaped pattern\nin residuals', xy=(40, -40000), xytext=(45, -60000),
            arrowprops=dict(facecolor='black', shrink=0.05, width=1), fontsize=12)

sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> sometimes relationships aren't linear*

. . .

*> linear model misses curvature, leading to systematic errors*

. . .

*> check your residuals*

---

## Handling Non-Linear Relationships
<p class="subheader">Transform variables to become linear</p>

*> here, adding a squared term captures the curvature in our data*

$$\text{income} = \beta_0 + \beta_1 \text{age} + \beta_2 \text{age}^2 + \varepsilon$$

*instead of*

$$\text{income} = \beta_0 + \beta_1 \text{age} + \varepsilon$$

. . .

```python
df['age_squared'] = df['age']**2
quadratic_model = smf.ols('income ~ age + age_squared', data=df).fit()
```

. . .

*> coefficient interpretations change:*

- Î²â = effect of age when age = 0 (not very meaningful here)
- Î²â = how the effect of age changes as age increases

. . .

*> other common transformations: log(y) ~ x or y ~ log(x) or log(y) ~ log(x)*

---

## Assumption 3: Homoskedasticity
<p class="subheader">Residuals should be spread out the same everywhere.</p>

*> which one of these figures shows homoskedasticity?*

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Sample data
np.random.seed(42)
x = np.linspace(0, 10, 50)

# Homoskedastic errors
y_homo = 2 + 0.5 * x + np.random.normal(0, 1, 50)

# Heteroskedastic errors (variance increases with x)
y_hetero = 2 + 0.5 * x + np.random.normal(0, 0.2 + 0.3 * x, 50)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 3))

# Homoskedastic model
slope1, intercept1, r1, p1, se1 = stats.linregress(x, y_homo)
y_pred1 = intercept1 + slope1 * x
residuals1 = y_homo - y_pred1

ax1.scatter(y_pred1, residuals1, alpha=0.7)
ax1.axhline(y=0, color='r', linestyle='-')
ax1.set_xlabel('Fitted Values ($\hat{y}$)')
ax1.set_ylabel('Residuals ($\hat{\\varepsilon}$)')

# Heteroskedastic model
slope2, intercept2, r2, p2, se2 = stats.linregress(x, y_hetero)
y_pred2 = intercept2 + slope2 * x
residuals2 = y_hetero - y_pred2

ax2.scatter(y_pred2, residuals2, alpha=0.7)
ax2.axhline(y=0, color='r', linestyle='-')
ax2.set_xlabel('Fitted Values ($\hat{y}$)')

sns.despine(trim=True)
plt.tight_layout()
```

. . .

*> the left figure shows constant variability (homoskedasticity)*

. . .

*> the right one has increasing variability (heteroskedasticity)*

. . .

*> residual plots should show that the model is equally wrong everywhere*

---

## Heteroskedasticity
<p class="subheader">When the spread of residuals changes across values of X</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set seed for reproducibility
np.random.seed(42)

# Generate education data (years)
education = np.concatenate([np.random.randint(10, 13, 50), 
                           np.random.randint(13, 17, 70),
                           np.random.randint(17, 22, 30)])

# Generate wage data with more pronounced heteroskedasticity
base_wage = 20000
education_effect = 2000
error_scale = 3000 * np.power(education/10, 2)  # More dramatic increase in variance
wages = base_wage + education_effect * education + np.random.normal(0, error_scale, len(education))

# Fit regression line
slope, intercept, r_value, p_value, std_err = stats.linregress(education, wages)


# Add regression line
x_line = np.array([min(education), max(education)])
y_line = intercept + slope * x_line

# Use the same data from previous slide
# Calculate residuals
predictions = intercept + slope * education
residuals = wages - predictions

# Create figure with two panels
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Left panel: Scatter with regression line and spread highlighted
ax1.scatter(education, wages, alpha=0.6, color='#4C72B0')
ax1.plot(x_line, y_line, color='red', linewidth=2)

# Add shaded regions to show increasing spread
x_regions = [12, 16, 20]
for i in range(len(x_regions)):
    x = x_regions[i]
    y_pred = intercept + slope * x
    spread = 4 * 2000 * np.sqrt(x) / 3
    ax1.fill_between([x-0.5, x+0.5], 
                     [y_pred-spread, y_pred-spread],
                     [y_pred+spread, y_pred+spread],
                     color='gray', alpha=0.3)
    
ax1.set_title('Education vs. Wages: Increasing Spread', fontsize=14)
ax1.set_xlabel('Years of Education', fontsize=12)
ax1.set_ylabel('Annual Wage ($)', fontsize=12)

# Add annotations for different education levels
ax1.annotate('High school\n(less spread)', xy=(11.5, 60000), fontsize=10)
ax1.annotate('Bachelor\'s\n(medium spread)', xy=(15.5, 68000), fontsize=10)
ax1.annotate('PhD\n(more spread)', xy=(19.5, 76000), fontsize=10)

# Right panel: Residuals plot showing heteroskedasticity
ax2.scatter(education, residuals, alpha=0.6, color='#4C72B0')
ax2.axhline(y=0, color='red', linestyle='-', linewidth=2)

ax2.set_title('Residuals vs. Education: Heteroskedasticity', fontsize=14)
ax2.set_xlabel('Years of Education', fontsize=12)
ax2.set_ylabel('Residuals ($)', fontsize=12)

sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> notice how the spread of points increases with more education*

. . .

*> PhD wages vary more than high school wages*

---

## Heteroskedasticity
<p class="subheader">It affects how we measure uncertainty in our estimates</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statsmodels.api as sm

# Set seed for reproducibility
np.random.seed(42)

# Generate education data (years)
education = np.concatenate([np.random.randint(10, 13, 50), 
                           np.random.randint(13, 17, 70),
                           np.random.randint(17, 22, 30)])

# Generate wage data with more pronounced heteroskedasticity
base_wage = 20000
education_effect = 2000
error_scale = 3000 * np.power(education/10, 2)  # More dramatic increase in variance
wages = base_wage + education_effect * education + np.random.normal(0, error_scale, len(education))

# Use statsmodels for better standard error calculation
X = sm.add_constant(education)
model = sm.OLS(wages, X).fit()
robust_model = sm.OLS(wages, X).fit(cov_type='HC3')  # Robust standard errors

# Get standard errors
std_err = model.bse[1]  # Standard error for slope
robust_std_err = robust_model.bse[1]  # Robust standard error for slope

# Parameters for plotting
x_range = np.linspace(min(education), max(education), 100)
X_pred = sm.add_constant(x_range)
y_pred = model.predict(X_pred)

# Calculate confidence intervals
conf_level = 0.95
degrees_freedom = len(education) - 2
t_critical = stats.t.ppf((1 + conf_level) / 2, degrees_freedom)

# Standard confidence interval
std_ci_width = t_critical * std_err
std_lower = y_pred - std_ci_width * x_range
std_upper = y_pred + std_ci_width * x_range

# Robust confidence interval
robust_ci_width = t_critical * robust_std_err
robust_lower = y_pred - robust_ci_width * x_range
robust_upper = y_pred + robust_ci_width * x_range

# Create figure
plt.figure(figsize=(10, 4))
plt.scatter(education, wages, alpha=0.6, color='#4C72B0', label='Data points')
plt.plot(x_range, y_pred, color='red', linewidth=2, label='Regression line')

# Add regular confidence interval
plt.fill_between(x_range, std_lower, std_upper, color='red', alpha=0.2, 
                label=f'Standard CI (Â±{std_err:.0f})')

# Add robust confidence interval
plt.fill_between(x_range, robust_lower, robust_upper, color='green', alpha=0.2,
                label=f'Robust CI (Â±{robust_std_err:.0f})')

plt.title('Impact of Heteroskedasticity on Confidence Intervals', fontsize=14)
plt.xlabel('Years of Education', fontsize=12)
plt.ylabel('Annual Wage ($)', fontsize=12)
plt.legend()
sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> standard methods assume constant spread (homoskedasticity)*

. . .

*> like using the wrong ruler to measure uncertainty*

. . .

*> with heteroskedasticity, we need robust standard errors*

. . .

*> these adjust for the changing spread in our data*

---

## Handling Heteroskedasticity
<p class="subheader">Robust standard errors give more accurate measures of uncertainty</p>

```python
# Fit the model with robust standard errors (HC3: heteroskedastic-constant)
robust_model = smf.ols('wages ~ education', data=df).fit(cov_type='HC3')
```

. . .

*> robust standard errors give more accurate confidence intervals*

. . .

*> and more reliable hypothesis tests*

. . .

*> especially important when heteroskedasticity is pronounced*


---

## Assumption 4: Normality
<p class="subheader">Residuals should be normally distributed</p>
```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# Sample data
np.random.seed(42)
n = 1000
# Normal errors
normal_residuals = np.random.normal(0, 1, n)
# Skewed errors
skewed_residuals = stats.skewnorm.rvs(5, size=n)
skewed_residuals = skewed_residuals - skewed_residuals.mean()  # Center at 0

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 3))

# Normal histogram with curve
sns.histplot(normal_residuals, kde=True, ax=ax1)
ax1.set_xlabel('Residuals ($\hat{\\varepsilon}$)')
ax1.set_yticks([])
ax1.set_ylabel('')

# Skewed histogram with curve
sns.histplot(skewed_residuals, kde=True, ax=ax2)
ax2.set_xlabel('Residuals ($\hat{\\varepsilon}$)')
ax2.set_yticks([])
ax2.set_ylabel('')

sns.despine(left=True, bottom=False, right=True, top=True, trim=True)

plt.tight_layout()
```

. . .

*> left shows a nice bell shape (roughly normally distributed)*

. . .

*> right shows a skewed distribution (not normally distributed)*

. . .

*> by the CLT we can still use regression without this if the sample is large*

---

## Multiple Regression
<p class="subheader">Wages depend on more than just education</p>

*Wages also depend on:*

- Experience
- Industry
- Location
- And many other factors

. . .

*> how can we handle multiple relationships at once?*

---

## Modeling Relationships Separately
<p class="subheader">What if we build a regression model for both relationships separately?</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set seed for reproducibility
np.random.seed(42)

# Generate education data (years)
education = np.concatenate([np.random.randint(10, 13, 50), 
                           np.random.randint(13, 17, 70),
                           np.random.randint(17, 22, 30)])

# Generate experience data (years) - negatively correlated with education
mean_experience = 40 - 1.5 * education  # Higher education -> less experience on average
experience = np.maximum(0, mean_experience + np.random.normal(0, 3, len(education)))

# Generate wage data affected by both education and experience
educ_effect = 1500
exp_effect = 500
base_wage = 15000
error_scale = 5000

wages = base_wage + educ_effect * education + exp_effect * experience + np.random.normal(0, error_scale, len(education))

# Create figure with side-by-side plots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot education vs wages
slope_educ, intercept_educ, r_educ, p_educ, se_educ = stats.linregress(education, wages)
ax1.scatter(education, wages, alpha=0.6, color='#4C72B0')
x_line = np.array([min(education), max(education)])
y_line = intercept_educ + slope_educ * x_line
ax1.plot(x_line, y_line, color='red', linewidth=2, 
        label=f'Slope â ${slope_educ:.0f}/year')
ax1.set_title('Education vs. Wages', fontsize=14)
ax1.set_xlabel('Years of Education', fontsize=12)
ax1.set_ylabel('Annual Wage ($)', fontsize=12)
ax1.legend()

# Plot experience vs wages
slope_exp, intercept_exp, r_exp, p_exp, se_exp = stats.linregress(experience, wages)
ax2.scatter(experience, wages, alpha=0.6, color='#4C72B0')
x_line = np.array([min(experience), max(experience)])
y_line = intercept_exp + slope_exp * x_line
ax2.plot(x_line, y_line, color='red', linewidth=2, 
        label=f'Slope â ${slope_exp:.0f}/year')
ax2.set_title('Experience vs. Wages', fontsize=14)
ax2.set_xlabel('Years of Experience', fontsize=12)
ax2.set_ylabel('Annual Wage ($)', fontsize=12)
ax2.legend()

sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> does this mean years of experience has a negative relationship with wages?*

---

## The Challenge: Related Variables
<p class="subheader">Education and experience are correlated!</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Plot the relationship between education and experience
plt.figure(figsize=(8, 5))
slope_rel, intercept_rel, r_rel, p_rel, se_rel = stats.linregress(education, experience)
plt.scatter(education, experience, alpha=0.6, color='#4C72B0')
x_line = np.array([min(education), max(education)])
y_line = intercept_rel + slope_rel * x_line
plt.plot(x_line, y_line, color='red', linewidth=2, 
         label=f'Correlation: {r_rel:.2f}')

plt.title('Relationship Between Education and Experience', fontsize=14)
plt.xlabel('Years of Education', fontsize=12)
plt.ylabel('Years of Experience', fontsize=12)
plt.legend()
sns.despine()
plt.tight_layout()
plt.show()
```

. . .

*> more education usually means less work experience*

. . .

*> if we look at one without accounting for the other, we get misleading results*

---

## Multiple Regression
<p class="subheader">We can adjust for multiple variables simultaneously.</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Fit multiple regression model
X = sm.add_constant(np.column_stack((education, experience)))
model = sm.OLS(wages, X).fit()

# Create a 3D figure
fig = plt.figure(figsize=(10, 5))
ax = fig.add_subplot(111, projection='3d')

# Plot original data points
ax.scatter(education, experience, wages, c='#4C72B0', alpha=0.6)

# Create a meshgrid to visualize the regression plane
edu_range = np.linspace(min(education), max(education), 20)
exp_range = np.linspace(min(experience), max(experience), 20)
edu_grid, exp_grid = np.meshgrid(edu_range, exp_range)
Z = model.params[0] + model.params[1] * edu_grid + model.params[2] * exp_grid

# Plot the regression plane
ax.plot_surface(edu_grid, exp_grid, Z, alpha=0.3, color='red')

# Add labels
ax.set_xlabel('Education (years)', fontsize=12)
ax.set_ylabel('Experience (years)', fontsize=12)
ax.set_zlabel('Wage ($)', fontsize=12)

# Rotate the plot for better viewing
ax.view_init(elev=20, azim=235)

plt.tight_layout()
plt.show()
```

. . .

*> multiple regression gives each variable's effect "holding others constant"*

---

## The Multiple Regression Equation
<p class="subheader">Extending the best-fitting line to multiple dimensions</p>

<br>

. . .

**Single Variable:**

$$\text{Wage} = \beta_0 + \beta_1 \times \text{Education} + \epsilon$$

. . .

**Multiple Variables:**

$$\text{Wage} = \beta_0 + \beta_1 \times \text{Education} + \beta_2 \times \text{Experience} + \epsilon$$

. . .

**Interpretation:**

- $\beta_0$ = Base wage (intercept)
- $\beta_1$ = Effect of one more year of education, *holding experience constant*
- $\beta_2$ = Effect of one more year of experience, *holding education constant*

---

## Example: Testing with Multiple Regression
<p class="subheader">We can test individual variables or groups of variables</p>

```python
import statsmodels.formula.api as smf

# Fit multiple regression model
model = smf.ols('INCLOG10 ~ EDU + AGE', data=data).fit()
```

. . .

*> can test each one like before (t-test)*

. . .

*> "Are education AND age related to wages?"*

. . .

*> does this mean the model without `AGE` was wrong?*

. . .

*> how do we know if we've included everything?*

---

## Indicator (dummy) Variables
<p class="subheader">... we can easily turn numerical or categorical variables into indicator variables. </p>

<br>

```python
# 1. Simple binary indicator (above/below threshold)
model1 = smf.ols('INCLOG10 ~ I(EDU > 12)', data=data).fit()
```

. . .

<br>

```python
# 2. Multiple thresholds/categories
model2 = smf.ols('INCLOG10 ~ I(EDU > 12) + I(EDU < 9)', data=data).fit()
```

. . .

<br>

```python
# 3. Indicators from existing categorical variable
model3 = smf.ols('INCLOG10 ~ EDU + C(DEGFIELD) data=data).fit()
```

---

## Looking Forward
<p class="subheader">Next steps in building the general linear model...</p>

<br>

. . .

**Next topics:**

- Omitted variable bias
- Fixed effects
- Multicollinearity
- Causality
- Basic time series
- Multiple slope models
