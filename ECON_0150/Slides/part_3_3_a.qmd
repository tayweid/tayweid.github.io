---
format:
  revealjs:
    css: custom.css
    transition: none
    aspect-ratio: "16:9"
---

## ECON 0150 | Economic Data Analysis {.center}
<p class="subheader">The economist's data analysis pipeline.</p>

<br> 

### *Part 3.3 | From Confidence Intervals to Hypothesis Testing*

---

## Confidence Intervals Recap
<p class="subheader">We found the distribution of sample means and used it to create intervals.</p>

:::: {.columns}
::: {.column width="50%"}
- Sample mean $\bar{x}$ follows a normal distribution
- Centered at population mean $\mu$
- Standard error = $\frac{\sigma}{\sqrt{n}}$
- 95% of samples will have $\bar{x}$ within 1.96 standard errors of $\mu$
:::

::: {.column width="50%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(123)
n_means = 1000
sample_size = 30
sample_means = [np.mean(np.random.normal(12, 2.5, sample_size)) 
                for _ in range(n_means)]

plt.figure(figsize=(5, 3))
plt.hist(sample_means, bins=25, density=True, alpha=0.5)
x = np.linspace(10, 14, 1000)
y = stats.norm.pdf(x, 12, 2.5/np.sqrt(sample_size))
plt.plot(x, y, 'r-')
plt.axvline(x=12, color='blue', linestyle='--')
plt.title(f'Distribution of Sample Means (n={sample_size})')
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)

plt.tight_layout()
plt.show()
```
:::
::::

. . .

*> in the wait time example, we were asking "where is the true mean wait time?"*

---

## Flipping The Question
<p class="subheader">What if we want to test a specific claim about the mean?</p>

<br>

. . .

*> "My boss claims the mean wait time is 10 minutes"*

. . .

*> Is our data consistent with that specific claim?*

. . .

*> Same math, different question...*

. . .

*> Instead of finding where μ might be, we're testing a specific value*

---

## Testing a Specific Value
<p class="subheader">Let's simulate data where μ=10 and see what sample means we'd get</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

np.random.seed(42)
true_mean = 10
std_dev = 2.5
sample_size = 30
n_samples = 1000

# Generate samples and calculate their means
sample_means = [np.mean(np.random.normal(true_mean, std_dev, sample_size)) 
                for _ in range(n_samples)]

plt.figure(figsize=(10, 4))
plt.hist(sample_means, bins=30, density=True, alpha=0.6, color='#4C72B0')
x = np.linspace(8.5, 11.5, 1000)
y = stats.norm.pdf(x, true_mean, std_dev/np.sqrt(sample_size))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=true_mean, color='red', linestyle='--', linewidth=2)

# Add annotation for the observed value
observed_mean = 10.85
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)
plt.annotate(f'Observed\nMean: {observed_mean}', xy=(observed_mean, 0.5),
             xytext=(observed_mean+0.3, 1.0), fontsize=12,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))

plt.title('Distribution of Sample Means if True Mean is 10', fontsize=16)
plt.xlabel('Sample Mean (minutes)', fontsize=14)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> if our sample gives $\bar{x}=10.85$, is that consistent with μ=10?*

. . .

*> how "surprising" would our observed $\bar{x}$ be if the null hypothesis were true?*

---

## Connecting to Confidence Intervals
<p class="subheader">The math is identical to confidence intervals</p>

:::: {.columns}
::: {.column width="60%"}
If true mean is $\mu_0 = 10$:

. . .

$$\bar{X} \sim N\left(\mu_0, \frac{\sigma}{\sqrt{n}}\right)$$

. . .

$$\bar{X} \sim N\left(10, \frac{2.5}{\sqrt{30}}\right)$$

. . .

$$\bar{X} \sim N(10, 0.456)$$

. . .

A 95% confidence interval around $\mu_0$ would be:

. . .

$$[10 - 1.96 \times 0.456, 10 + 1.96 \times 0.456]$$

. . .

$$[9.11, 10.89]$$
:::

::: {.column width="40%"}
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.norm.pdf(x, mu_0, std_err)

plt.figure(figsize=(5, 3))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Shade the confidence interval
ci_lower = mu_0 - 1.96 * std_err
ci_upper = mu_0 + 1.96 * std_err
plt.fill_between(x[(x >= ci_lower) & (x <= ci_upper)], 
                 0, 
                 y[(x >= ci_lower) & (x <= ci_upper)], 
                 color='blue', alpha=0.2)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

plt.yticks([])
plt.title('Sampling Distribution if μ=10')

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```
:::
::::

. . .

*> our observed mean ($\bar{x} = 10.85$) is within this interval — not surprising if μ=10*

. . .

*> but if we observed $\bar{x} = 11.5$, that would be outside the interval — surprising!*

---

## The Null Hypothesis
<p class="subheader">We formalize this approach by setting up a "null hypothesis"</p>

<br>

. . .

**Null Hypothesis** ($H_0$): *The specific value or claim we're testing*

. . .

- $H_0: \mu = 10$ (wait time is 10 minutes)

. . .

**Alternative Hypothesis** ($H_1$ or $H_a$): *What we accept if we reject the null*

. . .

- $H_1: \mu \neq 10$ (wait time is not 10 minutes)

. . .

**Testing Approach**: 
- Calculate how "surprising" our data would be if $H_0$ were true
- If sufficiently surprising, we reject $H_0$

---

## Quantifying "Surprise": p-values
<p class="subheader">The p-value measures how compatible our data is with the null hypothesis</p>

<br>

. . .

**p-value**: *The probability of observing a test statistic at least as extreme as ours, if the null hypothesis were true*

. . .

For our example:
- Null: $\mu = 10$
- Observed: $\bar{x} = 10.85$
- How likely is it to get $\bar{x}$ this far or farther from 10, if the true mean is 10?

---

## Calculating a p-value
<p class="subheader">We use the sampling distribution to find the probability</p>

```{python}
#| echo: false
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

mu_0 = 10
std_err = 2.5/np.sqrt(30)
observed_mean = 10.85

x = np.linspace(mu_0-3*std_err, mu_0+3*std_err, 1000)
y = stats.norm.pdf(x, mu_0, std_err)

# Calculate two-tailed p-value
z_score = (observed_mean - mu_0) / std_err
p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

plt.figure(figsize=(10, 4))
plt.plot(x, y, 'r-', linewidth=2)
plt.axvline(x=mu_0, color='blue', linestyle='--', linewidth=1)

# Mark the observed mean
plt.axvline(x=observed_mean, color='green', linestyle='-', linewidth=2)

# Shade the area representing the p-value
x_right = np.linspace(observed_mean, mu_0+3*std_err, 500)
y_right = stats.norm.pdf(x_right, mu_0, std_err)
plt.fill_between(x_right, 0, y_right, color='red', alpha=0.3)

x_left = np.linspace(mu_0-3*std_err, mu_0-(observed_mean-mu_0), 500)
y_left = stats.norm.pdf(x_left, mu_0, std_err)
plt.fill_between(x_left, 0, y_left, color='red', alpha=0.3)

plt.annotate(f'p-value = {p_value:.3f}', xy=(mu_0, 0.4),
             xytext=(mu_0, 0.6), fontsize=14, ha='center',
             bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))

plt.title('Two-tailed Test for μ=10', fontsize=16)
plt.xlabel('Sample Mean (minutes)', fontsize=14)
plt.yticks([])

sns.despine(left=True, bottom=False, right=True, top=True, offset=0, trim=True)
plt.tight_layout()
plt.show()
```

. . .

*> p-value = 0.064 (or 6.4%)*

. . .

*> interpretation: if μ=10, we'd see a sample mean this far from 10 about 6.4% of the time*

. . .

*> typically, we reject $H_0$ if p-value < 0.05 (5%)*

. . .

*> here, p-value > 0.05, so we don't reject the claim that μ=10*

---

## Test Statistic: The t-statistic
<p class="subheader">We can standardize our result for easier interpretation</p>

<br>

. . .

The **t-statistic** measures how many standard errors our sample mean is from the null value:

. . .

$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$$

. . .

Where:
- $\bar{x}$ is our sample mean (10.85)
- $\mu_0$ is our null value (10)
- $s$ is our sample standard deviation (2.5)
- $n$ is our sample size (30)

. . .

$$t = \frac{10.85 - 10}{2.5/\sqrt{30}} = \frac{0.85}{0.456} = 1.86$$

. . .

This follows a t-distribution with n-1 degrees of freedom (29)

---

## The t-test
<p class="subheader">Our example has become a formal hypothesis test</p>

:::: {.columns}
::: {.column width="60%"}
**One-sample t-test:**
- $H_0: \mu = 10$  
- $H_1: \mu \neq 10$
- Test statistic: $t = 1.86$
- Degrees of freedom: 29
- p-value: 0.064

**Decision rule:**
- If p-value < 0.05, reject $H_0$
- Otherwise, fail to reject $H_0$
:::

::: {.column width="40%"}
```{python}
#| echo: true
import numpy as np
from scipy import stats

# Sample data
sample_mean = 10.85
pop_mean = 10    # null hypothesis
std_dev = 2.5    
sample_size = 30

# Calculate t-statistic
t_stat = (sample_mean - pop_mean) / (std_dev / np.sqrt(sample_size))

# Calculate p-value
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=sample_size-1))

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.3f}")
```
:::
::::

. . .

*> not all test statistics use a t-distribution — depends on the test and sample size*

. . .

*> but t-tests are extremely common, especially in regression (coming soon!)*

---

## Statistical vs. Practical Significance
<p class="subheader">A caution about hypothesis testing</p>

<br>

. . .

**Statistical significance:**
- Formal rejection of the null hypothesis (p < 0.05)
- Only tells us if the effect is unlikely due to chance

. . .

**Practical significance:**
- Whether the effect size matters in the real world
- A statistically significant result can still be tiny

. . .

*> with large samples, even tiny differences can be statistically significant*

. . .

*> always consider the magnitude of the effect, not just the p-value*

---

## Common Misinterpretations
<p class="subheader">What a p-value is NOT</p>

<br>

. . .

❌ The probability that $H_0$ is true

. . .

❌ The probability that the results occurred by chance

. . .

❌ The probability that $H_1$ is true

. . .

✅ **Correct**: The probability of observing a test statistic at least as extreme as ours, if $H_0$ were true

---

## Looking Forward
<p class="subheader">The t-test framework extends to many scenarios</p>

<br>

. . .

**Next time:**
- Comparing means between two groups
- Two-sample t-tests
- Paired t-tests

. . .

**Coming soon:**
- This same framework underlies regression analysis
- Regression coefficients are tested using t-tests
- ANOVA uses the same fundamental approach

. . .

*> the hypothesis testing framework is foundational for modern economics*